<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  <a href="README_zh.md">ä¸­æ–‡</a> &nbsp ï½œ &nbsp English &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  Chinese Documentation</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documentation</a>
<p>


> â­ If you like this project please click the "Star" button in the upper right corner to support us. Your support is our motivation to move forward!

## ğŸ“ Introduction

EvalScope is a powerful and easily extensible model evaluation framework created by the [ModelScope Community](https://modelscope.cn/) aiming to provide a one-stop evaluation solution for large model developers.

Whether you want to evaluate the general capabilities of models conduct multi-model performance comparisons or need to stress test models EvalScope can meet your needs.

## âœ¨ Key Features

- **ğŸ“š Comprehensive Evaluation Benchmarks**: Built-in multiple industry-recognized evaluation benchmarks including MMLU C-Eval GSM8K and more.
- **ğŸ§© Multi-modal and Multi-domain Support**: Supports evaluation of various model types including Large Language Models (LLM) Vision Language Models (VLM) Embedding Reranker AIGC and more.
- **ğŸš€ Multi-backend Integration**: Seamlessly integrates multiple evaluation backends including OpenCompass VLMEvalKit RAGEval to meet different evaluation needs.
- **âš¡ Inference Performance Testing**: Provides powerful model service stress testing tools supporting multiple performance metrics such as TTFT TPOT.
- **ğŸ“Š Interactive Reports**: Provides WebUI visualization interface supporting multi-dimensional model comparison report overview and detailed inspection.
- **âš”ï¸ Arena Mode**: Supports multi-model battles (Pairwise Battle) intuitively ranking and evaluating models.
- **ğŸ”§ Highly Extensible**: Developers can easily add custom datasets models and evaluation metrics.

<details><summary>ğŸ›ï¸ Overall Architecture</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope Overall Architecture.
</p>

1.  **Input Layer**
    - **Model Sources**: API models (OpenAI API) Local models (ModelScope)
    - **Datasets**: Standard evaluation benchmarks (MMLU/GSM8k etc.) Custom data (MCQ/QA)

2.  **Core Functions**
    - **Multi-backend Evaluation**: Native backend OpenCompass MTEB VLMEvalKit RAGAS
    - **Performance Monitoring**: Supports multiple model service APIs and data formats tracking TTFT/TPOP and other metrics
    - **Tool Extensions**: Integrates Tool-Bench Needle-in-a-Haystack etc.

3.  **Output Layer**
    - **Structured Reports**: Supports JSON Table Logs
    - **Visualization Platform**: Supports Gradio Wandb SwanLab

</details>

## ğŸ‰ What's New

> [!IMPORTANT]
> **Version 1.0 Refactoring**
>
> Version 1.0 introduces a major overhaul of the evaluation framework establishing a new more modular and extensible API layer under `evalscope/api`. Key improvements include standardized data models for benchmarks samples and results; a registry-based design for components such as benchmarks and metrics; and a rewritten core evaluator that orchestrates the new architecture. Existing benchmark adapters have been migrated to this API resulting in cleaner more consistent and easier-to-maintain implementations.

- ğŸ”¥ **[2025.12.02]** Added support for custom multimodal VQA evaluation; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/vlm.html). Added support for visualizing model service stress testing in ClearML; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#clearml).
- ğŸ”¥ **[2025.11.26]** Added support for OpenAI-MRCR GSM8K-V MGSM MicroVQA IFBench SciCode benchmarks.
- ğŸ”¥ **[2025.11.18]** Added support for custom Function-Call (tool invocation) datasets to test whether models can timely and correctly call tools. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#function-calling-format-fc).
- ğŸ”¥ **[2025.11.14]** Added support for SWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini code evaluation benchmarks. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/swe_bench.html).
- ğŸ”¥ **[2025.11.12]** Added `pass@k` `vote@k` `pass^k` and other metric aggregation methods; added support for multimodal evaluation benchmarks such as A_OKVQA CMMU ScienceQA V*Bench.
- ğŸ”¥ **[2025.11.07]** Added support for Ï„Â²-bench an extended and enhanced version of Ï„-bench that includes a series of code fixes and adds telecom domain troubleshooting scenarios. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/tau2_bench.html).
- ğŸ”¥ **[2025.10.30]** Added support for BFCL-v4 enabling evaluation of agent capabilities including web search and long-term memory. See the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v4.html).
- ğŸ”¥ **[2025.10.27]** Added support for LogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQA and other evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.26]** Added support for Conll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 and other Named Entity Recognition evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.21]** Optimized sandbox environment usage in code evaluation supporting both local and remote operation modes. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html).
- ğŸ”¥ **[2025.10.20]** Added support for evaluation benchmarks including PolyMath SimpleVQA MathVerse MathVision AA-LCR; optimized evalscope perf performance to align with vLLM Bench. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/vs_vllm_bench.html).
- ğŸ”¥ **[2025.10.14]** Added support for OCRBench OCRBench-v2 DocVQA InfoVQA ChartQA and BLINK multimodal image-text evaluation benchmarks.
- ğŸ”¥ **[2025.09.22]** Code evaluation benchmarks (HumanEval LiveCodeBench) now support running in a sandbox environment. To use this feature please install [ms-enclave](https://github.com/modelscope/ms-enclave) first.
- ğŸ”¥ **[2025.09.19]** Added support for multimodal image-text evaluation benchmarks including RealWorldQA AI2D MMStar MMBench and OmniBench as well as pure text evaluation benchmarks such as Multi-IF HealthBench and AMC.
- ğŸ”¥ **[2025.09.05]** Added support for vision-language multimodal model evaluation tasks such as MathVista and MMMU. For more supported datasets please [refer to the documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/vlm.html).
- ğŸ”¥ **[2025.09.04]** Added support for image editing task evaluation including the [GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/image_edit.html).
- ğŸ”¥ **[2025.08.22]** Version 1.0 Refactoring. Break changes please [refer to](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#switching-to-version-v1-0).
<details><summary>More</summary>

- ğŸ”¥ **[2025.07.18]** The model stress testing now supports randomly generating image-text data for multimodal model evaluation. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#id4).
- ğŸ”¥ **[2025.07.16]** Support for [Ï„-bench](https://github.com/sierra-research/tau-bench) has been added enabling the evaluation of AI Agent performance and reliability in real-world scenarios involving dynamic user and tool interactions. For usage instructions please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#bench).
- ğŸ”¥ **[2025.07.14]** Support for "Humanity's Last Exam" ([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle)) a highly challenging evaluation benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam).
- ğŸ”¥ **[2025.07.03]** Refactored Arena Mode: now supports custom model battles outputs a model leaderboard and provides battle result visualization. See [reference](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html) for details.
- ğŸ”¥ **[2025.06.28]** Optimized custom dataset evaluation: now supports evaluation without reference answers. Enhanced LLM judge usage with built-in modes for "scoring directly without reference answers" and "checking answer consistency with reference answers". See [reference](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa) for details.
- ğŸ”¥ **[2025.06.19]** Added support for the [BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3) benchmark designed to evaluate model function-calling capabilities across various scenarios. For more information refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v3.html).
- ğŸ”¥ **[2025.06.02]** Added support for the Needle-in-a-Haystack test. Simply specify `needle_haystack` to conduct the test and a corresponding heatmap will be generated in the `outputs/reports` folder providing a visual representation of the model's performance. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/needle_haystack.html) for more details.
- ğŸ”¥ **[2025.05.29]** Added support for two long document evaluation benchmarks: [DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary) and [FRAMES](https://modelscope.cn/datasets/iic/frames/summary). For usage guidelines please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html).
- ğŸ”¥ **[2025.05.16]** Model service performance stress testing now supports setting various levels of concurrency and outputs a performance test report. [Reference example](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/quick_start.html#id3).
- ğŸ”¥ **[2025.05.13]** Added support for the [ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static) dataset to evaluate model's tool-calling capabilities. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html) for usage instructions. Also added support for the [DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview) and [Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val) benchmarks to assess the reasoning capabilities of models.
- ğŸ”¥ **[2025.04.29]** Added Qwen3 Evaluation Best Practices [welcome to read ğŸ“–](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** Support for text-to-image evaluation: Supports 8 metrics including MPS HPSv2.1Score etc. and evaluation benchmarks such as EvalMuse GenAI-Bench. Refer to the [user documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/t2i.html) for more details.
- ğŸ”¥ **[2025.04.10]** Model service stress testing tool now supports the `/v1/completions` endpoint (the default endpoint for vLLM benchmarking)
- ğŸ”¥ **[2025.04.08]** Support for evaluating embedding model services compatible with the OpenAI API has been added. For more details check the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters).
- ğŸ”¥ **[2025.03.27]** Added support for [AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview) and [ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary) evaluation benchmarks. For usage notes please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** The model inference service stress testing now supports generating prompts of specified length using random values. Refer to the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#using-the-random-dataset) for more details.
- ğŸ”¥ **[2025.03.13]** Added support for the [LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary) code evaluation benchmark which can be used by specifying `live_code_bench`. Supports evaluating QwQ-32B on LiveCodeBench refer to the [best practices](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html).
- ğŸ”¥ **[2025.03.11]** Added support for the [SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary) and [Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary) evaluation benchmarks. These are used to assess the factual accuracy of models and you can specify `simple_qa` and `chinese_simpleqa` for use. Support for specifying a judge model is also available. For more details refer to the [relevant parameter documentation](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).
- ğŸ”¥ **[2025.03.07]** Added support for the [QwQ-32B](https://modelscope.cn/models/Qwen/QwQ-32B/summary) model evaluate the model's reasoning ability and reasoning efficiency refer to [ğŸ“– Best Practices for QwQ-32B Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html) for more details.
- ğŸ”¥ **[2025.03.04]** Added support for the [SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary) dataset which covers 13 categories 72 first-level disciplines and 285 second-level disciplines totaling 26529 questions. You can use it by specifying `super_gpqa`.
- ğŸ”¥ **[2025.03.03]** Added support for evaluating the IQ and EQ of models. Refer to [ğŸ“– Best Practices for IQ and EQ Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html) to find out how smart your AI is!
- ğŸ”¥ **[2025.02.27]** Added support for evaluating the reasoning efficiency of models. Refer to [ğŸ“– Best Practices for Evaluating Thinking Efficiency](https://evalscope.readthedocs.io/en/latest/best_practice/think_eval.html). This implementation is inspired by the works [Overthinking](https://doi.org/10.48550/arXiv.2412.21187) and [Underthinking](https://doi.org/10.48550/arXiv.2501.18585).
- ğŸ”¥ **[2025.02.25]** Added support for two model inference-related evaluation benchmarks: [MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR) and [ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary). To use them simply specify `musr` and `process_bench` respectively in the datasets parameter.
- ğŸ”¥ **[2025.02.18]** Supports the AIME25 dataset which contains 15 questions (Grok3 scored 93 on this dataset).
- ğŸ”¥ **[2025.02.13]** Added support for evaluating DeepSeek distilled models including AIME24 MATH-500 and GPQA-Diamond datasetsï¼Œrefer to [best practice](https://evalscope.readthedocs.io/en/latest/best_practice/deepseek_r1_distill.html); Added support for specifying the `eval_batch_size` parameter to accelerate model evaluation.
- ğŸ”¥ **[2025.01.20]** Support for visualizing evaluation results including single model evaluation results and multi-model comparison refer to the [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html) for more details; Added [`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary) evaluation example evaluating the IQ and EQ of the model.
- ğŸ”¥ **[2025.01.07]** Native backend: Support for model API evaluation is now available. Refer to the [ğŸ“– Model API Evaluation Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#api) for more details. Additionally support for the `ifeval` evaluation benchmark has been added.
- ğŸ”¥ğŸ”¥ **[2024.12.31]** Support for adding benchmark evaluations refer to the [ğŸ“– Benchmark Evaluation Addition Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html); support for custom mixed dataset evaluations allowing for more comprehensive model evaluations with less data refer to the [ğŸ“– Mixed Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/collection/index.html).
- ğŸ”¥ **[2024.12.13]** Model evaluation optimization: no need to pass the `--template-type` parameter anymore; supports starting evaluation with `evalscope eval --args`. Refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html) for more details.
- ğŸ”¥ **[2024.11.26]** The model inference service performance evaluator has been completely refactored: it now supports local inference service startup and Speed Benchmark; asynchronous call error handling has been optimized. For more details refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).
- ğŸ”¥ **[2024.10.31]** The best practice for evaluating Multimodal-RAG has been updated please check the [ğŸ“– Blog](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag) for more details.
- ğŸ”¥ **[2024.10.23]** Supports multimodal RAG evaluation including the assessment of image-text retrieval using [CLIP_Benchmark](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/clip_benchmark.html) and extends [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html) to support end-to-end multimodal metrics evaluation.
- ğŸ”¥ **[2024.10.8]** Support for RAG evaluation including independent evaluation of embedding models and rerankers using [MTEB/CMTEB](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html) as well as end-to-end evaluation using [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html).
- ğŸ”¥ **[2024.09.18]** Our documentation has been updated to include a blog module featuring some technical research and discussions related to evaluations. We invite you to [ğŸ“– read it](https://evalscope.readthedocs.io/en/refact_readme/blog/index.html).
- ğŸ”¥ **[2024.09.12]** Support for LongWriter evaluation which supports 10000+ word generation. You can use the benchmark [LongBench-Write](evalscope/third_party/longbench_write/README.md) to measure the long output quality as well as the output length.
- ğŸ”¥ **[2024.08.30]** Support for custom dataset evaluations including text datasets and multimodal image-text datasets.
- ğŸ”¥ **[2024.08.20]** Updated the official documentation including getting started guides best practices and FAQs. Feel free to [ğŸ“–read it here](https://evalscope.readthedocs.io/en/latest/)!
- ğŸ”¥ **[2024.08.09]** Simplified the installation process allowing for pypi installation of vlmeval dependencies; optimized the multimodal model evaluation experience achieving up to 10x acceleration based on the OpenAI API evaluation chain.
- ğŸ”¥ **[2024.07.31]** Important change: The package name `llmuses` has been changed to `evalscope`. Please update your code accordingly.
- ğŸ”¥ **[2024.07.26]** Support for **VLMEvalKit** as a third-party evaluation framework to initiate multimodal model evaluation tasks.
- ğŸ”¥ **[2024.06.29]** Support for **OpenCompass** as a third-party evaluation framework which we have encapsulated at a higher level supporting pip installation and simplifying evaluation task configuration.
- ğŸ”¥ **[2024.06.13]** EvalScope seamlessly integrates with the fine-tuning framework SWIFT providing full-chain support from LLM training to evaluation.
- ğŸ”¥ **[2024.06.13]** Integrated the Agent evaluation dataset ToolBench.

</details>

## â¤ï¸ Community & Support

Welcome to join our community to communicate with other developers and get help.

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  WeChat Group | DingTalk Group
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ Environment Setup

We recommend using `conda` to create a virtual environment and install with `pip`.

1.  **Create and Activate Conda Environment** (Python 3.10 recommended)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **Install EvalScope**

    - **Method 1: Install via PyPI (Recommended)**
      ```shell
      pip install evalscope
      ```

    - **Method 2: Install from Source (For Development)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **Install Additional Dependencies** (Optional)
    Install corresponding feature extensions according to your needs:
    ```shell
    # Performance testing
    pip install 'evalscope[perf]'

    # Visualization App
    pip install 'evalscope[app]'

    # Other evaluation backends
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # Install all dependencies
    pip install 'evalscope[all]'
    ```
    > If you installed from source please replace `evalscope` with `.` for example `pip install '.[perf]'`.

> [!NOTE]
> This project was formerly known as `llmuses`. If you need to use `v0.4.3` or earlier versions please run `pip install llmuses<=0.4.3` and use `from llmuses import ...` for imports.


## ğŸš€ Quick Start

You can start evaluation tasks in two ways: **command line** or **Python code**.

### Method 1. Using Command Line

Execute the `evalscope eval` command in any path to start evaluation. The following command will evaluate the `Qwen/Qwen2.5-0.5B-Instruct` model on `gsm8k` and `arc` datasets taking only 5 samples from each dataset.

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### Method 2. Using Python Code

Use the `run_task` function and `TaskConfig` object to configure and start evaluation tasks.

```python
from evalscope import run_task TaskConfig

# Configure evaluation task
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# Start evaluation
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ Tip:</b> `run_task` also supports dictionaries YAML or JSON files as configuration.</summary>

**Using Python Dictionary**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**Using YAML File** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### Output Results
After evaluation completion you will see a report in the terminal in the following format:
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ Advanced Usage

### Custom Evaluation Parameters

You can fine-tune model loading inference and dataset configuration through command line parameters.

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: Model loading parameters such as `revision` `precision` etc.
- `--generation-config`: Model generation parameters such as `temperature` `max_tokens` etc.
- `--dataset-args`: Dataset configuration parameters such as `few_shot_num` etc.

For details please refer to [ğŸ“– Complete Parameter Guide](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).

### Evaluating Online Model APIs

EvalScope supports evaluating model services deployed via APIs (such as services deployed with vLLM). Simply specify the service address and API Key.

1.  **Start Model Service** (using vLLM as example)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **Run Evaluation**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ Arena Mode

Arena mode evaluates model performance through pairwise battles between models providing win rates and rankings perfect for horizontal comparison of multiple models.

```text
# Example evaluation results
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
For details please refer to [ğŸ“– Arena Mode Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html).

### ğŸ–Šï¸ Custom Dataset Evaluation

EvalScope allows you to easily add and evaluate your own datasets. For details please refer to [ğŸ“– Custom Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html).


## ğŸ§ª Other Evaluation Backends
EvalScope supports launching evaluation tasks through third-party evaluation frameworks (we call them "backends") to meet diverse evaluation needs.

- **Native**: EvalScope's default evaluation framework with comprehensive functionality.
- **OpenCompass**: Focuses on text-only evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: Focuses on multi-modal evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: Focuses on RAG evaluation supporting Embedding and Reranker models. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/index.html)
- **Third-party Evaluation Tools**: Supports evaluation tasks like [ToolBench](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html).

## âš¡ Inference Performance Evaluation Tool
EvalScope provides a powerful stress testing tool for evaluating the performance of large language model services.

- **Key Metrics**: Supports throughput (Tokens/s) first token latency (TTFT) token generation latency (TPOT) etc.
- **Result Recording**: Supports recording results to `wandb` and `swanlab`.
- **Speed Benchmarks**: Can generate speed benchmark results similar to official reports.

For details please refer to [ğŸ“– Performance Testing Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).

Example output is shown below:
<p align="center">
    <img src="docs/en/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š Visualizing Evaluation Results

EvalScope provides a Gradio-based WebUI for interactive analysis and comparison of evaluation results.

1.  **Install Dependencies**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **Start Service**
    ```bash
    evalscope app
    ```
    Visit `http://127.0.0.1:7861` to open the visualization interface.

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/setting.png" alt="Setting" style="width: 85%;" />
      <p>Settings Interface</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>Model Comparison</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>Report Overview</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_details.png" alt="Report Details" style="width: 85%;" />
      <p>Report Details</p>
    </td>
  </tr>
</table>

For details please refer to [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html).

## ğŸ‘·â€â™‚ï¸ Contributing

We welcome any contributions from the community! If you want to add new evaluation benchmarks models or features please refer to our [Contributing Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html).

Thanks to all developers who have contributed to EvalScope!

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š Citation

If you use EvalScope in your research please cite our work:
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  ä¸­æ–‡ &nbsp ï½œ &nbsp <a href="evalscope.md">English</a> &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/zh-cn/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  ä¸­æ–‡æ–‡æ¡£</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documents</a>
<p>


> â­ å¦‚æœä½ å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ "Star" æŒ‰é’®æ”¯æŒæˆ‘ä»¬ã€‚ä½ çš„æ”¯æŒæ˜¯æˆ‘ä»¬å‰è¿›çš„åŠ¨åŠ›ï¼

## ğŸ“ ç®€ä»‹

EvalScope æ˜¯ç”±[é­”æ­ç¤¾åŒº](https://modelscope.cn/)æ‰“é€ çš„ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§ã€æ˜“äºæ‰©å±•çš„æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¤§æ¨¡å‹å¼€å‘è€…æä¾›ä¸€ç«™å¼è¯„æµ‹è§£å†³æ–¹æ¡ˆã€‚

æ— è®ºæ‚¨æ˜¯æƒ³è¯„ä¼°æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€è¿›è¡Œå¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼Œè¿˜æ˜¯éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼ŒEvalScope éƒ½èƒ½æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- **ğŸ“š å…¨é¢çš„è¯„æµ‹åŸºå‡†**: å†…ç½® MMLU C-Eval GSM8K ç­‰å¤šä¸ªä¸šç•Œå…¬è®¤çš„è¯„æµ‹åŸºå‡†ã€‚
- **ğŸ§© å¤šæ¨¡æ€ä¸å¤šé¢†åŸŸæ”¯æŒ**: æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ (LLM)ã€å¤šæ¨¡æ€ (VLM)ã€Embeddingã€Rerankerã€AIGC ç­‰å¤šç§æ¨¡å‹çš„è¯„æµ‹ã€‚
- **ğŸš€ å¤šåç«¯é›†æˆ**: æ— ç¼é›†æˆ OpenCompass VLMEvalKit RAGEval ç­‰å¤šç§è¯„æµ‹åç«¯ï¼Œæ»¡è¶³ä¸åŒè¯„æµ‹éœ€æ±‚ã€‚
- **âš¡ æ¨ç†æ€§èƒ½æµ‹è¯•**: æä¾›å¼ºå¤§çš„æ¨¡å‹æœåŠ¡å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒ TTFT TPOT ç­‰å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ã€‚
- **ğŸ“Š äº¤äº’å¼æŠ¥å‘Š**: æä¾› WebUI å¯è§†åŒ–ç•Œé¢ï¼Œæ”¯æŒå¤šç»´åº¦æ¨¡å‹å¯¹æ¯”ã€æŠ¥å‘Šæ¦‚è§ˆå’Œè¯¦æƒ…æŸ¥é˜…ã€‚
- **âš”ï¸ ç«æŠ€åœºæ¨¡å¼**: æ”¯æŒå¤šæ¨¡å‹å¯¹æˆ˜ (Pairwise Battle)ï¼Œç›´è§‚åœ°å¯¹æ¨¡å‹è¿›è¡Œæ’åå’Œè¯„ä¼°ã€‚
- **ğŸ”§ é«˜åº¦å¯æ‰©å±•**: å¼€å‘è€…å¯ä»¥è½»æ¾æ·»åŠ è‡ªå®šä¹‰æ•°æ®é›†ã€æ¨¡å‹å’Œè¯„æµ‹æŒ‡æ ‡ã€‚

<details><summary>ğŸ›ï¸ æ•´ä½“æ¶æ„</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope æ•´ä½“æ¶æ„å›¾.
</p>

1.  **è¾“å…¥å±‚**
    - **æ¨¡å‹æ¥æº**: APIæ¨¡å‹ï¼ˆOpenAI APIï¼‰ã€æœ¬åœ°æ¨¡å‹ï¼ˆModelScopeï¼‰
    - **æ•°æ®é›†**: æ ‡å‡†è¯„æµ‹åŸºå‡†ï¼ˆMMLU/GSM8kç­‰ï¼‰ã€è‡ªå®šä¹‰æ•°æ®ï¼ˆMCQ/QAï¼‰

2.  **æ ¸å¿ƒåŠŸèƒ½**
    - **å¤šåç«¯è¯„ä¼°**: åŸç”Ÿåç«¯ã€OpenCompassã€MTEBã€VLMEvalKitã€RAGAS
    - **æ€§èƒ½ç›‘æ§**: æ”¯æŒå¤šç§æ¨¡å‹æœåŠ¡ API å’Œæ•°æ®æ ¼å¼ï¼Œè¿½è¸ª TTFT/TPOP ç­‰æŒ‡æ ‡
    - **å·¥å…·æ‰©å±•**: é›†æˆ Tool-Bench Needle-in-a-Haystack ç­‰

3.  **è¾“å‡ºå±‚**
    - **ç»“æ„åŒ–æŠ¥å‘Š**: æ”¯æŒ JSON Table Logs
    - **å¯è§†åŒ–å¹³å°**: æ”¯æŒ Gradio Wandb SwanLab

</details>

## ğŸ‰ å†…å®¹æ›´æ–°

> [!IMPORTANT]
> **ç‰ˆæœ¬ 1.0 é‡æ„**
>
> ç‰ˆæœ¬ 1.0 å¯¹è¯„æµ‹æ¡†æ¶è¿›è¡Œäº†é‡å¤§é‡æ„ï¼Œåœ¨ `evalscope/api` ä¸‹å»ºç«‹äº†å…¨æ–°çš„ã€æ›´æ¨¡å—åŒ–ä¸”æ˜“æ‰©å±•çš„ API å±‚ã€‚ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼šä¸ºåŸºå‡†ã€æ ·æœ¬å’Œç»“æœå¼•å…¥äº†æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹ï¼›å¯¹åŸºå‡†å’ŒæŒ‡æ ‡ç­‰ç»„ä»¶é‡‡ç”¨æ³¨å†Œè¡¨å¼è®¾è®¡ï¼›å¹¶é‡å†™äº†æ ¸å¿ƒè¯„æµ‹å™¨ä»¥ååŒæ–°æ¶æ„ã€‚ç°æœ‰çš„åŸºå‡†å·²è¿ç§»åˆ°è¿™ä¸€ APIï¼Œå®ç°æ›´åŠ ç®€æ´ã€ä¸€è‡´ä¸”æ˜“äºç»´æŠ¤ã€‚

- ğŸ”¥ **[2025.12.02]** æ”¯æŒè‡ªå®šä¹‰å¤šæ¨¡æ€VQAè¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/vlm.html) ï¼›æ”¯æŒæ¨¡å‹æœåŠ¡å‹æµ‹åœ¨ ClearML ä¸Šå¯è§†åŒ–ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#clearml)ã€‚
- ğŸ”¥ **[2025.11.26]** æ–°å¢æ”¯æŒ OpenAI-MRCRã€GSM8K-Vã€MGSMã€MicroVQAã€IFBenchã€SciCode è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.18]** æ”¯æŒè‡ªå®šä¹‰ Function-Callï¼ˆå·¥å…·è°ƒç”¨ï¼‰æ•°æ®é›†ï¼Œæ¥æµ‹è¯•æ¨¡å‹èƒ½å¦é€‚æ—¶å¹¶æ­£ç¡®è°ƒç”¨å·¥å…·ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#fc)
- ğŸ”¥ **[2025.11.14]** æ–°å¢æ”¯æŒSWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini ä»£ç è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/swe_bench.html)ã€‚
- ğŸ”¥ **[2025.11.12]** æ–°å¢`pass@k`ã€`vote@k`ã€`pass^k`ç­‰æŒ‡æ ‡èšåˆæ–¹æ³•ï¼›æ–°å¢æ”¯æŒA_OKVQA CMMU ScienceQ V*Benchç­‰å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.07]** æ–°å¢æ”¯æŒÏ„Â²-benchï¼Œæ˜¯ Ï„-bench çš„æ‰©å±•ä¸å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«ä¸€ç³»åˆ—ä»£ç ä¿®å¤ï¼Œå¹¶æ–°å¢äº†ç”µä¿¡ï¼ˆtelecomï¼‰é¢†åŸŸçš„æ•…éšœæ’æŸ¥åœºæ™¯ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/tau2_bench.html)ã€‚
- ğŸ”¥ **[2025.10.30]** æ–°å¢æ”¯æŒBFCL-v4ï¼Œæ”¯æŒagentçš„ç½‘ç»œæœç´¢å’Œé•¿æœŸè®°å¿†èƒ½åŠ›çš„è¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v4.html)ã€‚
- ğŸ”¥ **[2025.10.27]** æ–°å¢æ”¯æŒLogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQAç­‰è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.26]** æ–°å¢æ”¯æŒConll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 ç­‰å‘½åå®ä½“è¯†åˆ«è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.21]** ä¼˜åŒ–ä»£ç è¯„æµ‹ä¸­çš„æ²™ç®±ç¯å¢ƒä½¿ç”¨ï¼Œæ”¯æŒåœ¨æœ¬åœ°å’Œè¿œç¨‹ä¸¤ç§æ¨¡å¼ä¸‹è¿è¡Œï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/sandbox.html)ã€‚
- ğŸ”¥ **[2025.10.20]** æ–°å¢æ”¯æŒPolyMath SimpleVQA MathVerse MathVision AA-LCR ç­‰è¯„æµ‹åŸºå‡†ï¼›ä¼˜åŒ–evalscope perfè¡¨ç°ï¼Œå¯¹é½vLLM Benchï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/vs_vllm_bench.html)ã€‚
- ğŸ”¥ **[2025.10.14]** æ–°å¢æ”¯æŒOCRBench OCRBench-v2 DocVQA InfoVQA ChartQA BLINK ç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.22]** ä»£ç è¯„æµ‹åŸºå‡†(HumanEval LiveCodeBench)æ”¯æŒåœ¨æ²™ç®±ç¯å¢ƒä¸­è¿è¡Œï¼Œè¦ä½¿ç”¨è¯¥åŠŸèƒ½éœ€å…ˆå®‰è£…[ms-enclave](https://github.com/modelscope/ms-enclave)ã€‚
- ğŸ”¥ **[2025.09.19]** æ–°å¢æ”¯æŒRealWorldQAã€AI2Dã€MMStarã€MMBenchã€OmniBenchç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼Œå’ŒMulti-IFã€HealthBenchã€AMCç­‰çº¯æ–‡æœ¬è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.05]** æ”¯æŒè§†è§‰-è¯­è¨€å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è¯„æµ‹ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šMathVistaã€MMMUï¼Œæ›´å¤šæ”¯æŒæ•°æ®é›†è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/vlm.html)ã€‚
- ğŸ”¥ **[2025.09.04]** æ”¯æŒå›¾åƒç¼–è¾‘ä»»åŠ¡è¯„æµ‹ï¼Œæ”¯æŒ[GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/image_edit.html)ã€‚
- ğŸ”¥ **[2025.08.22]** Version 1.0 é‡æ„ï¼Œä¸å…¼å®¹çš„æ›´æ–°è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#v1-0)ã€‚
<details> <summary>æ›´å¤š</summary>

- ğŸ”¥ **[2025.07.18]** æ¨¡å‹å‹æµ‹æ”¯æŒéšæœºç”Ÿæˆå›¾æ–‡æ•°æ®ï¼Œç”¨äºå¤šæ¨¡æ€æ¨¡å‹å‹æµ‹ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#id4)ã€‚
- ğŸ”¥ **[2025.07.16]** æ”¯æŒ[Ï„-bench](https://github.com/sierra-research/tau-bench)ï¼Œç”¨äºè¯„ä¼° AI Agentåœ¨åŠ¨æ€ç”¨æˆ·å’Œå·¥å…·äº¤äº’çš„å®é™…ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#bench)ã€‚
- ğŸ”¥ **[2025.07.14]** æ”¯æŒâ€œäººç±»æœ€åçš„è€ƒè¯•â€([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle))ï¼Œè¿™ä¸€é«˜éš¾åº¦è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam)ã€‚
- ğŸ”¥ **[2025.07.03]** é‡æ„äº†ç«æŠ€åœºæ¨¡å¼ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å¯¹æˆ˜ï¼Œè¾“å‡ºæ¨¡å‹æ’è¡Œæ¦œï¼Œä»¥åŠå¯¹æˆ˜ç»“æœå¯è§†åŒ–ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚
- ğŸ”¥ **[2025.06.28]** ä¼˜åŒ–è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼Œæ”¯æŒæ— å‚è€ƒç­”æ¡ˆè¯„æµ‹ï¼›ä¼˜åŒ–LLMè£åˆ¤ä½¿ç”¨ï¼Œé¢„ç½®â€œæ— å‚è€ƒç­”æ¡ˆç›´æ¥æ‰“åˆ†â€ å’Œ â€œåˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´â€ä¸¤ç§æ¨¡å¼ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#qa)
- ğŸ”¥ **[2025.06.19]** æ–°å¢æ”¯æŒ[BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3)è¯„æµ‹åŸºå‡†ï¼Œç”¨äºè¯„æµ‹æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v3.html)ã€‚
- ğŸ”¥ **[2025.06.02]** æ–°å¢æ”¯æŒå¤§æµ·æé’ˆæµ‹è¯•ï¼ˆNeedle-in-a-Haystackï¼‰ï¼ŒæŒ‡å®š`needle_haystack`å³å¯è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åœ¨`outputs/reports`æ–‡ä»¶å¤¹ä¸‹ç”Ÿæˆå¯¹åº”çš„heatmapï¼Œç›´è§‚å±•ç°æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/needle_haystack.html)ã€‚
- ğŸ”¥ **[2025.05.29]** æ–°å¢æ”¯æŒ[DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary)å’Œ[FRAMES](https://modelscope.cn/datasets/iic/frames/summary)ä¸¤ä¸ªé•¿æ–‡æ¡£è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.05.16]** æ¨¡å‹æœåŠ¡æ€§èƒ½å‹æµ‹æ”¯æŒè®¾ç½®å¤šç§å¹¶å‘ï¼Œå¹¶è¾“å‡ºæ€§èƒ½å‹æµ‹æŠ¥å‘Šï¼Œ[å‚è€ƒç¤ºä¾‹](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/quick_start.html#id3)ã€‚
- ğŸ”¥ **[2025.05.13]** æ–°å¢æ”¯æŒ[ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static)æ•°æ®é›†ï¼Œè¯„æµ‹æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html)ï¼›æ”¯æŒ[DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview)å’Œ[Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val)è¯„æµ‹åŸºå‡†ï¼Œè¯„æµ‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
- ğŸ”¥ **[2025.04.29]** æ–°å¢Qwen3è¯„æµ‹æœ€ä½³å®è·µï¼Œ[æ¬¢è¿é˜…è¯»ğŸ“–](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** æ”¯æŒæ–‡ç”Ÿå›¾è¯„æµ‹ï¼šæ”¯æŒMPSã€HPSv2.1Scoreç­‰8ä¸ªæŒ‡æ ‡ï¼Œæ”¯æŒEvalMuseã€GenAI-Benchç­‰è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/t2i.html)
- ğŸ”¥ **[2025.04.10]** æ¨¡å‹æœåŠ¡å‹æµ‹å·¥å…·æ”¯æŒ`/v1/completions`ç«¯ç‚¹ï¼ˆä¹Ÿæ˜¯vLLMåŸºå‡†æµ‹è¯•çš„é»˜è®¤ç«¯ç‚¹ï¼‰
- ğŸ”¥ **[2025.04.08]** æ”¯æŒOpenAI APIå…¼å®¹çš„Embeddingæ¨¡å‹æœåŠ¡è¯„æµ‹ï¼ŒæŸ¥çœ‹[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters)
- ğŸ”¥ **[2025.03.27]** æ–°å¢æ”¯æŒ[AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview)å’Œ[ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary)è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** æ¨¡å‹æ¨ç†æœåŠ¡å‹æµ‹æ”¯æŒrandomç”ŸæˆæŒ‡å®šèŒƒå›´é•¿åº¦çš„promptï¼Œå‚è€ƒ[ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#random)
- ğŸ”¥ **[2025.03.13]** æ–°å¢æ”¯æŒ[LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary)ä»£ç è¯„æµ‹åŸºå‡†ï¼ŒæŒ‡å®š`live_code_bench`å³å¯ä½¿ç”¨ï¼›æ”¯æŒQwQ-32B åœ¨LiveCodeBenchä¸Šè¯„æµ‹ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.11]** æ–°å¢æ”¯æŒ[SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary)å’Œ[Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary)è¯„æµ‹åŸºå‡†ï¼Œç”¨ä¸è¯„æµ‹æ¨¡å‹çš„äº‹å®æ­£ç¡®æ€§ï¼ŒæŒ‡å®š`simple_qa`å’Œ`chinese_simpleqa`ä½¿ç”¨ã€‚åŒæ—¶æ”¯æŒæŒ‡å®šè£åˆ¤æ¨¡å‹ï¼Œå‚è€ƒ[ç›¸å…³å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚
- ğŸ”¥ **[2025.03.07]** æ–°å¢QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µï¼Œè¯„æµ‹äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»¥åŠæ¨ç†æ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.04]** æ–°å¢æ”¯æŒ[SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary)æ•°æ®é›†ï¼Œå…¶è¦†ç›– 13 ä¸ªé—¨ç±»ã€72 ä¸ªä¸€çº§å­¦ç§‘å’Œ 285 ä¸ªäºŒçº§å­¦ç§‘ï¼Œå…± 26529 ä¸ªé—®é¢˜ï¼ŒæŒ‡å®š`super_gpqa`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.03.03]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨¡å‹çš„æ™ºå•†å’Œæƒ…å•†ï¼Œå‚è€ƒ[ğŸ“–æ™ºå•†å’Œæƒ…å•†è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/iquiz.html)ï¼Œæ¥æµ‹æµ‹ä½ å®¶çš„AIæœ‰å¤šèªæ˜ï¼Ÿ
- ğŸ”¥ **[2025.02.27]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨ç†æ¨¡å‹çš„æ€è€ƒæ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–æ€è€ƒæ•ˆç‡è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/think_eval.html)ï¼Œè¯¥å®ç°å‚è€ƒäº†[Overthinking](https://doi.org/10.48550/arXiv.2412.21187) å’Œ [Underthinking](https://doi.org/10.48550/arXiv.2501.18585)ä¸¤ç¯‡å·¥ä½œã€‚
- ğŸ”¥ **[2025.02.25]** æ–°å¢æ”¯æŒ[MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR)å’Œ[ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary)ä¸¤ä¸ªæ¨¡å‹æ¨ç†ç›¸å…³è¯„æµ‹åŸºå‡†ï¼Œdatasetsåˆ†åˆ«æŒ‡å®š`musr`å’Œ`process_bench`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.02.18]** æ”¯æŒAIME25æ•°æ®é›†ï¼ŒåŒ…å«15é“é¢˜ç›®ï¼ˆGrok3 åœ¨è¯¥æ•°æ®é›†ä¸Šå¾—åˆ†ä¸º93åˆ†ï¼‰
- ğŸ”¥ **[2025.02.13]** æ”¯æŒDeepSeekè’¸é¦æ¨¡å‹è¯„æµ‹ï¼ŒåŒ…æ‹¬AIME24 MATH-500 GPQA-Diamondæ•°æ®é›†ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/deepseek_r1_distill.html)ï¼›æ”¯æŒæŒ‡å®š`eval_batch_size`å‚æ•°ï¼ŒåŠ é€Ÿæ¨¡å‹è¯„æµ‹
- ğŸ”¥ **[2025.01.20]** æ”¯æŒå¯è§†åŒ–è¯„æµ‹ç»“æœï¼ŒåŒ…æ‹¬å•æ¨¡å‹è¯„æµ‹ç»“æœå’Œå¤šæ¨¡å‹è¯„æµ‹ç»“æœå¯¹æ¯”ï¼Œå‚è€ƒ[ğŸ“–å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ï¼›æ–°å¢[`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary)è¯„æµ‹æ ·ä¾‹ï¼Œè¯„æµ‹æ¨¡å‹çš„IQå’ŒEQã€‚
- ğŸ”¥ **[2025.01.07]** Native backend: æ”¯æŒæ¨¡å‹APIè¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–æ¨¡å‹APIè¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#api)ï¼›æ–°å¢æ”¯æŒ`ifeval`è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ğŸ”¥ **[2024.12.31]** æ”¯æŒåŸºå‡†è¯„æµ‹æ·»åŠ ï¼Œå‚è€ƒ[ğŸ“–åŸºå‡†è¯„æµ‹æ·»åŠ æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ï¼›æ”¯æŒè‡ªå®šä¹‰æ··åˆæ•°æ®é›†è¯„æµ‹ï¼Œç”¨æ›´å°‘çš„æ•°æ®ï¼Œæ›´å…¨é¢çš„è¯„æµ‹æ¨¡å‹ï¼Œå‚è€ƒ[ğŸ“–æ··åˆæ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/collection/index.html)
- ğŸ”¥ **[2024.12.13]** æ¨¡å‹è¯„æµ‹ä¼˜åŒ–ï¼Œä¸å†éœ€è¦ä¼ é€’`--template-type`å‚æ•°ï¼›æ”¯æŒ`evalscope eval --args`å¯åŠ¨è¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html)
- ğŸ”¥ **[2024.11.26]** æ¨¡å‹æ¨ç†å‹æµ‹å·¥å…·é‡æ„å®Œæˆï¼šæ”¯æŒæœ¬åœ°å¯åŠ¨æ¨ç†æœåŠ¡ã€æ”¯æŒSpeed Benchmarkï¼›ä¼˜åŒ–å¼‚æ­¥è°ƒç”¨é”™è¯¯å¤„ç†ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)
- ğŸ”¥ **[2024.10.31]** å¤šæ¨¡æ€RAGè¯„æµ‹æœ€ä½³å®è·µå‘å¸ƒï¼Œå‚è€ƒ[ğŸ“–åšå®¢](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag)
- ğŸ”¥ **[2024.10.23]** æ”¯æŒå¤šæ¨¡æ€RAGè¯„æµ‹ï¼ŒåŒ…æ‹¬[CLIP_Benchmark](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/clip_benchmark.html)è¯„æµ‹å›¾æ–‡æ£€ç´¢å™¨ï¼Œä»¥åŠæ‰©å±•äº†[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)ä»¥æ”¯æŒç«¯åˆ°ç«¯å¤šæ¨¡æ€æŒ‡æ ‡è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.10.8]** æ”¯æŒRAGè¯„æµ‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨[MTEB/CMTEB](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html)è¿›è¡Œembeddingæ¨¡å‹å’Œrerankerçš„ç‹¬ç«‹è¯„æµ‹ï¼Œä»¥åŠä½¿ç”¨[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)è¿›è¡Œç«¯åˆ°ç«¯è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.09.18]** æˆ‘ä»¬çš„æ–‡æ¡£å¢åŠ äº†åšå®¢æ¨¡å—ï¼ŒåŒ…å«ä¸€äº›è¯„æµ‹ç›¸å…³çš„æŠ€æœ¯è°ƒç ”å’Œåˆ†äº«ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/blog/index.html)
- ğŸ”¥ **[2024.09.12]** æ”¯æŒ LongWriter è¯„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºå‡†æµ‹è¯• [LongBench-Write](evalscope/third_party/longbench_write/README.md) æ¥è¯„æµ‹é•¿è¾“å‡ºçš„è´¨é‡ä»¥åŠè¾“å‡ºé•¿åº¦ã€‚
- ğŸ”¥ **[2024.08.30]** æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ•°æ®é›†å’Œå¤šæ¨¡æ€å›¾æ–‡æ•°æ®é›†ã€‚
- ğŸ”¥ **[2024.08.20]** æ›´æ–°äº†å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…æ‹¬å¿«é€Ÿä¸Šæ‰‹ã€æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜ç­‰ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/)ã€‚
- ğŸ”¥ **[2024.08.09]** ç®€åŒ–å®‰è£…æ–¹å¼ï¼Œæ”¯æŒpypiå®‰è£…vlmevalç›¸å…³ä¾èµ–ï¼›ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä½“éªŒï¼ŒåŸºäºOpenAI APIæ–¹å¼çš„è¯„æµ‹é“¾è·¯ï¼Œæœ€é«˜åŠ é€Ÿ10å€ã€‚
- ğŸ”¥ **[2024.07.31]** é‡è¦ä¿®æ”¹ï¼š`llmuses`åŒ…åä¿®æ”¹ä¸º`evalscope`ï¼Œè¯·åŒæ­¥ä¿®æ”¹æ‚¨çš„ä»£ç ã€‚
- ğŸ”¥ **[2024.07.26]** æ”¯æŒ**VLMEvalKit**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œå‘èµ·å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä»»åŠ¡ã€‚
- ğŸ”¥ **[2024.06.29]** æ”¯æŒ**OpenCompass**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†é«˜çº§å°è£…ï¼Œæ”¯æŒpipæ–¹å¼å®‰è£…ï¼Œç®€åŒ–äº†è¯„æµ‹ä»»åŠ¡é…ç½®ã€‚
- ğŸ”¥ **[2024.06.13]** EvalScopeä¸å¾®è°ƒæ¡†æ¶SWIFTè¿›è¡Œæ— ç¼å¯¹æ¥ï¼Œæä¾›LLMä»è®­ç»ƒåˆ°è¯„æµ‹çš„å…¨é“¾è·¯æ”¯æŒ ã€‚
- ğŸ”¥ **[2024.06.13]** æ¥å…¥Agentè¯„æµ‹é›†ToolBenchã€‚
</details>

## â¤ï¸ ç¤¾åŒºä¸æ”¯æŒ

æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºï¼Œä¸å…¶ä»–å¼€å‘è€…äº¤æµå¹¶è·å–å¸®åŠ©ã€‚

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  å¾®ä¿¡ç¾¤ | é’‰é’‰ç¾¤
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬æ¨èä½¿ç”¨ `conda` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä½¿ç”¨ `pip` å®‰è£…ã€‚

1.  **åˆ›å»ºå¹¶æ¿€æ´» Conda ç¯å¢ƒ** (æ¨èä½¿ç”¨ Python 3.10)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **å®‰è£… EvalScope**

    - **æ–¹å¼ä¸€ï¼šé€šè¿‡ PyPI å®‰è£… (æ¨è)**
      ```shell
      pip install evalscope
      ```

    - **æ–¹å¼äºŒï¼šé€šè¿‡æºç å®‰è£… (ç”¨äºå¼€å‘)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **å®‰è£…é¢å¤–ä¾èµ–** (å¯é€‰)
    æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œå®‰è£…ç›¸åº”çš„åŠŸèƒ½æ‰©å±•ï¼š
    ```shell
    # æ€§èƒ½æµ‹è¯•
    pip install 'evalscope[perf]'

    # å¯è§†åŒ–App
    pip install 'evalscope[app]'

    # å…¶ä»–è¯„æµ‹åç«¯
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # å®‰è£…æ‰€æœ‰ä¾èµ–
    pip install 'evalscope[all]'
    ```
    > å¦‚æœæ‚¨é€šè¿‡æºç å®‰è£…ï¼Œè¯·å°† `evalscope` æ›¿æ¢ä¸º `.`ï¼Œä¾‹å¦‚ `pip install '.[perf]'`ã€‚

> [!NOTE]
> æœ¬é¡¹ç›®æ›¾ç”¨å `llmuses`ã€‚å¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ `v0.4.3` æˆ–æ›´æ—©ç‰ˆæœ¬ï¼Œè¯·è¿è¡Œ `pip install llmuses<=0.4.3` å¹¶ä½¿ç”¨ `from llmuses import ...` å¯¼å…¥ã€‚


## ğŸš€ å¿«é€Ÿå¼€å§‹

æ‚¨å¯ä»¥é€šè¿‡**å‘½ä»¤è¡Œ**æˆ– **Python ä»£ç **ä¸¤ç§æ–¹å¼å¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

### æ–¹å¼1. ä½¿ç”¨å‘½ä»¤è¡Œ

åœ¨ä»»æ„è·¯å¾„ä¸‹æ‰§è¡Œ `evalscope eval` å‘½ä»¤å³å¯å¼€å§‹è¯„æµ‹ã€‚ä»¥ä¸‹å‘½ä»¤å°†åœ¨ `gsm8k` å’Œ `arc` æ•°æ®é›†ä¸Šè¯„æµ‹ `Qwen/Qwen2.5-0.5B-Instruct` æ¨¡å‹ï¼Œæ¯ä¸ªæ•°æ®é›†åªå– 5 ä¸ªæ ·æœ¬ã€‚

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### æ–¹å¼2. ä½¿ç”¨Pythonä»£ç 

ä½¿ç”¨ `run_task` å‡½æ•°å’Œ `TaskConfig` å¯¹è±¡æ¥é…ç½®å’Œå¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

```python
from evalscope import run_task TaskConfig

# é…ç½®è¯„æµ‹ä»»åŠ¡
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# å¯åŠ¨è¯„æµ‹
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ æç¤ºï¼š</b> `run_task` è¿˜æ”¯æŒå­—å…¸ã€YAML æˆ– JSON æ–‡ä»¶ä½œä¸ºé…ç½®ã€‚</summary>

**ä½¿ç”¨ Python å­—å…¸**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**ä½¿ç”¨ YAML æ–‡ä»¶** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### è¾“å‡ºç»“æœ
è¯„æµ‹å®Œæˆåï¼Œæ‚¨å°†åœ¨ç»ˆç«¯çœ‹åˆ°å¦‚ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ è¿›é˜¶ç”¨æ³•

### è‡ªå®šä¹‰è¯„æµ‹å‚æ•°

æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ç²¾ç»†åŒ–æ§åˆ¶æ¨¡å‹åŠ è½½ã€æ¨ç†å’Œæ•°æ®é›†é…ç½®ã€‚

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: æ¨¡å‹åŠ è½½å‚æ•°ï¼Œå¦‚ `revision` `precision` ç­‰ã€‚
- `--generation-config`: æ¨¡å‹ç”Ÿæˆå‚æ•°ï¼Œå¦‚ `temperature` `max_tokens` ç­‰ã€‚
- `--dataset-args`: æ•°æ®é›†é…ç½®å‚æ•°ï¼Œå¦‚ `few_shot_num` ç­‰ã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å…¨éƒ¨å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚

### è¯„æµ‹åœ¨çº¿æ¨¡å‹ API

EvalScope æ”¯æŒè¯„æµ‹é€šè¿‡ API éƒ¨ç½²çš„æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ vLLM éƒ¨ç½²çš„æœåŠ¡ï¼‰ã€‚åªéœ€æŒ‡å®šæœåŠ¡åœ°å€å’Œ API Key å³å¯ã€‚

1.  **å¯åŠ¨æ¨¡å‹æœåŠ¡** (ä»¥ vLLM ä¸ºä¾‹)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **è¿è¡Œè¯„æµ‹**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ ç«æŠ€åœºæ¨¡å¼ (Arena)

ç«æŠ€åœºæ¨¡å¼é€šè¿‡æ¨¡å‹é—´çš„ä¸¤ä¸¤å¯¹æˆ˜ï¼ˆPairwise Battleï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ç»™å‡ºèƒœç‡å’Œæ’åï¼Œéå¸¸é€‚åˆå¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”ã€‚

```text
# è¯„æµ‹ç»“æœç¤ºä¾‹
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– ç«æŠ€åœºæ¨¡å¼ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚

### ğŸ–Šï¸ è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹

EvalScope å…è®¸æ‚¨è½»æ¾æ·»åŠ å’Œè¯„æµ‹è‡ªå·±çš„æ•°æ®é›†ã€‚è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/index.html)ã€‚


## ğŸ§ª å…¶ä»–è¯„æµ‹åç«¯
EvalScope æ”¯æŒé€šè¿‡ç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåç«¯â€ï¼‰å‘èµ·è¯„æµ‹ä»»åŠ¡ï¼Œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„è¯„æµ‹éœ€æ±‚ã€‚

- **Native**: EvalScope çš„é»˜è®¤è¯„æµ‹æ¡†æ¶ï¼ŒåŠŸèƒ½å…¨é¢ã€‚
- **OpenCompass**: ä¸“æ³¨äºçº¯æ–‡æœ¬è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: ä¸“æ³¨äºå¤šæ¨¡æ€è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: ä¸“æ³¨äº RAG è¯„æµ‹ï¼Œæ”¯æŒ Embedding å’Œ Reranker æ¨¡å‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/index.html)
- **ç¬¬ä¸‰æ–¹è¯„æµ‹å·¥å…·**: æ”¯æŒ [ToolBench](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html) ç­‰è¯„æµ‹ä»»åŠ¡ã€‚

## âš¡ æ¨ç†æ€§èƒ½è¯„æµ‹å·¥å…·
EvalScope æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„æ€§èƒ½ã€‚

- **å…³é”®æŒ‡æ ‡**: æ”¯æŒååé‡ (Tokens/s)ã€é¦–å­—å»¶è¿Ÿ (TTFT)ã€Token ç”Ÿæˆå»¶è¿Ÿ (TPOT) ç­‰ã€‚
- **ç»“æœè®°å½•**: æ”¯æŒå°†ç»“æœè®°å½•åˆ° `wandb` å’Œ `swanlab`ã€‚
- **é€Ÿåº¦åŸºå‡†**: å¯ç”Ÿæˆç±»ä¼¼å®˜æ–¹æŠ¥å‘Šçš„é€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– æ€§èƒ½æµ‹è¯•ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)ã€‚

è¾“å‡ºç¤ºä¾‹å¦‚ä¸‹ï¼š
<p align="center">
    <img src="docs/zh/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š å¯è§†åŒ–è¯„æµ‹ç»“æœ

EvalScope æä¾›äº†ä¸€ä¸ªåŸºäº Gradio çš„ WebUIï¼Œç”¨äºäº¤äº’å¼åœ°åˆ†æå’Œæ¯”è¾ƒè¯„æµ‹ç»“æœã€‚

1.  **å®‰è£…ä¾èµ–**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **å¯åŠ¨æœåŠ¡**
    ```bash
    evalscope app
    ```
    è®¿é—® `http://127.0.0.1:7861` å³å¯æ‰“å¼€å¯è§†åŒ–ç•Œé¢ã€‚

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/setting.png" alt="Setting" style="width: 90%;" />
      <p>è®¾ç½®ç•Œé¢</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>æ¨¡å‹æ¯”è¾ƒ</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>æŠ¥å‘Šæ¦‚è§ˆ</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_details.png" alt="Report Details" style="width: 91%;" />
      <p>æŠ¥å‘Šè¯¦æƒ…</p>
    </td>
  </tr>
</table>

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ã€‚

## ğŸ‘·â€â™‚ï¸ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ¥è‡ªç¤¾åŒºçš„ä»»ä½•è´¡çŒ®ï¼å¦‚æœæ‚¨å¸Œæœ›æ·»åŠ æ–°çš„è¯„æµ‹åŸºå‡†ã€æ¨¡å‹æˆ–åŠŸèƒ½ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [è´¡çŒ®æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ã€‚

æ„Ÿè°¢æ‰€æœ‰ä¸º EvalScope åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº† EvalScopeï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼š
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

# Arena Mode

Arena mode allows you to configure multiple candidate models and specify a baseline model. The evaluation is conducted through pairwise battles between each candidate model and the baseline model with the win rate and ranking of each model outputted at the end. This approach is suitable for comparative evaluation among multiple models and intuitively reflects the strengths and weaknesses of each model.

## Data Preparation

To support arena mode **all candidate models need to run inference on the same dataset**. The dataset can be a general QA dataset or a domain-specific one. Below is an example using a custom `general_qa` dataset. See the [documentation](../advanced_guides/custom_dataset/llm.md#question-answering-format-qa) for details on using this dataset.

The JSONL file for the `general_qa` dataset should be in the following format. Only the `query` field is required; no additional fields are necessary. Below are two example files:

- Example content of the `arena.jsonl` file:
    ```json
    {"query": "How can I improve my time management skills?"}
    {"query": "What are the most effective ways to deal with stress?"}
    {"query": "What are the main differences between Python and JavaScript programming languages?"}
    {"query": "How can I increase my productivity while working from home?"}
    {"query": "Can you explain the basics of quantum computing?"}
    ```

- Example content of the `example.jsonl` file (with reference answers):
    ```json
    {"query": "What is the capital of France?" "response": "The capital of France is Paris."}
    {"query": "What is the largest mammal in the world?" "response": "The largest mammal in the world is the blue whale."}
    {"query": "How does photosynthesis work?" "response": "Photosynthesis is the process by which green plants use sunlight to synthesize foods with the help of chlorophyll."}
    {"query": "What is the theory of relativity?" "response": "The theory of relativity developed by Albert Einstein describes the laws of physics in relation to observers in different frames of reference."}
    {"query": "Who wrote 'To Kill a Mockingbird'?" "response": "Harper Lee wrote 'To Kill a Mockingbird'."}
    ```

## Candidate Model Inference

After preparing the dataset you can use EvalScope's `run_task` method to perform inference with the candidate models and obtain their outputs for subsequent battles.

Below is an example of how to configure inference tasks for three candidate models: `Qwen2.5-0.5B-Instruct` `Qwen2.5-7B-Instruct` and `Qwen2.5-72B-Instruct` using the same configuration for inference.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task
from evalscope.constants import EvalType

models = ['qwen2.5-72b-instruct' 'qwen2.5-7b-instruct' 'qwen2.5-0.5b-instruct']

task_list = [TaskConfig(
    model=model
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=os.getenv('DASHSCOPE_API_KEY')
    eval_type=EvalType.SERVICE
    datasets=[
        'general_qa'
    ]
    dataset_args={
        'general_qa': {
            'dataset_id': 'custom_eval/text/qa'
            'subset_list': [
                'arena'
                'example'
            ]
        }
    }
    eval_batch_size=10
    generation_config={
        'temperature': 0
        'n': 1
        'max_tokens': 4096
    }) for model in models]

run_task(task_cfg=task_list)
```

<details><summary>Click to view inference results</summary>

Since the `arena` subset does not have reference answers no evaluation metrics are available for this subset. The `example` subset has reference answers so evaluation metrics will be output.
```text
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| Model                 | Dataset    | Metric          | Subset   |   Num |   Score | Cat.0   |
+=======================+============+=================+==========+=======+=========+=========+
| qwen2.5-0.5b-instruct | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-P       | example  |    12 |  0.1341 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-F       | example  |    12 |  0.1983 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-R       | example  |    12 |  0.55   | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-P       | example  |    12 |  0.0404 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-F       | example  |    12 |  0.0716 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-P       | example  |    12 |  0.1193 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-F       | example  |    12 |  0.1754 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-1          | example  |    12 |  0.1192 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-2          | example  |    12 |  0.0403 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-3          | example  |    12 |  0.0135 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-4          | example  |    12 |  0.0079 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-P       | example  |    12 |  0.1149 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-F       | example  |    12 |  0.1612 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-R       | example  |    12 |  0.6833 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-P       | example  |    12 |  0.0813 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-F       | example  |    12 |  0.1027 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-P       | example  |    12 |  0.101  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-F       | example  |    12 |  0.1361 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-1          | example  |    12 |  0.1009 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-2          | example  |    12 |  0.0807 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-P       | example  |    12 |  0.104  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-F       | example  |    12 |  0.1418 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-R       | example  |    12 |  0.7    | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-P       | example  |    12 |  0.078  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-F       | example  |    12 |  0.0964 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-P       | example  |    12 |  0.0942 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-F       | example  |    12 |  0.1235 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-1          | example  |    12 |  0.0939 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-2          | example  |    12 |  0.0777 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
```
</details>

## Candidate Model Battles

Next you can use EvalScope's `general_arena` method to conduct battles among candidate models and get their win rates and rankings on each subset. To achieve robust automatic battles you need to configure an LLM as the judge that compares the outputs of models.

During evaluation EvalScope will automatically parse the public evaluation set of candidate models use the judge model to compare the output of each candidate model with the baseline and determine which is better (to avoid model bias outputs are swapped for two rounds per comparison). The judge model's outputs are parsed as win draw or loss and each candidate model's **Elo score** and **win rate** are calculated.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task

task_cfg = TaskConfig(
    model_id='Arena'  # Model ID is 'Arena'; you can omit specifying model ID
    datasets=[
        'general_arena'  # Must be 'general_arena' indicating arena mode
    ]
    dataset_args={
        'general_arena': {
            # 'system_prompt': 'xxx' # Optional: customize the judge model's system prompt here
            # 'prompt_template': 'xxx' # Optional: customize the judge model's prompt template here
            'extra_params':{
                # Configure candidate model names and corresponding report paths
                # Report paths refer to the output paths from the previous step for parsing model inference results
                'models':[
                    {
                        'name': 'qwen2.5-0.5b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-0.5b-instruct'
                    }
                    {
                        'name': 'qwen2.5-7b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-7b-instruct'
                    }
                    {
                        'name': 'qwen2.5-72b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-72b-instruct'
                    }
                ]
                # Set baseline model must be one of the candidate models
                'baseline': 'qwen2.5-7b'
            }
        }
    }
    # Configure judge model parameters
    judge_model_args={
        'model_id': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': os.getenv('DASHSCOPE_API_KEY')
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 8000
        }
    }
    judge_worker_num=5
    # use_cache='outputs/xxx' # Optional: to add new candidate models to existing results specify the existing results path
)

run_task(task_cfg=task_cfg)
```

<details><summary>Click to view evaluation results</summary>

```text
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Model   | Dataset       | Metric        | Subset                                     |   Num |   Score | Cat.0   |
+=========+===============+===============+============================================+=======+=========+=========+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.5469 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.075  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.8382 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | OVERALL                                    |    44 |  0.3617 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.3906 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.025  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.7276 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | OVERALL                                    |    44 |  0.2826 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.6875 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.9412 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | OVERALL                                    |    44 |  0.4469 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+ 
```
</details>


The automatically generated model leaderboard is as follows (output file located in `outputs/xxx/reports/Arena/leaderboard.txt`):

The leaderboard is sorted by win rate in descending order. As shown the `qwen2.5-72b` model performs best across all subsets with the highest win rate while the `qwen2.5-0.5b` model performs the worst.

```text
=== OVERALL LEADERBOARD ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== DATASET LEADERBOARD: general_qa ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== SUBSET LEADERBOARD: general_qa - example ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            54.7  (-15.6 / +14.1)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            1.8  (+0.0 / +7.2)

=== SUBSET LEADERBOARD: general_qa - arena ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            83.8  (-11.1 / +10.3)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            7.5  (-5.0 / +1.6)
```

## Visualization of Battle Results

To intuitively display the results of the battles between candidate models and the baseline EvalScope provides a visualization feature allowing you to compare the results of each candidate model against the baseline model for each sample.

Run the command below to launch the visualization interface:
```shell
evalscope app
```
Open `http://localhost:7860` in your browser to view the visualization page.

Workflow:
1. Select the latest `general_arena` evaluation report and click the "Load and View" button.
2. Click dataset details and select the battle results between your candidate model and the baseline.
3. Adjust the threshold to filter battle results (normalized scores range from 0-1; 0.5 indicates a tie scores above 0.5 indicate the candidate is better than the baseline below 0.5 means worse).

Example below: a battle between `qwen2.5-72b` and `qwen2.5-7b`. The model judged the 72b as better:

![image](https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/arena_example.jpg)


# Sandbox Environment Usage

To complete LLM code capability evaluation we need to set up an independent evaluation environment to avoid executing erroneous code in the development environment and causing unavoidable losses. Currently EvalScope has integrated the [ms-enclave](https://github.com/modelscope/ms-enclave) sandbox environment allowing users to evaluate model code capabilities in a controlled environment such as using evaluation benchmarks like HumanEval and LiveCodeBench.

The following introduces two different sandbox usage methods:

- Local usage: Set up the sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine;
- Remote usage: Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

## 1. Local Usage

Use Docker to set up a sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine.

### Environment Setup

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in your local Python environment:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration
When running evaluations add the `use_sandbox` and `sandbox_type` parameters to automatically enable the sandbox environment. Other parameters remain the same as regular evaluations:

Here's a complete example code for model evaluation on HumanEval:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will automatically start and manage the sandbox environment ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] Local sandbox manager started
...
```

## 2. Remote Usage

Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

### Environment Setup

You need to install and configure separately on both the remote machine and local machine.

#### Remote Machine

The environment installation on the remote machine is similar to the local usage method described above:

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in remote Python environment:

```bash
pip install evalscope[sandbox]
```

3. **Start sandbox server**: Run the following command to start the sandbox server:

```bash
ms-enclave server --host 0.0.0.0 --port 1234
```

#### Local Machine

The local machine does not need Docker installation at this point but needs to install EvalScope:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration

When running evaluations add the `use_sandbox` parameter to automatically enable the sandbox environment and specify the remote sandbox server's API address in `sandbox_manager_config`:

Complete example code is as follows:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    sandbox_manager_config={
        'base_url': 'http://<remote_host>:1234'  # remote sandbox manager URL
    }
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will communicate with the remote sandbox server through API ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] HTTP sandbox manager started connected to http://<remote_host>:1234
...
```


# EvalScope Service Deployment

## Introduction

EvalScope service mode provides HTTP API-based evaluation and stress testing capabilities designed to address the following scenarios:

1. **Remote Invocation**: Support remote evaluation functionality through network without configuring complex evaluation environments locally
2. **Service Integration**: Easily integrate evaluation capabilities into existing workflows CI/CD pipelines or automated testing systems
3. **Multi-user Collaboration**: Support multiple users or systems calling the evaluation service simultaneously improving resource utilization
4. **Unified Management**: Centrally manage evaluation resources and configurations for easier maintenance and monitoring
5. **Flexible Deployment**: Can be deployed on dedicated servers or container environments decoupled from business systems

The Flask service encapsulates EvalScope's core evaluation (eval) and stress testing (perf) functionalities providing services through standard RESTful APIs making evaluation capabilities callable and integrable like other microservices.

## Features

- **Model Evaluation** (`/api/v1/eval`): Support evaluation of OpenAI API-compatible models
- **Performance Testing** (`/api/v1/perf`): Support performance benchmarking of OpenAI API-compatible models
- **Parameter Query**: Provide parameter description endpoints

## Environment Setup


### Full Installation (Recommended)

```bash
pip install evalscope[service]
```

### Development Environment Installation

```bash
# Clone repository
git clone https://github.com/modelscope/evalscope.git
cd evalscope

# Install development version with service
pip install -e '.[service]'
```

## Starting the Service

### Command Line Launch

```bash
# Use default configuration (host: 0.0.0.0 port: 9000)
evalscope service

# Custom host and port
evalscope service --host 127.0.0.1 --port 9000

# Enable debug mode
evalscope service --debug
```

### Python Code Launch

```python
from evalscope.service import run_service

# Start service
run_service(host='0.0.0.0' port=9000 debug=False)
```

## API Endpoints

### 1. Health Check

```bash
GET /health
```

**Response Example:**
```json
{
  "status": "ok"
  "service": "evalscope"
  "timestamp": "2025-12-04T10:00:00"
}
```

### 2. Model Evaluation

```bash
POST /api/v1/eval
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
  "api_key": "your-api-key"
  "datasets": ["gsm8k" "iquiz"]
  "limit": 10
  "generation_config": {
    "temperature": 0.0
    "max_tokens": 2048
  }
}
```

**Required Parameters:**
- `model`: Model name
- `datasets`: List of datasets
- `api_url`: API endpoint URL (OpenAI-compatible)

**Optional Parameters:**
- `api_key`: API key (default: "EMPTY")
- `limit`: Evaluation sample quantity limit
- `eval_batch_size`: Batch size (default: 1)
- `generation_config`: Generation configuration
  - `temperature`: Temperature parameter (default: 0.0)
  - `max_tokens`: Maximum generation tokens (default: 2048)
  - `top_p`: Nucleus sampling parameter
  - `top_k`: Top-k sampling parameter
- `work_dir`: Output directory
- `debug`: Debug mode
- `seed`: Random seed (default: 42)

**Response Example:**
```json
{
  "status": "success"
  "message": "Evaluation completed"
  "result": {"...": "..."}
  "output_dir": "/path/to/outputs/20251204_100000"
}
```

### 3. Performance Testing

```bash
POST /api/v1/perf
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  "api": "openai"
  "api_key": "your-api-key"
  "number": 100
  "parallel": 10
  "dataset": "openqa"
  "max_tokens": 2048
  "temperature": 0.0
}
```

**Required Parameters:**
- `model`: Model name
- `url`: Complete API endpoint URL

**Optional Parameters:**
- `api`: API type (openai/dashscope/anthropic/gemini default: "openai")
- `api_key`: API key
- `number`: Total number of requests (default: 1000)
- `parallel`: Concurrency level (default: 1)
- `rate`: Requests per second limit (default: -1 unlimited)
- `dataset`: Dataset name (default: "openqa")
- `max_tokens`: Maximum generation tokens (default: 2048)
- `temperature`: Temperature parameter (default: 0.0)
- `stream`: Whether to use streaming output (default: true)
- `debug`: Debug mode

**Response Example:**
```json
{
  "status": "success"
  "message": "Performance test completed"
  "output_dir": "/path/to/outputs"
  "results": {
    "parallel_10_number_100": {
      "metrics": {"...": "..."}
      "percentiles": {"...": "..."}
    }
  }
}
```

### 4. Get Evaluation Parameter Description

```bash
GET /api/v1/eval/params
```

Returns descriptions of all parameters supported by the evaluation endpoint.

### 5. Get Performance Test Parameter Description

```bash
GET /api/v1/perf/params
```

Returns descriptions of all parameters supported by the performance test endpoint.

## Usage Examples

### Testing Evaluation Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "your-api-key"
    "datasets": ["gsm8k"]
    "limit": 5
  }'
```

### Testing Performance Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    "api": "openai"
    "number": 50
    "parallel": 5
  }'
```

### Using Python requests

```python
import requests

# Evaluation request
eval_response = requests.post(
    'http://localhost:9000/api/v1/eval'
    json={
        'model': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': 'your-api-key'
        'datasets': ['gsm8k' 'iquiz']
        'limit': 10
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 2048
        }
    }
)
print(eval_response.json())

# Performance test request
perf_response = requests.post(
    'http://localhost:9000/api/v1/perf'
    json={
        'model': 'qwen-plus'
        'url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions'
        'api': 'openai'
        'number': 100
        'parallel': 10
        'dataset': 'openqa'
    }
)
print(perf_response.json())
```

## Important Notes

1. **OpenAI API-Compatible Models Only**: This service is designed specifically for OpenAI API-compatible models
2. **Long-Running Tasks**: Evaluation and performance testing tasks may take considerable time. We recommend setting appropriate HTTP timeout values on the client side as the API calls are synchronous and will block until completion.
3. **Output Directory**: Evaluation results are saved in the configured `work_dir` default is `outputs/`
4. **Error Handling**: The service returns detailed error messages and stack traces (in debug mode)
5. **Resource Management**: Pay attention to concurrency settings during stress testing to avoid server overload

## Error Codes

- `400`: Invalid request parameters
- `404`: Endpoint not found
- `500`: Internal server error

## Example Scenarios

### Scenario 1: Quick Evaluation of Qwen Model

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "sk-..."
    "datasets": ["gsm8k"]
    "limit": 100
  }'
```

### Scenario 2: Stress Testing Locally Deployed Model

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5"
    "url": "http://localhost:8000/v1/chat/completions"
    "api": "openai"
    "number": 1000
    "parallel": 20
    "max_tokens": 2048
  }'
```

### Scenario 3: Multi-Dataset Evaluation

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "datasets": ["gsm8k" "iquiz" "ceval"]
    "limit": 50
    "eval_batch_size": 4
  }'
```

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  <a href="README_zh.md">ä¸­æ–‡</a> &nbsp ï½œ &nbsp English &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  Chinese Documentation</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documentation</a>
<p>


> â­ If you like this project please click the "Star" button in the upper right corner to support us. Your support is our motivation to move forward!

## ğŸ“ Introduction

EvalScope is a powerful and easily extensible model evaluation framework created by the [ModelScope Community](https://modelscope.cn/) aiming to provide a one-stop evaluation solution for large model developers.

Whether you want to evaluate the general capabilities of models conduct multi-model performance comparisons or need to stress test models EvalScope can meet your needs.

## âœ¨ Key Features

- **ğŸ“š Comprehensive Evaluation Benchmarks**: Built-in multiple industry-recognized evaluation benchmarks including MMLU C-Eval GSM8K and more.
- **ğŸ§© Multi-modal and Multi-domain Support**: Supports evaluation of various model types including Large Language Models (LLM) Vision Language Models (VLM) Embedding Reranker AIGC and more.
- **ğŸš€ Multi-backend Integration**: Seamlessly integrates multiple evaluation backends including OpenCompass VLMEvalKit RAGEval to meet different evaluation needs.
- **âš¡ Inference Performance Testing**: Provides powerful model service stress testing tools supporting multiple performance metrics such as TTFT TPOT.
- **ğŸ“Š Interactive Reports**: Provides WebUI visualization interface supporting multi-dimensional model comparison report overview and detailed inspection.
- **âš”ï¸ Arena Mode**: Supports multi-model battles (Pairwise Battle) intuitively ranking and evaluating models.
- **ğŸ”§ Highly Extensible**: Developers can easily add custom datasets models and evaluation metrics.

<details><summary>ğŸ›ï¸ Overall Architecture</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope Overall Architecture.
</p>

1.  **Input Layer**
    - **Model Sources**: API models (OpenAI API) Local models (ModelScope)
    - **Datasets**: Standard evaluation benchmarks (MMLU/GSM8k etc.) Custom data (MCQ/QA)

2.  **Core Functions**
    - **Multi-backend Evaluation**: Native backend OpenCompass MTEB VLMEvalKit RAGAS
    - **Performance Monitoring**: Supports multiple model service APIs and data formats tracking TTFT/TPOP and other metrics
    - **Tool Extensions**: Integrates Tool-Bench Needle-in-a-Haystack etc.

3.  **Output Layer**
    - **Structured Reports**: Supports JSON Table Logs
    - **Visualization Platform**: Supports Gradio Wandb SwanLab

</details>

## ğŸ‰ What's New

> [!IMPORTANT]
> **Version 1.0 Refactoring**
>
> Version 1.0 introduces a major overhaul of the evaluation framework establishing a new more modular and extensible API layer under `evalscope/api`. Key improvements include standardized data models for benchmarks samples and results; a registry-based design for components such as benchmarks and metrics; and a rewritten core evaluator that orchestrates the new architecture. Existing benchmark adapters have been migrated to this API resulting in cleaner more consistent and easier-to-maintain implementations.

- ğŸ”¥ **[2025.12.02]** Added support for custom multimodal VQA evaluation; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/vlm.html). Added support for visualizing model service stress testing in ClearML; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#clearml).
- ğŸ”¥ **[2025.11.26]** Added support for OpenAI-MRCR GSM8K-V MGSM MicroVQA IFBench SciCode benchmarks.
- ğŸ”¥ **[2025.11.18]** Added support for custom Function-Call (tool invocation) datasets to test whether models can timely and correctly call tools. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#function-calling-format-fc).
- ğŸ”¥ **[2025.11.14]** Added support for SWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini code evaluation benchmarks. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/swe_bench.html).
- ğŸ”¥ **[2025.11.12]** Added `pass@k` `vote@k` `pass^k` and other metric aggregation methods; added support for multimodal evaluation benchmarks such as A_OKVQA CMMU ScienceQA V*Bench.
- ğŸ”¥ **[2025.11.07]** Added support for Ï„Â²-bench an extended and enhanced version of Ï„-bench that includes a series of code fixes and adds telecom domain troubleshooting scenarios. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/tau2_bench.html).
- ğŸ”¥ **[2025.10.30]** Added support for BFCL-v4 enabling evaluation of agent capabilities including web search and long-term memory. See the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v4.html).
- ğŸ”¥ **[2025.10.27]** Added support for LogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQA and other evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.26]** Added support for Conll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 and other Named Entity Recognition evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.21]** Optimized sandbox environment usage in code evaluation supporting both local and remote operation modes. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html).
- ğŸ”¥ **[2025.10.20]** Added support for evaluation benchmarks including PolyMath SimpleVQA MathVerse MathVision AA-LCR; optimized evalscope perf performance to align with vLLM Bench. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/vs_vllm_bench.html).
- ğŸ”¥ **[2025.10.14]** Added support for OCRBench OCRBench-v2 DocVQA InfoVQA ChartQA and BLINK multimodal image-text evaluation benchmarks.
- ğŸ”¥ **[2025.09.22]** Code evaluation benchmarks (HumanEval LiveCodeBench) now support running in a sandbox environment. To use this feature please install [ms-enclave](https://github.com/modelscope/ms-enclave) first.
- ğŸ”¥ **[2025.09.19]** Added support for multimodal image-text evaluation benchmarks including RealWorldQA AI2D MMStar MMBench and OmniBench as well as pure text evaluation benchmarks such as Multi-IF HealthBench and AMC.
- ğŸ”¥ **[2025.09.05]** Added support for vision-language multimodal model evaluation tasks such as MathVista and MMMU. For more supported datasets please [refer to the documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/vlm.html).
- ğŸ”¥ **[2025.09.04]** Added support for image editing task evaluation including the [GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/image_edit.html).
- ğŸ”¥ **[2025.08.22]** Version 1.0 Refactoring. Break changes please [refer to](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#switching-to-version-v1-0).
<details><summary>More</summary>

- ğŸ”¥ **[2025.07.18]** The model stress testing now supports randomly generating image-text data for multimodal model evaluation. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#id4).
- ğŸ”¥ **[2025.07.16]** Support for [Ï„-bench](https://github.com/sierra-research/tau-bench) has been added enabling the evaluation of AI Agent performance and reliability in real-world scenarios involving dynamic user and tool interactions. For usage instructions please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#bench).
- ğŸ”¥ **[2025.07.14]** Support for "Humanity's Last Exam" ([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle)) a highly challenging evaluation benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam).
- ğŸ”¥ **[2025.07.03]** Refactored Arena Mode: now supports custom model battles outputs a model leaderboard and provides battle result visualization. See [reference](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html) for details.
- ğŸ”¥ **[2025.06.28]** Optimized custom dataset evaluation: now supports evaluation without reference answers. Enhanced LLM judge usage with built-in modes for "scoring directly without reference answers" and "checking answer consistency with reference answers". See [reference](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa) for details.
- ğŸ”¥ **[2025.06.19]** Added support for the [BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3) benchmark designed to evaluate model function-calling capabilities across various scenarios. For more information refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v3.html).
- ğŸ”¥ **[2025.06.02]** Added support for the Needle-in-a-Haystack test. Simply specify `needle_haystack` to conduct the test and a corresponding heatmap will be generated in the `outputs/reports` folder providing a visual representation of the model's performance. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/needle_haystack.html) for more details.
- ğŸ”¥ **[2025.05.29]** Added support for two long document evaluation benchmarks: [DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary) and [FRAMES](https://modelscope.cn/datasets/iic/frames/summary). For usage guidelines please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html).
- ğŸ”¥ **[2025.05.16]** Model service performance stress testing now supports setting various levels of concurrency and outputs a performance test report. [Reference example](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/quick_start.html#id3).
- ğŸ”¥ **[2025.05.13]** Added support for the [ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static) dataset to evaluate model's tool-calling capabilities. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html) for usage instructions. Also added support for the [DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview) and [Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val) benchmarks to assess the reasoning capabilities of models.
- ğŸ”¥ **[2025.04.29]** Added Qwen3 Evaluation Best Practices [welcome to read ğŸ“–](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** Support for text-to-image evaluation: Supports 8 metrics including MPS HPSv2.1Score etc. and evaluation benchmarks such as EvalMuse GenAI-Bench. Refer to the [user documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/t2i.html) for more details.
- ğŸ”¥ **[2025.04.10]** Model service stress testing tool now supports the `/v1/completions` endpoint (the default endpoint for vLLM benchmarking)
- ğŸ”¥ **[2025.04.08]** Support for evaluating embedding model services compatible with the OpenAI API has been added. For more details check the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters).
- ğŸ”¥ **[2025.03.27]** Added support for [AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview) and [ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary) evaluation benchmarks. For usage notes please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** The model inference service stress testing now supports generating prompts of specified length using random values. Refer to the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#using-the-random-dataset) for more details.
- ğŸ”¥ **[2025.03.13]** Added support for the [LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary) code evaluation benchmark which can be used by specifying `live_code_bench`. Supports evaluating QwQ-32B on LiveCodeBench refer to the [best practices](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html).
- ğŸ”¥ **[2025.03.11]** Added support for the [SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary) and [Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary) evaluation benchmarks. These are used to assess the factual accuracy of models and you can specify `simple_qa` and `chinese_simpleqa` for use. Support for specifying a judge model is also available. For more details refer to the [relevant parameter documentation](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).
- ğŸ”¥ **[2025.03.07]** Added support for the [QwQ-32B](https://modelscope.cn/models/Qwen/QwQ-32B/summary) model evaluate the model's reasoning ability and reasoning efficiency refer to [ğŸ“– Best Practices for QwQ-32B Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html) for more details.
- ğŸ”¥ **[2025.03.04]** Added support for the [SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary) dataset which covers 13 categories 72 first-level disciplines and 285 second-level disciplines totaling 26529 questions. You can use it by specifying `super_gpqa`.
- ğŸ”¥ **[2025.03.03]** Added support for evaluating the IQ and EQ of models. Refer to [ğŸ“– Best Practices for IQ and EQ Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html) to find out how smart your AI is!
- ğŸ”¥ **[2025.02.27]** Added support for evaluating the reasoning efficiency of models. Refer to [ğŸ“– Best Practices for Evaluating Thinking Efficiency](https://evalscope.readthedocs.io/en/latest/best_practice/think_eval.html). This implementation is inspired by the works [Overthinking](https://doi.org/10.48550/arXiv.2412.21187) and [Underthinking](https://doi.org/10.48550/arXiv.2501.18585).
- ğŸ”¥ **[2025.02.25]** Added support for two model inference-related evaluation benchmarks: [MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR) and [ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary). To use them simply specify `musr` and `process_bench` respectively in the datasets parameter.
- ğŸ”¥ **[2025.02.18]** Supports the AIME25 dataset which contains 15 questions (Grok3 scored 93 on this dataset).
- ğŸ”¥ **[2025.02.13]** Added support for evaluating DeepSeek distilled models including AIME24 MATH-500 and GPQA-Diamond datasetsï¼Œrefer to [best practice](https://evalscope.readthedocs.io/en/latest/best_practice/deepseek_r1_distill.html); Added support for specifying the `eval_batch_size` parameter to accelerate model evaluation.
- ğŸ”¥ **[2025.01.20]** Support for visualizing evaluation results including single model evaluation results and multi-model comparison refer to the [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html) for more details; Added [`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary) evaluation example evaluating the IQ and EQ of the model.
- ğŸ”¥ **[2025.01.07]** Native backend: Support for model API evaluation is now available. Refer to the [ğŸ“– Model API Evaluation Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#api) for more details. Additionally support for the `ifeval` evaluation benchmark has been added.
- ğŸ”¥ğŸ”¥ **[2024.12.31]** Support for adding benchmark evaluations refer to the [ğŸ“– Benchmark Evaluation Addition Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html); support for custom mixed dataset evaluations allowing for more comprehensive model evaluations with less data refer to the [ğŸ“– Mixed Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/collection/index.html).
- ğŸ”¥ **[2024.12.13]** Model evaluation optimization: no need to pass the `--template-type` parameter anymore; supports starting evaluation with `evalscope eval --args`. Refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html) for more details.
- ğŸ”¥ **[2024.11.26]** The model inference service performance evaluator has been completely refactored: it now supports local inference service startup and Speed Benchmark; asynchronous call error handling has been optimized. For more details refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).
- ğŸ”¥ **[2024.10.31]** The best practice for evaluating Multimodal-RAG has been updated please check the [ğŸ“– Blog](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag) for more details.
- ğŸ”¥ **[2024.10.23]** Supports multimodal RAG evaluation including the assessment of image-text retrieval using [CLIP_Benchmark](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/clip_benchmark.html) and extends [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html) to support end-to-end multimodal metrics evaluation.
- ğŸ”¥ **[2024.10.8]** Support for RAG evaluation including independent evaluation of embedding models and rerankers using [MTEB/CMTEB](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html) as well as end-to-end evaluation using [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html).
- ğŸ”¥ **[2024.09.18]** Our documentation has been updated to include a blog module featuring some technical research and discussions related to evaluations. We invite you to [ğŸ“– read it](https://evalscope.readthedocs.io/en/refact_readme/blog/index.html).
- ğŸ”¥ **[2024.09.12]** Support for LongWriter evaluation which supports 10000+ word generation. You can use the benchmark [LongBench-Write](evalscope/third_party/longbench_write/README.md) to measure the long output quality as well as the output length.
- ğŸ”¥ **[2024.08.30]** Support for custom dataset evaluations including text datasets and multimodal image-text datasets.
- ğŸ”¥ **[2024.08.20]** Updated the official documentation including getting started guides best practices and FAQs. Feel free to [ğŸ“–read it here](https://evalscope.readthedocs.io/en/latest/)!
- ğŸ”¥ **[2024.08.09]** Simplified the installation process allowing for pypi installation of vlmeval dependencies; optimized the multimodal model evaluation experience achieving up to 10x acceleration based on the OpenAI API evaluation chain.
- ğŸ”¥ **[2024.07.31]** Important change: The package name `llmuses` has been changed to `evalscope`. Please update your code accordingly.
- ğŸ”¥ **[2024.07.26]** Support for **VLMEvalKit** as a third-party evaluation framework to initiate multimodal model evaluation tasks.
- ğŸ”¥ **[2024.06.29]** Support for **OpenCompass** as a third-party evaluation framework which we have encapsulated at a higher level supporting pip installation and simplifying evaluation task configuration.
- ğŸ”¥ **[2024.06.13]** EvalScope seamlessly integrates with the fine-tuning framework SWIFT providing full-chain support from LLM training to evaluation.
- ğŸ”¥ **[2024.06.13]** Integrated the Agent evaluation dataset ToolBench.

</details>

## â¤ï¸ Community & Support

Welcome to join our community to communicate with other developers and get help.

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  WeChat Group | DingTalk Group
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ Environment Setup

We recommend using `conda` to create a virtual environment and install with `pip`.

1.  **Create and Activate Conda Environment** (Python 3.10 recommended)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **Install EvalScope**

    - **Method 1: Install via PyPI (Recommended)**
      ```shell
      pip install evalscope
      ```

    - **Method 2: Install from Source (For Development)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **Install Additional Dependencies** (Optional)
    Install corresponding feature extensions according to your needs:
    ```shell
    # Performance testing
    pip install 'evalscope[perf]'

    # Visualization App
    pip install 'evalscope[app]'

    # Other evaluation backends
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # Install all dependencies
    pip install 'evalscope[all]'
    ```
    > If you installed from source please replace `evalscope` with `.` for example `pip install '.[perf]'`.

> [!NOTE]
> This project was formerly known as `llmuses`. If you need to use `v0.4.3` or earlier versions please run `pip install llmuses<=0.4.3` and use `from llmuses import ...` for imports.


## ğŸš€ Quick Start

You can start evaluation tasks in two ways: **command line** or **Python code**.

### Method 1. Using Command Line

Execute the `evalscope eval` command in any path to start evaluation. The following command will evaluate the `Qwen/Qwen2.5-0.5B-Instruct` model on `gsm8k` and `arc` datasets taking only 5 samples from each dataset.

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### Method 2. Using Python Code

Use the `run_task` function and `TaskConfig` object to configure and start evaluation tasks.

```python
from evalscope import run_task TaskConfig

# Configure evaluation task
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# Start evaluation
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ Tip:</b> `run_task` also supports dictionaries YAML or JSON files as configuration.</summary>

**Using Python Dictionary**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**Using YAML File** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### Output Results
After evaluation completion you will see a report in the terminal in the following format:
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ Advanced Usage

### Custom Evaluation Parameters

You can fine-tune model loading inference and dataset configuration through command line parameters.

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: Model loading parameters such as `revision` `precision` etc.
- `--generation-config`: Model generation parameters such as `temperature` `max_tokens` etc.
- `--dataset-args`: Dataset configuration parameters such as `few_shot_num` etc.

For details please refer to [ğŸ“– Complete Parameter Guide](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).

### Evaluating Online Model APIs

EvalScope supports evaluating model services deployed via APIs (such as services deployed with vLLM). Simply specify the service address and API Key.

1.  **Start Model Service** (using vLLM as example)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **Run Evaluation**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ Arena Mode

Arena mode evaluates model performance through pairwise battles between models providing win rates and rankings perfect for horizontal comparison of multiple models.

```text
# Example evaluation results
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
For details please refer to [ğŸ“– Arena Mode Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html).

### ğŸ–Šï¸ Custom Dataset Evaluation

EvalScope allows you to easily add and evaluate your own datasets. For details please refer to [ğŸ“– Custom Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html).


## ğŸ§ª Other Evaluation Backends
EvalScope supports launching evaluation tasks through third-party evaluation frameworks (we call them "backends") to meet diverse evaluation needs.

- **Native**: EvalScope's default evaluation framework with comprehensive functionality.
- **OpenCompass**: Focuses on text-only evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: Focuses on multi-modal evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: Focuses on RAG evaluation supporting Embedding and Reranker models. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/index.html)
- **Third-party Evaluation Tools**: Supports evaluation tasks like [ToolBench](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html).

## âš¡ Inference Performance Evaluation Tool
EvalScope provides a powerful stress testing tool for evaluating the performance of large language model services.

- **Key Metrics**: Supports throughput (Tokens/s) first token latency (TTFT) token generation latency (TPOT) etc.
- **Result Recording**: Supports recording results to `wandb` and `swanlab`.
- **Speed Benchmarks**: Can generate speed benchmark results similar to official reports.

For details please refer to [ğŸ“– Performance Testing Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).

Example output is shown below:
<p align="center">
    <img src="docs/en/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š Visualizing Evaluation Results

EvalScope provides a Gradio-based WebUI for interactive analysis and comparison of evaluation results.

1.  **Install Dependencies**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **Start Service**
    ```bash
    evalscope app
    ```
    Visit `http://127.0.0.1:7861` to open the visualization interface.

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/setting.png" alt="Setting" style="width: 85%;" />
      <p>Settings Interface</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>Model Comparison</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>Report Overview</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_details.png" alt="Report Details" style="width: 85%;" />
      <p>Report Details</p>
    </td>
  </tr>
</table>

For details please refer to [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html).

## ğŸ‘·â€â™‚ï¸ Contributing

We welcome any contributions from the community! If you want to add new evaluation benchmarks models or features please refer to our [Contributing Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html).

Thanks to all developers who have contributed to EvalScope!

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š Citation

If you use EvalScope in your research please cite our work:
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  ä¸­æ–‡ &nbsp ï½œ &nbsp <a href="evalscope.md">English</a> &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/zh-cn/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  ä¸­æ–‡æ–‡æ¡£</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documents</a>
<p>


> â­ å¦‚æœä½ å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ "Star" æŒ‰é’®æ”¯æŒæˆ‘ä»¬ã€‚ä½ çš„æ”¯æŒæ˜¯æˆ‘ä»¬å‰è¿›çš„åŠ¨åŠ›ï¼

## ğŸ“ ç®€ä»‹

EvalScope æ˜¯ç”±[é­”æ­ç¤¾åŒº](https://modelscope.cn/)æ‰“é€ çš„ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§ã€æ˜“äºæ‰©å±•çš„æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¤§æ¨¡å‹å¼€å‘è€…æä¾›ä¸€ç«™å¼è¯„æµ‹è§£å†³æ–¹æ¡ˆã€‚

æ— è®ºæ‚¨æ˜¯æƒ³è¯„ä¼°æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€è¿›è¡Œå¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼Œè¿˜æ˜¯éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼ŒEvalScope éƒ½èƒ½æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- **ğŸ“š å…¨é¢çš„è¯„æµ‹åŸºå‡†**: å†…ç½® MMLU C-Eval GSM8K ç­‰å¤šä¸ªä¸šç•Œå…¬è®¤çš„è¯„æµ‹åŸºå‡†ã€‚
- **ğŸ§© å¤šæ¨¡æ€ä¸å¤šé¢†åŸŸæ”¯æŒ**: æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ (LLM)ã€å¤šæ¨¡æ€ (VLM)ã€Embeddingã€Rerankerã€AIGC ç­‰å¤šç§æ¨¡å‹çš„è¯„æµ‹ã€‚
- **ğŸš€ å¤šåç«¯é›†æˆ**: æ— ç¼é›†æˆ OpenCompass VLMEvalKit RAGEval ç­‰å¤šç§è¯„æµ‹åç«¯ï¼Œæ»¡è¶³ä¸åŒè¯„æµ‹éœ€æ±‚ã€‚
- **âš¡ æ¨ç†æ€§èƒ½æµ‹è¯•**: æä¾›å¼ºå¤§çš„æ¨¡å‹æœåŠ¡å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒ TTFT TPOT ç­‰å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ã€‚
- **ğŸ“Š äº¤äº’å¼æŠ¥å‘Š**: æä¾› WebUI å¯è§†åŒ–ç•Œé¢ï¼Œæ”¯æŒå¤šç»´åº¦æ¨¡å‹å¯¹æ¯”ã€æŠ¥å‘Šæ¦‚è§ˆå’Œè¯¦æƒ…æŸ¥é˜…ã€‚
- **âš”ï¸ ç«æŠ€åœºæ¨¡å¼**: æ”¯æŒå¤šæ¨¡å‹å¯¹æˆ˜ (Pairwise Battle)ï¼Œç›´è§‚åœ°å¯¹æ¨¡å‹è¿›è¡Œæ’åå’Œè¯„ä¼°ã€‚
- **ğŸ”§ é«˜åº¦å¯æ‰©å±•**: å¼€å‘è€…å¯ä»¥è½»æ¾æ·»åŠ è‡ªå®šä¹‰æ•°æ®é›†ã€æ¨¡å‹å’Œè¯„æµ‹æŒ‡æ ‡ã€‚

<details><summary>ğŸ›ï¸ æ•´ä½“æ¶æ„</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope æ•´ä½“æ¶æ„å›¾.
</p>

1.  **è¾“å…¥å±‚**
    - **æ¨¡å‹æ¥æº**: APIæ¨¡å‹ï¼ˆOpenAI APIï¼‰ã€æœ¬åœ°æ¨¡å‹ï¼ˆModelScopeï¼‰
    - **æ•°æ®é›†**: æ ‡å‡†è¯„æµ‹åŸºå‡†ï¼ˆMMLU/GSM8kç­‰ï¼‰ã€è‡ªå®šä¹‰æ•°æ®ï¼ˆMCQ/QAï¼‰

2.  **æ ¸å¿ƒåŠŸèƒ½**
    - **å¤šåç«¯è¯„ä¼°**: åŸç”Ÿåç«¯ã€OpenCompassã€MTEBã€VLMEvalKitã€RAGAS
    - **æ€§èƒ½ç›‘æ§**: æ”¯æŒå¤šç§æ¨¡å‹æœåŠ¡ API å’Œæ•°æ®æ ¼å¼ï¼Œè¿½è¸ª TTFT/TPOP ç­‰æŒ‡æ ‡
    - **å·¥å…·æ‰©å±•**: é›†æˆ Tool-Bench Needle-in-a-Haystack ç­‰

3.  **è¾“å‡ºå±‚**
    - **ç»“æ„åŒ–æŠ¥å‘Š**: æ”¯æŒ JSON Table Logs
    - **å¯è§†åŒ–å¹³å°**: æ”¯æŒ Gradio Wandb SwanLab

</details>

## ğŸ‰ å†…å®¹æ›´æ–°

> [!IMPORTANT]
> **ç‰ˆæœ¬ 1.0 é‡æ„**
>
> ç‰ˆæœ¬ 1.0 å¯¹è¯„æµ‹æ¡†æ¶è¿›è¡Œäº†é‡å¤§é‡æ„ï¼Œåœ¨ `evalscope/api` ä¸‹å»ºç«‹äº†å…¨æ–°çš„ã€æ›´æ¨¡å—åŒ–ä¸”æ˜“æ‰©å±•çš„ API å±‚ã€‚ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼šä¸ºåŸºå‡†ã€æ ·æœ¬å’Œç»“æœå¼•å…¥äº†æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹ï¼›å¯¹åŸºå‡†å’ŒæŒ‡æ ‡ç­‰ç»„ä»¶é‡‡ç”¨æ³¨å†Œè¡¨å¼è®¾è®¡ï¼›å¹¶é‡å†™äº†æ ¸å¿ƒè¯„æµ‹å™¨ä»¥ååŒæ–°æ¶æ„ã€‚ç°æœ‰çš„åŸºå‡†å·²è¿ç§»åˆ°è¿™ä¸€ APIï¼Œå®ç°æ›´åŠ ç®€æ´ã€ä¸€è‡´ä¸”æ˜“äºç»´æŠ¤ã€‚

- ğŸ”¥ **[2025.12.02]** æ”¯æŒè‡ªå®šä¹‰å¤šæ¨¡æ€VQAè¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/vlm.html) ï¼›æ”¯æŒæ¨¡å‹æœåŠ¡å‹æµ‹åœ¨ ClearML ä¸Šå¯è§†åŒ–ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#clearml)ã€‚
- ğŸ”¥ **[2025.11.26]** æ–°å¢æ”¯æŒ OpenAI-MRCRã€GSM8K-Vã€MGSMã€MicroVQAã€IFBenchã€SciCode è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.18]** æ”¯æŒè‡ªå®šä¹‰ Function-Callï¼ˆå·¥å…·è°ƒç”¨ï¼‰æ•°æ®é›†ï¼Œæ¥æµ‹è¯•æ¨¡å‹èƒ½å¦é€‚æ—¶å¹¶æ­£ç¡®è°ƒç”¨å·¥å…·ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#fc)
- ğŸ”¥ **[2025.11.14]** æ–°å¢æ”¯æŒSWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini ä»£ç è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/swe_bench.html)ã€‚
- ğŸ”¥ **[2025.11.12]** æ–°å¢`pass@k`ã€`vote@k`ã€`pass^k`ç­‰æŒ‡æ ‡èšåˆæ–¹æ³•ï¼›æ–°å¢æ”¯æŒA_OKVQA CMMU ScienceQ V*Benchç­‰å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.07]** æ–°å¢æ”¯æŒÏ„Â²-benchï¼Œæ˜¯ Ï„-bench çš„æ‰©å±•ä¸å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«ä¸€ç³»åˆ—ä»£ç ä¿®å¤ï¼Œå¹¶æ–°å¢äº†ç”µä¿¡ï¼ˆtelecomï¼‰é¢†åŸŸçš„æ•…éšœæ’æŸ¥åœºæ™¯ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/tau2_bench.html)ã€‚
- ğŸ”¥ **[2025.10.30]** æ–°å¢æ”¯æŒBFCL-v4ï¼Œæ”¯æŒagentçš„ç½‘ç»œæœç´¢å’Œé•¿æœŸè®°å¿†èƒ½åŠ›çš„è¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v4.html)ã€‚
- ğŸ”¥ **[2025.10.27]** æ–°å¢æ”¯æŒLogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQAç­‰è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.26]** æ–°å¢æ”¯æŒConll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 ç­‰å‘½åå®ä½“è¯†åˆ«è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.21]** ä¼˜åŒ–ä»£ç è¯„æµ‹ä¸­çš„æ²™ç®±ç¯å¢ƒä½¿ç”¨ï¼Œæ”¯æŒåœ¨æœ¬åœ°å’Œè¿œç¨‹ä¸¤ç§æ¨¡å¼ä¸‹è¿è¡Œï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/sandbox.html)ã€‚
- ğŸ”¥ **[2025.10.20]** æ–°å¢æ”¯æŒPolyMath SimpleVQA MathVerse MathVision AA-LCR ç­‰è¯„æµ‹åŸºå‡†ï¼›ä¼˜åŒ–evalscope perfè¡¨ç°ï¼Œå¯¹é½vLLM Benchï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/vs_vllm_bench.html)ã€‚
- ğŸ”¥ **[2025.10.14]** æ–°å¢æ”¯æŒOCRBench OCRBench-v2 DocVQA InfoVQA ChartQA BLINK ç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.22]** ä»£ç è¯„æµ‹åŸºå‡†(HumanEval LiveCodeBench)æ”¯æŒåœ¨æ²™ç®±ç¯å¢ƒä¸­è¿è¡Œï¼Œè¦ä½¿ç”¨è¯¥åŠŸèƒ½éœ€å…ˆå®‰è£…[ms-enclave](https://github.com/modelscope/ms-enclave)ã€‚
- ğŸ”¥ **[2025.09.19]** æ–°å¢æ”¯æŒRealWorldQAã€AI2Dã€MMStarã€MMBenchã€OmniBenchç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼Œå’ŒMulti-IFã€HealthBenchã€AMCç­‰çº¯æ–‡æœ¬è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.05]** æ”¯æŒè§†è§‰-è¯­è¨€å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è¯„æµ‹ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šMathVistaã€MMMUï¼Œæ›´å¤šæ”¯æŒæ•°æ®é›†è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/vlm.html)ã€‚
- ğŸ”¥ **[2025.09.04]** æ”¯æŒå›¾åƒç¼–è¾‘ä»»åŠ¡è¯„æµ‹ï¼Œæ”¯æŒ[GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/image_edit.html)ã€‚
- ğŸ”¥ **[2025.08.22]** Version 1.0 é‡æ„ï¼Œä¸å…¼å®¹çš„æ›´æ–°è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#v1-0)ã€‚
<details> <summary>æ›´å¤š</summary>

- ğŸ”¥ **[2025.07.18]** æ¨¡å‹å‹æµ‹æ”¯æŒéšæœºç”Ÿæˆå›¾æ–‡æ•°æ®ï¼Œç”¨äºå¤šæ¨¡æ€æ¨¡å‹å‹æµ‹ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#id4)ã€‚
- ğŸ”¥ **[2025.07.16]** æ”¯æŒ[Ï„-bench](https://github.com/sierra-research/tau-bench)ï¼Œç”¨äºè¯„ä¼° AI Agentåœ¨åŠ¨æ€ç”¨æˆ·å’Œå·¥å…·äº¤äº’çš„å®é™…ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#bench)ã€‚
- ğŸ”¥ **[2025.07.14]** æ”¯æŒâ€œäººç±»æœ€åçš„è€ƒè¯•â€([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle))ï¼Œè¿™ä¸€é«˜éš¾åº¦è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam)ã€‚
- ğŸ”¥ **[2025.07.03]** é‡æ„äº†ç«æŠ€åœºæ¨¡å¼ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å¯¹æˆ˜ï¼Œè¾“å‡ºæ¨¡å‹æ’è¡Œæ¦œï¼Œä»¥åŠå¯¹æˆ˜ç»“æœå¯è§†åŒ–ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚
- ğŸ”¥ **[2025.06.28]** ä¼˜åŒ–è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼Œæ”¯æŒæ— å‚è€ƒç­”æ¡ˆè¯„æµ‹ï¼›ä¼˜åŒ–LLMè£åˆ¤ä½¿ç”¨ï¼Œé¢„ç½®â€œæ— å‚è€ƒç­”æ¡ˆç›´æ¥æ‰“åˆ†â€ å’Œ â€œåˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´â€ä¸¤ç§æ¨¡å¼ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#qa)
- ğŸ”¥ **[2025.06.19]** æ–°å¢æ”¯æŒ[BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3)è¯„æµ‹åŸºå‡†ï¼Œç”¨äºè¯„æµ‹æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v3.html)ã€‚
- ğŸ”¥ **[2025.06.02]** æ–°å¢æ”¯æŒå¤§æµ·æé’ˆæµ‹è¯•ï¼ˆNeedle-in-a-Haystackï¼‰ï¼ŒæŒ‡å®š`needle_haystack`å³å¯è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åœ¨`outputs/reports`æ–‡ä»¶å¤¹ä¸‹ç”Ÿæˆå¯¹åº”çš„heatmapï¼Œç›´è§‚å±•ç°æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/needle_haystack.html)ã€‚
- ğŸ”¥ **[2025.05.29]** æ–°å¢æ”¯æŒ[DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary)å’Œ[FRAMES](https://modelscope.cn/datasets/iic/frames/summary)ä¸¤ä¸ªé•¿æ–‡æ¡£è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.05.16]** æ¨¡å‹æœåŠ¡æ€§èƒ½å‹æµ‹æ”¯æŒè®¾ç½®å¤šç§å¹¶å‘ï¼Œå¹¶è¾“å‡ºæ€§èƒ½å‹æµ‹æŠ¥å‘Šï¼Œ[å‚è€ƒç¤ºä¾‹](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/quick_start.html#id3)ã€‚
- ğŸ”¥ **[2025.05.13]** æ–°å¢æ”¯æŒ[ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static)æ•°æ®é›†ï¼Œè¯„æµ‹æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html)ï¼›æ”¯æŒ[DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview)å’Œ[Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val)è¯„æµ‹åŸºå‡†ï¼Œè¯„æµ‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
- ğŸ”¥ **[2025.04.29]** æ–°å¢Qwen3è¯„æµ‹æœ€ä½³å®è·µï¼Œ[æ¬¢è¿é˜…è¯»ğŸ“–](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** æ”¯æŒæ–‡ç”Ÿå›¾è¯„æµ‹ï¼šæ”¯æŒMPSã€HPSv2.1Scoreç­‰8ä¸ªæŒ‡æ ‡ï¼Œæ”¯æŒEvalMuseã€GenAI-Benchç­‰è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/t2i.html)
- ğŸ”¥ **[2025.04.10]** æ¨¡å‹æœåŠ¡å‹æµ‹å·¥å…·æ”¯æŒ`/v1/completions`ç«¯ç‚¹ï¼ˆä¹Ÿæ˜¯vLLMåŸºå‡†æµ‹è¯•çš„é»˜è®¤ç«¯ç‚¹ï¼‰
- ğŸ”¥ **[2025.04.08]** æ”¯æŒOpenAI APIå…¼å®¹çš„Embeddingæ¨¡å‹æœåŠ¡è¯„æµ‹ï¼ŒæŸ¥çœ‹[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters)
- ğŸ”¥ **[2025.03.27]** æ–°å¢æ”¯æŒ[AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview)å’Œ[ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary)è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** æ¨¡å‹æ¨ç†æœåŠ¡å‹æµ‹æ”¯æŒrandomç”ŸæˆæŒ‡å®šèŒƒå›´é•¿åº¦çš„promptï¼Œå‚è€ƒ[ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#random)
- ğŸ”¥ **[2025.03.13]** æ–°å¢æ”¯æŒ[LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary)ä»£ç è¯„æµ‹åŸºå‡†ï¼ŒæŒ‡å®š`live_code_bench`å³å¯ä½¿ç”¨ï¼›æ”¯æŒQwQ-32B åœ¨LiveCodeBenchä¸Šè¯„æµ‹ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.11]** æ–°å¢æ”¯æŒ[SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary)å’Œ[Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary)è¯„æµ‹åŸºå‡†ï¼Œç”¨ä¸è¯„æµ‹æ¨¡å‹çš„äº‹å®æ­£ç¡®æ€§ï¼ŒæŒ‡å®š`simple_qa`å’Œ`chinese_simpleqa`ä½¿ç”¨ã€‚åŒæ—¶æ”¯æŒæŒ‡å®šè£åˆ¤æ¨¡å‹ï¼Œå‚è€ƒ[ç›¸å…³å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚
- ğŸ”¥ **[2025.03.07]** æ–°å¢QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µï¼Œè¯„æµ‹äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»¥åŠæ¨ç†æ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.04]** æ–°å¢æ”¯æŒ[SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary)æ•°æ®é›†ï¼Œå…¶è¦†ç›– 13 ä¸ªé—¨ç±»ã€72 ä¸ªä¸€çº§å­¦ç§‘å’Œ 285 ä¸ªäºŒçº§å­¦ç§‘ï¼Œå…± 26529 ä¸ªé—®é¢˜ï¼ŒæŒ‡å®š`super_gpqa`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.03.03]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨¡å‹çš„æ™ºå•†å’Œæƒ…å•†ï¼Œå‚è€ƒ[ğŸ“–æ™ºå•†å’Œæƒ…å•†è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/iquiz.html)ï¼Œæ¥æµ‹æµ‹ä½ å®¶çš„AIæœ‰å¤šèªæ˜ï¼Ÿ
- ğŸ”¥ **[2025.02.27]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨ç†æ¨¡å‹çš„æ€è€ƒæ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–æ€è€ƒæ•ˆç‡è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/think_eval.html)ï¼Œè¯¥å®ç°å‚è€ƒäº†[Overthinking](https://doi.org/10.48550/arXiv.2412.21187) å’Œ [Underthinking](https://doi.org/10.48550/arXiv.2501.18585)ä¸¤ç¯‡å·¥ä½œã€‚
- ğŸ”¥ **[2025.02.25]** æ–°å¢æ”¯æŒ[MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR)å’Œ[ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary)ä¸¤ä¸ªæ¨¡å‹æ¨ç†ç›¸å…³è¯„æµ‹åŸºå‡†ï¼Œdatasetsåˆ†åˆ«æŒ‡å®š`musr`å’Œ`process_bench`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.02.18]** æ”¯æŒAIME25æ•°æ®é›†ï¼ŒåŒ…å«15é“é¢˜ç›®ï¼ˆGrok3 åœ¨è¯¥æ•°æ®é›†ä¸Šå¾—åˆ†ä¸º93åˆ†ï¼‰
- ğŸ”¥ **[2025.02.13]** æ”¯æŒDeepSeekè’¸é¦æ¨¡å‹è¯„æµ‹ï¼ŒåŒ…æ‹¬AIME24 MATH-500 GPQA-Diamondæ•°æ®é›†ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/deepseek_r1_distill.html)ï¼›æ”¯æŒæŒ‡å®š`eval_batch_size`å‚æ•°ï¼ŒåŠ é€Ÿæ¨¡å‹è¯„æµ‹
- ğŸ”¥ **[2025.01.20]** æ”¯æŒå¯è§†åŒ–è¯„æµ‹ç»“æœï¼ŒåŒ…æ‹¬å•æ¨¡å‹è¯„æµ‹ç»“æœå’Œå¤šæ¨¡å‹è¯„æµ‹ç»“æœå¯¹æ¯”ï¼Œå‚è€ƒ[ğŸ“–å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ï¼›æ–°å¢[`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary)è¯„æµ‹æ ·ä¾‹ï¼Œè¯„æµ‹æ¨¡å‹çš„IQå’ŒEQã€‚
- ğŸ”¥ **[2025.01.07]** Native backend: æ”¯æŒæ¨¡å‹APIè¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–æ¨¡å‹APIè¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#api)ï¼›æ–°å¢æ”¯æŒ`ifeval`è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ğŸ”¥ **[2024.12.31]** æ”¯æŒåŸºå‡†è¯„æµ‹æ·»åŠ ï¼Œå‚è€ƒ[ğŸ“–åŸºå‡†è¯„æµ‹æ·»åŠ æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ï¼›æ”¯æŒè‡ªå®šä¹‰æ··åˆæ•°æ®é›†è¯„æµ‹ï¼Œç”¨æ›´å°‘çš„æ•°æ®ï¼Œæ›´å…¨é¢çš„è¯„æµ‹æ¨¡å‹ï¼Œå‚è€ƒ[ğŸ“–æ··åˆæ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/collection/index.html)
- ğŸ”¥ **[2024.12.13]** æ¨¡å‹è¯„æµ‹ä¼˜åŒ–ï¼Œä¸å†éœ€è¦ä¼ é€’`--template-type`å‚æ•°ï¼›æ”¯æŒ`evalscope eval --args`å¯åŠ¨è¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html)
- ğŸ”¥ **[2024.11.26]** æ¨¡å‹æ¨ç†å‹æµ‹å·¥å…·é‡æ„å®Œæˆï¼šæ”¯æŒæœ¬åœ°å¯åŠ¨æ¨ç†æœåŠ¡ã€æ”¯æŒSpeed Benchmarkï¼›ä¼˜åŒ–å¼‚æ­¥è°ƒç”¨é”™è¯¯å¤„ç†ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)
- ğŸ”¥ **[2024.10.31]** å¤šæ¨¡æ€RAGè¯„æµ‹æœ€ä½³å®è·µå‘å¸ƒï¼Œå‚è€ƒ[ğŸ“–åšå®¢](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag)
- ğŸ”¥ **[2024.10.23]** æ”¯æŒå¤šæ¨¡æ€RAGè¯„æµ‹ï¼ŒåŒ…æ‹¬[CLIP_Benchmark](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/clip_benchmark.html)è¯„æµ‹å›¾æ–‡æ£€ç´¢å™¨ï¼Œä»¥åŠæ‰©å±•äº†[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)ä»¥æ”¯æŒç«¯åˆ°ç«¯å¤šæ¨¡æ€æŒ‡æ ‡è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.10.8]** æ”¯æŒRAGè¯„æµ‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨[MTEB/CMTEB](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html)è¿›è¡Œembeddingæ¨¡å‹å’Œrerankerçš„ç‹¬ç«‹è¯„æµ‹ï¼Œä»¥åŠä½¿ç”¨[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)è¿›è¡Œç«¯åˆ°ç«¯è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.09.18]** æˆ‘ä»¬çš„æ–‡æ¡£å¢åŠ äº†åšå®¢æ¨¡å—ï¼ŒåŒ…å«ä¸€äº›è¯„æµ‹ç›¸å…³çš„æŠ€æœ¯è°ƒç ”å’Œåˆ†äº«ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/blog/index.html)
- ğŸ”¥ **[2024.09.12]** æ”¯æŒ LongWriter è¯„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºå‡†æµ‹è¯• [LongBench-Write](evalscope/third_party/longbench_write/README.md) æ¥è¯„æµ‹é•¿è¾“å‡ºçš„è´¨é‡ä»¥åŠè¾“å‡ºé•¿åº¦ã€‚
- ğŸ”¥ **[2024.08.30]** æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ•°æ®é›†å’Œå¤šæ¨¡æ€å›¾æ–‡æ•°æ®é›†ã€‚
- ğŸ”¥ **[2024.08.20]** æ›´æ–°äº†å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…æ‹¬å¿«é€Ÿä¸Šæ‰‹ã€æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜ç­‰ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/)ã€‚
- ğŸ”¥ **[2024.08.09]** ç®€åŒ–å®‰è£…æ–¹å¼ï¼Œæ”¯æŒpypiå®‰è£…vlmevalç›¸å…³ä¾èµ–ï¼›ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä½“éªŒï¼ŒåŸºäºOpenAI APIæ–¹å¼çš„è¯„æµ‹é“¾è·¯ï¼Œæœ€é«˜åŠ é€Ÿ10å€ã€‚
- ğŸ”¥ **[2024.07.31]** é‡è¦ä¿®æ”¹ï¼š`llmuses`åŒ…åä¿®æ”¹ä¸º`evalscope`ï¼Œè¯·åŒæ­¥ä¿®æ”¹æ‚¨çš„ä»£ç ã€‚
- ğŸ”¥ **[2024.07.26]** æ”¯æŒ**VLMEvalKit**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œå‘èµ·å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä»»åŠ¡ã€‚
- ğŸ”¥ **[2024.06.29]** æ”¯æŒ**OpenCompass**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†é«˜çº§å°è£…ï¼Œæ”¯æŒpipæ–¹å¼å®‰è£…ï¼Œç®€åŒ–äº†è¯„æµ‹ä»»åŠ¡é…ç½®ã€‚
- ğŸ”¥ **[2024.06.13]** EvalScopeä¸å¾®è°ƒæ¡†æ¶SWIFTè¿›è¡Œæ— ç¼å¯¹æ¥ï¼Œæä¾›LLMä»è®­ç»ƒåˆ°è¯„æµ‹çš„å…¨é“¾è·¯æ”¯æŒ ã€‚
- ğŸ”¥ **[2024.06.13]** æ¥å…¥Agentè¯„æµ‹é›†ToolBenchã€‚
</details>

## â¤ï¸ ç¤¾åŒºä¸æ”¯æŒ

æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºï¼Œä¸å…¶ä»–å¼€å‘è€…äº¤æµå¹¶è·å–å¸®åŠ©ã€‚

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  å¾®ä¿¡ç¾¤ | é’‰é’‰ç¾¤
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬æ¨èä½¿ç”¨ `conda` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä½¿ç”¨ `pip` å®‰è£…ã€‚

1.  **åˆ›å»ºå¹¶æ¿€æ´» Conda ç¯å¢ƒ** (æ¨èä½¿ç”¨ Python 3.10)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **å®‰è£… EvalScope**

    - **æ–¹å¼ä¸€ï¼šé€šè¿‡ PyPI å®‰è£… (æ¨è)**
      ```shell
      pip install evalscope
      ```

    - **æ–¹å¼äºŒï¼šé€šè¿‡æºç å®‰è£… (ç”¨äºå¼€å‘)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **å®‰è£…é¢å¤–ä¾èµ–** (å¯é€‰)
    æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œå®‰è£…ç›¸åº”çš„åŠŸèƒ½æ‰©å±•ï¼š
    ```shell
    # æ€§èƒ½æµ‹è¯•
    pip install 'evalscope[perf]'

    # å¯è§†åŒ–App
    pip install 'evalscope[app]'

    # å…¶ä»–è¯„æµ‹åç«¯
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # å®‰è£…æ‰€æœ‰ä¾èµ–
    pip install 'evalscope[all]'
    ```
    > å¦‚æœæ‚¨é€šè¿‡æºç å®‰è£…ï¼Œè¯·å°† `evalscope` æ›¿æ¢ä¸º `.`ï¼Œä¾‹å¦‚ `pip install '.[perf]'`ã€‚

> [!NOTE]
> æœ¬é¡¹ç›®æ›¾ç”¨å `llmuses`ã€‚å¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ `v0.4.3` æˆ–æ›´æ—©ç‰ˆæœ¬ï¼Œè¯·è¿è¡Œ `pip install llmuses<=0.4.3` å¹¶ä½¿ç”¨ `from llmuses import ...` å¯¼å…¥ã€‚


## ğŸš€ å¿«é€Ÿå¼€å§‹

æ‚¨å¯ä»¥é€šè¿‡**å‘½ä»¤è¡Œ**æˆ– **Python ä»£ç **ä¸¤ç§æ–¹å¼å¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

### æ–¹å¼1. ä½¿ç”¨å‘½ä»¤è¡Œ

åœ¨ä»»æ„è·¯å¾„ä¸‹æ‰§è¡Œ `evalscope eval` å‘½ä»¤å³å¯å¼€å§‹è¯„æµ‹ã€‚ä»¥ä¸‹å‘½ä»¤å°†åœ¨ `gsm8k` å’Œ `arc` æ•°æ®é›†ä¸Šè¯„æµ‹ `Qwen/Qwen2.5-0.5B-Instruct` æ¨¡å‹ï¼Œæ¯ä¸ªæ•°æ®é›†åªå– 5 ä¸ªæ ·æœ¬ã€‚

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### æ–¹å¼2. ä½¿ç”¨Pythonä»£ç 

ä½¿ç”¨ `run_task` å‡½æ•°å’Œ `TaskConfig` å¯¹è±¡æ¥é…ç½®å’Œå¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

```python
from evalscope import run_task TaskConfig

# é…ç½®è¯„æµ‹ä»»åŠ¡
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# å¯åŠ¨è¯„æµ‹
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ æç¤ºï¼š</b> `run_task` è¿˜æ”¯æŒå­—å…¸ã€YAML æˆ– JSON æ–‡ä»¶ä½œä¸ºé…ç½®ã€‚</summary>

**ä½¿ç”¨ Python å­—å…¸**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**ä½¿ç”¨ YAML æ–‡ä»¶** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### è¾“å‡ºç»“æœ
è¯„æµ‹å®Œæˆåï¼Œæ‚¨å°†åœ¨ç»ˆç«¯çœ‹åˆ°å¦‚ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ è¿›é˜¶ç”¨æ³•

### è‡ªå®šä¹‰è¯„æµ‹å‚æ•°

æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ç²¾ç»†åŒ–æ§åˆ¶æ¨¡å‹åŠ è½½ã€æ¨ç†å’Œæ•°æ®é›†é…ç½®ã€‚

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: æ¨¡å‹åŠ è½½å‚æ•°ï¼Œå¦‚ `revision` `precision` ç­‰ã€‚
- `--generation-config`: æ¨¡å‹ç”Ÿæˆå‚æ•°ï¼Œå¦‚ `temperature` `max_tokens` ç­‰ã€‚
- `--dataset-args`: æ•°æ®é›†é…ç½®å‚æ•°ï¼Œå¦‚ `few_shot_num` ç­‰ã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å…¨éƒ¨å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚

### è¯„æµ‹åœ¨çº¿æ¨¡å‹ API

EvalScope æ”¯æŒè¯„æµ‹é€šè¿‡ API éƒ¨ç½²çš„æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ vLLM éƒ¨ç½²çš„æœåŠ¡ï¼‰ã€‚åªéœ€æŒ‡å®šæœåŠ¡åœ°å€å’Œ API Key å³å¯ã€‚

1.  **å¯åŠ¨æ¨¡å‹æœåŠ¡** (ä»¥ vLLM ä¸ºä¾‹)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **è¿è¡Œè¯„æµ‹**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ ç«æŠ€åœºæ¨¡å¼ (Arena)

ç«æŠ€åœºæ¨¡å¼é€šè¿‡æ¨¡å‹é—´çš„ä¸¤ä¸¤å¯¹æˆ˜ï¼ˆPairwise Battleï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ç»™å‡ºèƒœç‡å’Œæ’åï¼Œéå¸¸é€‚åˆå¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”ã€‚

```text
# è¯„æµ‹ç»“æœç¤ºä¾‹
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– ç«æŠ€åœºæ¨¡å¼ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚

### ğŸ–Šï¸ è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹

EvalScope å…è®¸æ‚¨è½»æ¾æ·»åŠ å’Œè¯„æµ‹è‡ªå·±çš„æ•°æ®é›†ã€‚è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/index.html)ã€‚


## ğŸ§ª å…¶ä»–è¯„æµ‹åç«¯
EvalScope æ”¯æŒé€šè¿‡ç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåç«¯â€ï¼‰å‘èµ·è¯„æµ‹ä»»åŠ¡ï¼Œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„è¯„æµ‹éœ€æ±‚ã€‚

- **Native**: EvalScope çš„é»˜è®¤è¯„æµ‹æ¡†æ¶ï¼ŒåŠŸèƒ½å…¨é¢ã€‚
- **OpenCompass**: ä¸“æ³¨äºçº¯æ–‡æœ¬è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: ä¸“æ³¨äºå¤šæ¨¡æ€è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: ä¸“æ³¨äº RAG è¯„æµ‹ï¼Œæ”¯æŒ Embedding å’Œ Reranker æ¨¡å‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/index.html)
- **ç¬¬ä¸‰æ–¹è¯„æµ‹å·¥å…·**: æ”¯æŒ [ToolBench](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html) ç­‰è¯„æµ‹ä»»åŠ¡ã€‚

## âš¡ æ¨ç†æ€§èƒ½è¯„æµ‹å·¥å…·
EvalScope æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„æ€§èƒ½ã€‚

- **å…³é”®æŒ‡æ ‡**: æ”¯æŒååé‡ (Tokens/s)ã€é¦–å­—å»¶è¿Ÿ (TTFT)ã€Token ç”Ÿæˆå»¶è¿Ÿ (TPOT) ç­‰ã€‚
- **ç»“æœè®°å½•**: æ”¯æŒå°†ç»“æœè®°å½•åˆ° `wandb` å’Œ `swanlab`ã€‚
- **é€Ÿåº¦åŸºå‡†**: å¯ç”Ÿæˆç±»ä¼¼å®˜æ–¹æŠ¥å‘Šçš„é€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– æ€§èƒ½æµ‹è¯•ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)ã€‚

è¾“å‡ºç¤ºä¾‹å¦‚ä¸‹ï¼š
<p align="center">
    <img src="docs/zh/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š å¯è§†åŒ–è¯„æµ‹ç»“æœ

EvalScope æä¾›äº†ä¸€ä¸ªåŸºäº Gradio çš„ WebUIï¼Œç”¨äºäº¤äº’å¼åœ°åˆ†æå’Œæ¯”è¾ƒè¯„æµ‹ç»“æœã€‚

1.  **å®‰è£…ä¾èµ–**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **å¯åŠ¨æœåŠ¡**
    ```bash
    evalscope app
    ```
    è®¿é—® `http://127.0.0.1:7861` å³å¯æ‰“å¼€å¯è§†åŒ–ç•Œé¢ã€‚

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/setting.png" alt="Setting" style="width: 90%;" />
      <p>è®¾ç½®ç•Œé¢</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>æ¨¡å‹æ¯”è¾ƒ</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>æŠ¥å‘Šæ¦‚è§ˆ</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_details.png" alt="Report Details" style="width: 91%;" />
      <p>æŠ¥å‘Šè¯¦æƒ…</p>
    </td>
  </tr>
</table>

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ã€‚

## ğŸ‘·â€â™‚ï¸ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ¥è‡ªç¤¾åŒºçš„ä»»ä½•è´¡çŒ®ï¼å¦‚æœæ‚¨å¸Œæœ›æ·»åŠ æ–°çš„è¯„æµ‹åŸºå‡†ã€æ¨¡å‹æˆ–åŠŸèƒ½ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [è´¡çŒ®æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ã€‚

æ„Ÿè°¢æ‰€æœ‰ä¸º EvalScope åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº† EvalScopeï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼š
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

# Arena Mode

Arena mode allows you to configure multiple candidate models and specify a baseline model. The evaluation is conducted through pairwise battles between each candidate model and the baseline model with the win rate and ranking of each model outputted at the end. This approach is suitable for comparative evaluation among multiple models and intuitively reflects the strengths and weaknesses of each model.

## Data Preparation

To support arena mode **all candidate models need to run inference on the same dataset**. The dataset can be a general QA dataset or a domain-specific one. Below is an example using a custom `general_qa` dataset. See the [documentation](../advanced_guides/custom_dataset/llm.md#question-answering-format-qa) for details on using this dataset.

The JSONL file for the `general_qa` dataset should be in the following format. Only the `query` field is required; no additional fields are necessary. Below are two example files:

- Example content of the `arena.jsonl` file:
    ```json
    {"query": "How can I improve my time management skills?"}
    {"query": "What are the most effective ways to deal with stress?"}
    {"query": "What are the main differences between Python and JavaScript programming languages?"}
    {"query": "How can I increase my productivity while working from home?"}
    {"query": "Can you explain the basics of quantum computing?"}
    ```

- Example content of the `example.jsonl` file (with reference answers):
    ```json
    {"query": "What is the capital of France?" "response": "The capital of France is Paris."}
    {"query": "What is the largest mammal in the world?" "response": "The largest mammal in the world is the blue whale."}
    {"query": "How does photosynthesis work?" "response": "Photosynthesis is the process by which green plants use sunlight to synthesize foods with the help of chlorophyll."}
    {"query": "What is the theory of relativity?" "response": "The theory of relativity developed by Albert Einstein describes the laws of physics in relation to observers in different frames of reference."}
    {"query": "Who wrote 'To Kill a Mockingbird'?" "response": "Harper Lee wrote 'To Kill a Mockingbird'."}
    ```

## Candidate Model Inference

After preparing the dataset you can use EvalScope's `run_task` method to perform inference with the candidate models and obtain their outputs for subsequent battles.

Below is an example of how to configure inference tasks for three candidate models: `Qwen2.5-0.5B-Instruct` `Qwen2.5-7B-Instruct` and `Qwen2.5-72B-Instruct` using the same configuration for inference.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task
from evalscope.constants import EvalType

models = ['qwen2.5-72b-instruct' 'qwen2.5-7b-instruct' 'qwen2.5-0.5b-instruct']

task_list = [TaskConfig(
    model=model
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=os.getenv('DASHSCOPE_API_KEY')
    eval_type=EvalType.SERVICE
    datasets=[
        'general_qa'
    ]
    dataset_args={
        'general_qa': {
            'dataset_id': 'custom_eval/text/qa'
            'subset_list': [
                'arena'
                'example'
            ]
        }
    }
    eval_batch_size=10
    generation_config={
        'temperature': 0
        'n': 1
        'max_tokens': 4096
    }) for model in models]

run_task(task_cfg=task_list)
```

<details><summary>Click to view inference results</summary>

Since the `arena` subset does not have reference answers no evaluation metrics are available for this subset. The `example` subset has reference answers so evaluation metrics will be output.
```text
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| Model                 | Dataset    | Metric          | Subset   |   Num |   Score | Cat.0   |
+=======================+============+=================+==========+=======+=========+=========+
| qwen2.5-0.5b-instruct | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-P       | example  |    12 |  0.1341 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-F       | example  |    12 |  0.1983 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-R       | example  |    12 |  0.55   | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-P       | example  |    12 |  0.0404 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-F       | example  |    12 |  0.0716 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-P       | example  |    12 |  0.1193 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-F       | example  |    12 |  0.1754 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-1          | example  |    12 |  0.1192 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-2          | example  |    12 |  0.0403 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-3          | example  |    12 |  0.0135 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-4          | example  |    12 |  0.0079 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-P       | example  |    12 |  0.1149 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-F       | example  |    12 |  0.1612 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-R       | example  |    12 |  0.6833 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-P       | example  |    12 |  0.0813 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-F       | example  |    12 |  0.1027 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-P       | example  |    12 |  0.101  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-F       | example  |    12 |  0.1361 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-1          | example  |    12 |  0.1009 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-2          | example  |    12 |  0.0807 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-P       | example  |    12 |  0.104  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-F       | example  |    12 |  0.1418 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-R       | example  |    12 |  0.7    | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-P       | example  |    12 |  0.078  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-F       | example  |    12 |  0.0964 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-P       | example  |    12 |  0.0942 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-F       | example  |    12 |  0.1235 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-1          | example  |    12 |  0.0939 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-2          | example  |    12 |  0.0777 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
```
</details>

## Candidate Model Battles

Next you can use EvalScope's `general_arena` method to conduct battles among candidate models and get their win rates and rankings on each subset. To achieve robust automatic battles you need to configure an LLM as the judge that compares the outputs of models.

During evaluation EvalScope will automatically parse the public evaluation set of candidate models use the judge model to compare the output of each candidate model with the baseline and determine which is better (to avoid model bias outputs are swapped for two rounds per comparison). The judge model's outputs are parsed as win draw or loss and each candidate model's **Elo score** and **win rate** are calculated.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task

task_cfg = TaskConfig(
    model_id='Arena'  # Model ID is 'Arena'; you can omit specifying model ID
    datasets=[
        'general_arena'  # Must be 'general_arena' indicating arena mode
    ]
    dataset_args={
        'general_arena': {
            # 'system_prompt': 'xxx' # Optional: customize the judge model's system prompt here
            # 'prompt_template': 'xxx' # Optional: customize the judge model's prompt template here
            'extra_params':{
                # Configure candidate model names and corresponding report paths
                # Report paths refer to the output paths from the previous step for parsing model inference results
                'models':[
                    {
                        'name': 'qwen2.5-0.5b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-0.5b-instruct'
                    }
                    {
                        'name': 'qwen2.5-7b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-7b-instruct'
                    }
                    {
                        'name': 'qwen2.5-72b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-72b-instruct'
                    }
                ]
                # Set baseline model must be one of the candidate models
                'baseline': 'qwen2.5-7b'
            }
        }
    }
    # Configure judge model parameters
    judge_model_args={
        'model_id': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': os.getenv('DASHSCOPE_API_KEY')
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 8000
        }
    }
    judge_worker_num=5
    # use_cache='outputs/xxx' # Optional: to add new candidate models to existing results specify the existing results path
)

run_task(task_cfg=task_cfg)
```

<details><summary>Click to view evaluation results</summary>

```text
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Model   | Dataset       | Metric        | Subset                                     |   Num |   Score | Cat.0   |
+=========+===============+===============+============================================+=======+=========+=========+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.5469 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.075  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.8382 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | OVERALL                                    |    44 |  0.3617 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.3906 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.025  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.7276 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | OVERALL                                    |    44 |  0.2826 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.6875 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.9412 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | OVERALL                                    |    44 |  0.4469 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+ 
```
</details>


The automatically generated model leaderboard is as follows (output file located in `outputs/xxx/reports/Arena/leaderboard.txt`):

The leaderboard is sorted by win rate in descending order. As shown the `qwen2.5-72b` model performs best across all subsets with the highest win rate while the `qwen2.5-0.5b` model performs the worst.

```text
=== OVERALL LEADERBOARD ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== DATASET LEADERBOARD: general_qa ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== SUBSET LEADERBOARD: general_qa - example ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            54.7  (-15.6 / +14.1)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            1.8  (+0.0 / +7.2)

=== SUBSET LEADERBOARD: general_qa - arena ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            83.8  (-11.1 / +10.3)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            7.5  (-5.0 / +1.6)
```

## Visualization of Battle Results

To intuitively display the results of the battles between candidate models and the baseline EvalScope provides a visualization feature allowing you to compare the results of each candidate model against the baseline model for each sample.

Run the command below to launch the visualization interface:
```shell
evalscope app
```
Open `http://localhost:7860` in your browser to view the visualization page.

Workflow:
1. Select the latest `general_arena` evaluation report and click the "Load and View" button.
2. Click dataset details and select the battle results between your candidate model and the baseline.
3. Adjust the threshold to filter battle results (normalized scores range from 0-1; 0.5 indicates a tie scores above 0.5 indicate the candidate is better than the baseline below 0.5 means worse).

Example below: a battle between `qwen2.5-72b` and `qwen2.5-7b`. The model judged the 72b as better:

![image](https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/arena_example.jpg)


# Sandbox Environment Usage

To complete LLM code capability evaluation we need to set up an independent evaluation environment to avoid executing erroneous code in the development environment and causing unavoidable losses. Currently EvalScope has integrated the [ms-enclave](https://github.com/modelscope/ms-enclave) sandbox environment allowing users to evaluate model code capabilities in a controlled environment such as using evaluation benchmarks like HumanEval and LiveCodeBench.

The following introduces two different sandbox usage methods:

- Local usage: Set up the sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine;
- Remote usage: Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

## 1. Local Usage

Use Docker to set up a sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine.

### Environment Setup

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in your local Python environment:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration
When running evaluations add the `use_sandbox` and `sandbox_type` parameters to automatically enable the sandbox environment. Other parameters remain the same as regular evaluations:

Here's a complete example code for model evaluation on HumanEval:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will automatically start and manage the sandbox environment ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] Local sandbox manager started
...
```

## 2. Remote Usage

Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

### Environment Setup

You need to install and configure separately on both the remote machine and local machine.

#### Remote Machine

The environment installation on the remote machine is similar to the local usage method described above:

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in remote Python environment:

```bash
pip install evalscope[sandbox]
```

3. **Start sandbox server**: Run the following command to start the sandbox server:

```bash
ms-enclave server --host 0.0.0.0 --port 1234
```

#### Local Machine

The local machine does not need Docker installation at this point but needs to install EvalScope:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration

When running evaluations add the `use_sandbox` parameter to automatically enable the sandbox environment and specify the remote sandbox server's API address in `sandbox_manager_config`:

Complete example code is as follows:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    sandbox_manager_config={
        'base_url': 'http://<remote_host>:1234'  # remote sandbox manager URL
    }
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will communicate with the remote sandbox server through API ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] HTTP sandbox manager started connected to http://<remote_host>:1234
...
```


# EvalScope Service Deployment

## Introduction

EvalScope service mode provides HTTP API-based evaluation and stress testing capabilities designed to address the following scenarios:

1. **Remote Invocation**: Support remote evaluation functionality through network without configuring complex evaluation environments locally
2. **Service Integration**: Easily integrate evaluation capabilities into existing workflows CI/CD pipelines or automated testing systems
3. **Multi-user Collaboration**: Support multiple users or systems calling the evaluation service simultaneously improving resource utilization
4. **Unified Management**: Centrally manage evaluation resources and configurations for easier maintenance and monitoring
5. **Flexible Deployment**: Can be deployed on dedicated servers or container environments decoupled from business systems

The Flask service encapsulates EvalScope's core evaluation (eval) and stress testing (perf) functionalities providing services through standard RESTful APIs making evaluation capabilities callable and integrable like other microservices.

## Features

- **Model Evaluation** (`/api/v1/eval`): Support evaluation of OpenAI API-compatible models
- **Performance Testing** (`/api/v1/perf`): Support performance benchmarking of OpenAI API-compatible models
- **Parameter Query**: Provide parameter description endpoints

## Environment Setup


### Full Installation (Recommended)

```bash
pip install evalscope[service]
```

### Development Environment Installation

```bash
# Clone repository
git clone https://github.com/modelscope/evalscope.git
cd evalscope

# Install development version with service
pip install -e '.[service]'
```

## Starting the Service

### Command Line Launch

```bash
# Use default configuration (host: 0.0.0.0 port: 9000)
evalscope service

# Custom host and port
evalscope service --host 127.0.0.1 --port 9000

# Enable debug mode
evalscope service --debug
```

### Python Code Launch

```python
from evalscope.service import run_service

# Start service
run_service(host='0.0.0.0' port=9000 debug=False)
```

## API Endpoints

### 1. Health Check

```bash
GET /health
```

**Response Example:**
```json
{
  "status": "ok"
  "service": "evalscope"
  "timestamp": "2025-12-04T10:00:00"
}
```

### 2. Model Evaluation

```bash
POST /api/v1/eval
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
  "api_key": "your-api-key"
  "datasets": ["gsm8k" "iquiz"]
  "limit": 10
  "generation_config": {
    "temperature": 0.0
    "max_tokens": 2048
  }
}
```

**Required Parameters:**
- `model`: Model name
- `datasets`: List of datasets
- `api_url`: API endpoint URL (OpenAI-compatible)

**Optional Parameters:**
- `api_key`: API key (default: "EMPTY")
- `limit`: Evaluation sample quantity limit
- `eval_batch_size`: Batch size (default: 1)
- `generation_config`: Generation configuration
  - `temperature`: Temperature parameter (default: 0.0)
  - `max_tokens`: Maximum generation tokens (default: 2048)
  - `top_p`: Nucleus sampling parameter
  - `top_k`: Top-k sampling parameter
- `work_dir`: Output directory
- `debug`: Debug mode
- `seed`: Random seed (default: 42)

**Response Example:**
```json
{
  "status": "success"
  "message": "Evaluation completed"
  "result": {"...": "..."}
  "output_dir": "/path/to/outputs/20251204_100000"
}
```

### 3. Performance Testing

```bash
POST /api/v1/perf
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  "api": "openai"
  "api_key": "your-api-key"
  "number": 100
  "parallel": 10
  "dataset": "openqa"
  "max_tokens": 2048
  "temperature": 0.0
}
```

**Required Parameters:**
- `model`: Model name
- `url`: Complete API endpoint URL

**Optional Parameters:**
- `api`: API type (openai/dashscope/anthropic/gemini default: "openai")
- `api_key`: API key
- `number`: Total number of requests (default: 1000)
- `parallel`: Concurrency level (default: 1)
- `rate`: Requests per second limit (default: -1 unlimited)
- `dataset`: Dataset name (default: "openqa")
- `max_tokens`: Maximum generation tokens (default: 2048)
- `temperature`: Temperature parameter (default: 0.0)
- `stream`: Whether to use streaming output (default: true)
- `debug`: Debug mode

**Response Example:**
```json
{
  "status": "success"
  "message": "Performance test completed"
  "output_dir": "/path/to/outputs"
  "results": {
    "parallel_10_number_100": {
      "metrics": {"...": "..."}
      "percentiles": {"...": "..."}
    }
  }
}
```

### 4. Get Evaluation Parameter Description

```bash
GET /api/v1/eval/params
```

Returns descriptions of all parameters supported by the evaluation endpoint.

### 5. Get Performance Test Parameter Description

```bash
GET /api/v1/perf/params
```

Returns descriptions of all parameters supported by the performance test endpoint.

## Usage Examples

### Testing Evaluation Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "your-api-key"
    "datasets": ["gsm8k"]
    "limit": 5
  }'
```

### Testing Performance Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    "api": "openai"
    "number": 50
    "parallel": 5
  }'
```

### Using Python requests

```python
import requests

# Evaluation request
eval_response = requests.post(
    'http://localhost:9000/api/v1/eval'
    json={
        'model': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': 'your-api-key'
        'datasets': ['gsm8k' 'iquiz']
        'limit': 10
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 2048
        }
    }
)
print(eval_response.json())

# Performance test request
perf_response = requests.post(
    'http://localhost:9000/api/v1/perf'
    json={
        'model': 'qwen-plus'
        'url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions'
        'api': 'openai'
        'number': 100
        'parallel': 10
        'dataset': 'openqa'
    }
)
print(perf_response.json())
```

## Important Notes

1. **OpenAI API-Compatible Models Only**: This service is designed specifically for OpenAI API-compatible models
2. **Long-Running Tasks**: Evaluation and performance testing tasks may take considerable time. We recommend setting appropriate HTTP timeout values on the client side as the API calls are synchronous and will block until completion.
3. **Output Directory**: Evaluation results are saved in the configured `work_dir` default is `outputs/`
4. **Error Handling**: The service returns detailed error messages and stack traces (in debug mode)
5. **Resource Management**: Pay attention to concurrency settings during stress testing to avoid server overload

## Error Codes

- `400`: Invalid request parameters
- `404`: Endpoint not found
- `500`: Internal server error

## Example Scenarios

### Scenario 1: Quick Evaluation of Qwen Model

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "sk-..."
    "datasets": ["gsm8k"]
    "limit": 100
  }'
```

### Scenario 2: Stress Testing Locally Deployed Model

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5"
    "url": "http://localhost:8000/v1/chat/completions"
    "api": "openai"
    "number": 1000
    "parallel": 20
    "max_tokens": 2048
  }'
```

### Scenario 3: Multi-Dataset Evaluation

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "datasets": ["gsm8k" "iquiz" "ceval"]
    "limit": 50
    "eval_batch_size": 4
  }'
```

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  <a href="README_zh.md">ä¸­æ–‡</a> &nbsp ï½œ &nbsp English &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  Chinese Documentation</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documentation</a>
<p>


> â­ If you like this project please click the "Star" button in the upper right corner to support us. Your support is our motivation to move forward!

## ğŸ“ Introduction

EvalScope is a powerful and easily extensible model evaluation framework created by the [ModelScope Community](https://modelscope.cn/) aiming to provide a one-stop evaluation solution for large model developers.

Whether you want to evaluate the general capabilities of models conduct multi-model performance comparisons or need to stress test models EvalScope can meet your needs.

## âœ¨ Key Features

- **ğŸ“š Comprehensive Evaluation Benchmarks**: Built-in multiple industry-recognized evaluation benchmarks including MMLU C-Eval GSM8K and more.
- **ğŸ§© Multi-modal and Multi-domain Support**: Supports evaluation of various model types including Large Language Models (LLM) Vision Language Models (VLM) Embedding Reranker AIGC and more.
- **ğŸš€ Multi-backend Integration**: Seamlessly integrates multiple evaluation backends including OpenCompass VLMEvalKit RAGEval to meet different evaluation needs.
- **âš¡ Inference Performance Testing**: Provides powerful model service stress testing tools supporting multiple performance metrics such as TTFT TPOT.
- **ğŸ“Š Interactive Reports**: Provides WebUI visualization interface supporting multi-dimensional model comparison report overview and detailed inspection.
- **âš”ï¸ Arena Mode**: Supports multi-model battles (Pairwise Battle) intuitively ranking and evaluating models.
- **ğŸ”§ Highly Extensible**: Developers can easily add custom datasets models and evaluation metrics.

<details><summary>ğŸ›ï¸ Overall Architecture</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope Overall Architecture.
</p>

1.  **Input Layer**
    - **Model Sources**: API models (OpenAI API) Local models (ModelScope)
    - **Datasets**: Standard evaluation benchmarks (MMLU/GSM8k etc.) Custom data (MCQ/QA)

2.  **Core Functions**
    - **Multi-backend Evaluation**: Native backend OpenCompass MTEB VLMEvalKit RAGAS
    - **Performance Monitoring**: Supports multiple model service APIs and data formats tracking TTFT/TPOP and other metrics
    - **Tool Extensions**: Integrates Tool-Bench Needle-in-a-Haystack etc.

3.  **Output Layer**
    - **Structured Reports**: Supports JSON Table Logs
    - **Visualization Platform**: Supports Gradio Wandb SwanLab

</details>

## ğŸ‰ What's New

> [!IMPORTANT]
> **Version 1.0 Refactoring**
>
> Version 1.0 introduces a major overhaul of the evaluation framework establishing a new more modular and extensible API layer under `evalscope/api`. Key improvements include standardized data models for benchmarks samples and results; a registry-based design for components such as benchmarks and metrics; and a rewritten core evaluator that orchestrates the new architecture. Existing benchmark adapters have been migrated to this API resulting in cleaner more consistent and easier-to-maintain implementations.

- ğŸ”¥ **[2025.12.02]** Added support for custom multimodal VQA evaluation; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/vlm.html). Added support for visualizing model service stress testing in ClearML; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#clearml).
- ğŸ”¥ **[2025.11.26]** Added support for OpenAI-MRCR GSM8K-V MGSM MicroVQA IFBench SciCode benchmarks.
- ğŸ”¥ **[2025.11.18]** Added support for custom Function-Call (tool invocation) datasets to test whether models can timely and correctly call tools. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#function-calling-format-fc).
- ğŸ”¥ **[2025.11.14]** Added support for SWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini code evaluation benchmarks. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/swe_bench.html).
- ğŸ”¥ **[2025.11.12]** Added `pass@k` `vote@k` `pass^k` and other metric aggregation methods; added support for multimodal evaluation benchmarks such as A_OKVQA CMMU ScienceQA V*Bench.
- ğŸ”¥ **[2025.11.07]** Added support for Ï„Â²-bench an extended and enhanced version of Ï„-bench that includes a series of code fixes and adds telecom domain troubleshooting scenarios. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/tau2_bench.html).
- ğŸ”¥ **[2025.10.30]** Added support for BFCL-v4 enabling evaluation of agent capabilities including web search and long-term memory. See the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v4.html).
- ğŸ”¥ **[2025.10.27]** Added support for LogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQA and other evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.26]** Added support for Conll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 and other Named Entity Recognition evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.21]** Optimized sandbox environment usage in code evaluation supporting both local and remote operation modes. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html).
- ğŸ”¥ **[2025.10.20]** Added support for evaluation benchmarks including PolyMath SimpleVQA MathVerse MathVision AA-LCR; optimized evalscope perf performance to align with vLLM Bench. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/vs_vllm_bench.html).
- ğŸ”¥ **[2025.10.14]** Added support for OCRBench OCRBench-v2 DocVQA InfoVQA ChartQA and BLINK multimodal image-text evaluation benchmarks.
- ğŸ”¥ **[2025.09.22]** Code evaluation benchmarks (HumanEval LiveCodeBench) now support running in a sandbox environment. To use this feature please install [ms-enclave](https://github.com/modelscope/ms-enclave) first.
- ğŸ”¥ **[2025.09.19]** Added support for multimodal image-text evaluation benchmarks including RealWorldQA AI2D MMStar MMBench and OmniBench as well as pure text evaluation benchmarks such as Multi-IF HealthBench and AMC.
- ğŸ”¥ **[2025.09.05]** Added support for vision-language multimodal model evaluation tasks such as MathVista and MMMU. For more supported datasets please [refer to the documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/vlm.html).
- ğŸ”¥ **[2025.09.04]** Added support for image editing task evaluation including the [GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/image_edit.html).
- ğŸ”¥ **[2025.08.22]** Version 1.0 Refactoring. Break changes please [refer to](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#switching-to-version-v1-0).
<details><summary>More</summary>

- ğŸ”¥ **[2025.07.18]** The model stress testing now supports randomly generating image-text data for multimodal model evaluation. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#id4).
- ğŸ”¥ **[2025.07.16]** Support for [Ï„-bench](https://github.com/sierra-research/tau-bench) has been added enabling the evaluation of AI Agent performance and reliability in real-world scenarios involving dynamic user and tool interactions. For usage instructions please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#bench).
- ğŸ”¥ **[2025.07.14]** Support for "Humanity's Last Exam" ([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle)) a highly challenging evaluation benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam).
- ğŸ”¥ **[2025.07.03]** Refactored Arena Mode: now supports custom model battles outputs a model leaderboard and provides battle result visualization. See [reference](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html) for details.
- ğŸ”¥ **[2025.06.28]** Optimized custom dataset evaluation: now supports evaluation without reference answers. Enhanced LLM judge usage with built-in modes for "scoring directly without reference answers" and "checking answer consistency with reference answers". See [reference](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa) for details.
- ğŸ”¥ **[2025.06.19]** Added support for the [BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3) benchmark designed to evaluate model function-calling capabilities across various scenarios. For more information refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v3.html).
- ğŸ”¥ **[2025.06.02]** Added support for the Needle-in-a-Haystack test. Simply specify `needle_haystack` to conduct the test and a corresponding heatmap will be generated in the `outputs/reports` folder providing a visual representation of the model's performance. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/needle_haystack.html) for more details.
- ğŸ”¥ **[2025.05.29]** Added support for two long document evaluation benchmarks: [DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary) and [FRAMES](https://modelscope.cn/datasets/iic/frames/summary). For usage guidelines please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html).
- ğŸ”¥ **[2025.05.16]** Model service performance stress testing now supports setting various levels of concurrency and outputs a performance test report. [Reference example](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/quick_start.html#id3).
- ğŸ”¥ **[2025.05.13]** Added support for the [ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static) dataset to evaluate model's tool-calling capabilities. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html) for usage instructions. Also added support for the [DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview) and [Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val) benchmarks to assess the reasoning capabilities of models.
- ğŸ”¥ **[2025.04.29]** Added Qwen3 Evaluation Best Practices [welcome to read ğŸ“–](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** Support for text-to-image evaluation: Supports 8 metrics including MPS HPSv2.1Score etc. and evaluation benchmarks such as EvalMuse GenAI-Bench. Refer to the [user documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/t2i.html) for more details.
- ğŸ”¥ **[2025.04.10]** Model service stress testing tool now supports the `/v1/completions` endpoint (the default endpoint for vLLM benchmarking)
- ğŸ”¥ **[2025.04.08]** Support for evaluating embedding model services compatible with the OpenAI API has been added. For more details check the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters).
- ğŸ”¥ **[2025.03.27]** Added support for [AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview) and [ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary) evaluation benchmarks. For usage notes please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** The model inference service stress testing now supports generating prompts of specified length using random values. Refer to the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#using-the-random-dataset) for more details.
- ğŸ”¥ **[2025.03.13]** Added support for the [LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary) code evaluation benchmark which can be used by specifying `live_code_bench`. Supports evaluating QwQ-32B on LiveCodeBench refer to the [best practices](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html).
- ğŸ”¥ **[2025.03.11]** Added support for the [SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary) and [Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary) evaluation benchmarks. These are used to assess the factual accuracy of models and you can specify `simple_qa` and `chinese_simpleqa` for use. Support for specifying a judge model is also available. For more details refer to the [relevant parameter documentation](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).
- ğŸ”¥ **[2025.03.07]** Added support for the [QwQ-32B](https://modelscope.cn/models/Qwen/QwQ-32B/summary) model evaluate the model's reasoning ability and reasoning efficiency refer to [ğŸ“– Best Practices for QwQ-32B Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html) for more details.
- ğŸ”¥ **[2025.03.04]** Added support for the [SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary) dataset which covers 13 categories 72 first-level disciplines and 285 second-level disciplines totaling 26529 questions. You can use it by specifying `super_gpqa`.
- ğŸ”¥ **[2025.03.03]** Added support for evaluating the IQ and EQ of models. Refer to [ğŸ“– Best Practices for IQ and EQ Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html) to find out how smart your AI is!
- ğŸ”¥ **[2025.02.27]** Added support for evaluating the reasoning efficiency of models. Refer to [ğŸ“– Best Practices for Evaluating Thinking Efficiency](https://evalscope.readthedocs.io/en/latest/best_practice/think_eval.html). This implementation is inspired by the works [Overthinking](https://doi.org/10.48550/arXiv.2412.21187) and [Underthinking](https://doi.org/10.48550/arXiv.2501.18585).
- ğŸ”¥ **[2025.02.25]** Added support for two model inference-related evaluation benchmarks: [MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR) and [ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary). To use them simply specify `musr` and `process_bench` respectively in the datasets parameter.
- ğŸ”¥ **[2025.02.18]** Supports the AIME25 dataset which contains 15 questions (Grok3 scored 93 on this dataset).
- ğŸ”¥ **[2025.02.13]** Added support for evaluating DeepSeek distilled models including AIME24 MATH-500 and GPQA-Diamond datasetsï¼Œrefer to [best practice](https://evalscope.readthedocs.io/en/latest/best_practice/deepseek_r1_distill.html); Added support for specifying the `eval_batch_size` parameter to accelerate model evaluation.
- ğŸ”¥ **[2025.01.20]** Support for visualizing evaluation results including single model evaluation results and multi-model comparison refer to the [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html) for more details; Added [`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary) evaluation example evaluating the IQ and EQ of the model.
- ğŸ”¥ **[2025.01.07]** Native backend: Support for model API evaluation is now available. Refer to the [ğŸ“– Model API Evaluation Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#api) for more details. Additionally support for the `ifeval` evaluation benchmark has been added.
- ğŸ”¥ğŸ”¥ **[2024.12.31]** Support for adding benchmark evaluations refer to the [ğŸ“– Benchmark Evaluation Addition Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html); support for custom mixed dataset evaluations allowing for more comprehensive model evaluations with less data refer to the [ğŸ“– Mixed Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/collection/index.html).
- ğŸ”¥ **[2024.12.13]** Model evaluation optimization: no need to pass the `--template-type` parameter anymore; supports starting evaluation with `evalscope eval --args`. Refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html) for more details.
- ğŸ”¥ **[2024.11.26]** The model inference service performance evaluator has been completely refactored: it now supports local inference service startup and Speed Benchmark; asynchronous call error handling has been optimized. For more details refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).
- ğŸ”¥ **[2024.10.31]** The best practice for evaluating Multimodal-RAG has been updated please check the [ğŸ“– Blog](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag) for more details.
- ğŸ”¥ **[2024.10.23]** Supports multimodal RAG evaluation including the assessment of image-text retrieval using [CLIP_Benchmark](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/clip_benchmark.html) and extends [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html) to support end-to-end multimodal metrics evaluation.
- ğŸ”¥ **[2024.10.8]** Support for RAG evaluation including independent evaluation of embedding models and rerankers using [MTEB/CMTEB](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html) as well as end-to-end evaluation using [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html).
- ğŸ”¥ **[2024.09.18]** Our documentation has been updated to include a blog module featuring some technical research and discussions related to evaluations. We invite you to [ğŸ“– read it](https://evalscope.readthedocs.io/en/refact_readme/blog/index.html).
- ğŸ”¥ **[2024.09.12]** Support for LongWriter evaluation which supports 10000+ word generation. You can use the benchmark [LongBench-Write](evalscope/third_party/longbench_write/README.md) to measure the long output quality as well as the output length.
- ğŸ”¥ **[2024.08.30]** Support for custom dataset evaluations including text datasets and multimodal image-text datasets.
- ğŸ”¥ **[2024.08.20]** Updated the official documentation including getting started guides best practices and FAQs. Feel free to [ğŸ“–read it here](https://evalscope.readthedocs.io/en/latest/)!
- ğŸ”¥ **[2024.08.09]** Simplified the installation process allowing for pypi installation of vlmeval dependencies; optimized the multimodal model evaluation experience achieving up to 10x acceleration based on the OpenAI API evaluation chain.
- ğŸ”¥ **[2024.07.31]** Important change: The package name `llmuses` has been changed to `evalscope`. Please update your code accordingly.
- ğŸ”¥ **[2024.07.26]** Support for **VLMEvalKit** as a third-party evaluation framework to initiate multimodal model evaluation tasks.
- ğŸ”¥ **[2024.06.29]** Support for **OpenCompass** as a third-party evaluation framework which we have encapsulated at a higher level supporting pip installation and simplifying evaluation task configuration.
- ğŸ”¥ **[2024.06.13]** EvalScope seamlessly integrates with the fine-tuning framework SWIFT providing full-chain support from LLM training to evaluation.
- ğŸ”¥ **[2024.06.13]** Integrated the Agent evaluation dataset ToolBench.

</details>

## â¤ï¸ Community & Support

Welcome to join our community to communicate with other developers and get help.

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  WeChat Group | DingTalk Group
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ Environment Setup

We recommend using `conda` to create a virtual environment and install with `pip`.

1.  **Create and Activate Conda Environment** (Python 3.10 recommended)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **Install EvalScope**

    - **Method 1: Install via PyPI (Recommended)**
      ```shell
      pip install evalscope
      ```

    - **Method 2: Install from Source (For Development)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **Install Additional Dependencies** (Optional)
    Install corresponding feature extensions according to your needs:
    ```shell
    # Performance testing
    pip install 'evalscope[perf]'

    # Visualization App
    pip install 'evalscope[app]'

    # Other evaluation backends
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # Install all dependencies
    pip install 'evalscope[all]'
    ```
    > If you installed from source please replace `evalscope` with `.` for example `pip install '.[perf]'`.

> [!NOTE]
> This project was formerly known as `llmuses`. If you need to use `v0.4.3` or earlier versions please run `pip install llmuses<=0.4.3` and use `from llmuses import ...` for imports.


## ğŸš€ Quick Start

You can start evaluation tasks in two ways: **command line** or **Python code**.

### Method 1. Using Command Line

Execute the `evalscope eval` command in any path to start evaluation. The following command will evaluate the `Qwen/Qwen2.5-0.5B-Instruct` model on `gsm8k` and `arc` datasets taking only 5 samples from each dataset.

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### Method 2. Using Python Code

Use the `run_task` function and `TaskConfig` object to configure and start evaluation tasks.

```python
from evalscope import run_task TaskConfig

# Configure evaluation task
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# Start evaluation
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ Tip:</b> `run_task` also supports dictionaries YAML or JSON files as configuration.</summary>

**Using Python Dictionary**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**Using YAML File** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### Output Results
After evaluation completion you will see a report in the terminal in the following format:
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ Advanced Usage

### Custom Evaluation Parameters

You can fine-tune model loading inference and dataset configuration through command line parameters.

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: Model loading parameters such as `revision` `precision` etc.
- `--generation-config`: Model generation parameters such as `temperature` `max_tokens` etc.
- `--dataset-args`: Dataset configuration parameters such as `few_shot_num` etc.

For details please refer to [ğŸ“– Complete Parameter Guide](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).

### Evaluating Online Model APIs

EvalScope supports evaluating model services deployed via APIs (such as services deployed with vLLM). Simply specify the service address and API Key.

1.  **Start Model Service** (using vLLM as example)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **Run Evaluation**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ Arena Mode

Arena mode evaluates model performance through pairwise battles between models providing win rates and rankings perfect for horizontal comparison of multiple models.

```text
# Example evaluation results
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
For details please refer to [ğŸ“– Arena Mode Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html).

### ğŸ–Šï¸ Custom Dataset Evaluation

EvalScope allows you to easily add and evaluate your own datasets. For details please refer to [ğŸ“– Custom Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html).


## ğŸ§ª Other Evaluation Backends
EvalScope supports launching evaluation tasks through third-party evaluation frameworks (we call them "backends") to meet diverse evaluation needs.

- **Native**: EvalScope's default evaluation framework with comprehensive functionality.
- **OpenCompass**: Focuses on text-only evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: Focuses on multi-modal evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: Focuses on RAG evaluation supporting Embedding and Reranker models. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/index.html)
- **Third-party Evaluation Tools**: Supports evaluation tasks like [ToolBench](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html).

## âš¡ Inference Performance Evaluation Tool
EvalScope provides a powerful stress testing tool for evaluating the performance of large language model services.

- **Key Metrics**: Supports throughput (Tokens/s) first token latency (TTFT) token generation latency (TPOT) etc.
- **Result Recording**: Supports recording results to `wandb` and `swanlab`.
- **Speed Benchmarks**: Can generate speed benchmark results similar to official reports.

For details please refer to [ğŸ“– Performance Testing Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).

Example output is shown below:
<p align="center">
    <img src="docs/en/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š Visualizing Evaluation Results

EvalScope provides a Gradio-based WebUI for interactive analysis and comparison of evaluation results.

1.  **Install Dependencies**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **Start Service**
    ```bash
    evalscope app
    ```
    Visit `http://127.0.0.1:7861` to open the visualization interface.

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/setting.png" alt="Setting" style="width: 85%;" />
      <p>Settings Interface</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>Model Comparison</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>Report Overview</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_details.png" alt="Report Details" style="width: 85%;" />
      <p>Report Details</p>
    </td>
  </tr>
</table>

For details please refer to [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html).

## ğŸ‘·â€â™‚ï¸ Contributing

We welcome any contributions from the community! If you want to add new evaluation benchmarks models or features please refer to our [Contributing Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html).

Thanks to all developers who have contributed to EvalScope!

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š Citation

If you use EvalScope in your research please cite our work:
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  ä¸­æ–‡ &nbsp ï½œ &nbsp <a href="evalscope.md">English</a> &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/zh-cn/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  ä¸­æ–‡æ–‡æ¡£</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documents</a>
<p>


> â­ å¦‚æœä½ å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ "Star" æŒ‰é’®æ”¯æŒæˆ‘ä»¬ã€‚ä½ çš„æ”¯æŒæ˜¯æˆ‘ä»¬å‰è¿›çš„åŠ¨åŠ›ï¼

## ğŸ“ ç®€ä»‹

EvalScope æ˜¯ç”±[é­”æ­ç¤¾åŒº](https://modelscope.cn/)æ‰“é€ çš„ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§ã€æ˜“äºæ‰©å±•çš„æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¤§æ¨¡å‹å¼€å‘è€…æä¾›ä¸€ç«™å¼è¯„æµ‹è§£å†³æ–¹æ¡ˆã€‚

æ— è®ºæ‚¨æ˜¯æƒ³è¯„ä¼°æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€è¿›è¡Œå¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼Œè¿˜æ˜¯éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼ŒEvalScope éƒ½èƒ½æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- **ğŸ“š å…¨é¢çš„è¯„æµ‹åŸºå‡†**: å†…ç½® MMLU C-Eval GSM8K ç­‰å¤šä¸ªä¸šç•Œå…¬è®¤çš„è¯„æµ‹åŸºå‡†ã€‚
- **ğŸ§© å¤šæ¨¡æ€ä¸å¤šé¢†åŸŸæ”¯æŒ**: æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ (LLM)ã€å¤šæ¨¡æ€ (VLM)ã€Embeddingã€Rerankerã€AIGC ç­‰å¤šç§æ¨¡å‹çš„è¯„æµ‹ã€‚
- **ğŸš€ å¤šåç«¯é›†æˆ**: æ— ç¼é›†æˆ OpenCompass VLMEvalKit RAGEval ç­‰å¤šç§è¯„æµ‹åç«¯ï¼Œæ»¡è¶³ä¸åŒè¯„æµ‹éœ€æ±‚ã€‚
- **âš¡ æ¨ç†æ€§èƒ½æµ‹è¯•**: æä¾›å¼ºå¤§çš„æ¨¡å‹æœåŠ¡å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒ TTFT TPOT ç­‰å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ã€‚
- **ğŸ“Š äº¤äº’å¼æŠ¥å‘Š**: æä¾› WebUI å¯è§†åŒ–ç•Œé¢ï¼Œæ”¯æŒå¤šç»´åº¦æ¨¡å‹å¯¹æ¯”ã€æŠ¥å‘Šæ¦‚è§ˆå’Œè¯¦æƒ…æŸ¥é˜…ã€‚
- **âš”ï¸ ç«æŠ€åœºæ¨¡å¼**: æ”¯æŒå¤šæ¨¡å‹å¯¹æˆ˜ (Pairwise Battle)ï¼Œç›´è§‚åœ°å¯¹æ¨¡å‹è¿›è¡Œæ’åå’Œè¯„ä¼°ã€‚
- **ğŸ”§ é«˜åº¦å¯æ‰©å±•**: å¼€å‘è€…å¯ä»¥è½»æ¾æ·»åŠ è‡ªå®šä¹‰æ•°æ®é›†ã€æ¨¡å‹å’Œè¯„æµ‹æŒ‡æ ‡ã€‚

<details><summary>ğŸ›ï¸ æ•´ä½“æ¶æ„</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope æ•´ä½“æ¶æ„å›¾.
</p>

1.  **è¾“å…¥å±‚**
    - **æ¨¡å‹æ¥æº**: APIæ¨¡å‹ï¼ˆOpenAI APIï¼‰ã€æœ¬åœ°æ¨¡å‹ï¼ˆModelScopeï¼‰
    - **æ•°æ®é›†**: æ ‡å‡†è¯„æµ‹åŸºå‡†ï¼ˆMMLU/GSM8kç­‰ï¼‰ã€è‡ªå®šä¹‰æ•°æ®ï¼ˆMCQ/QAï¼‰

2.  **æ ¸å¿ƒåŠŸèƒ½**
    - **å¤šåç«¯è¯„ä¼°**: åŸç”Ÿåç«¯ã€OpenCompassã€MTEBã€VLMEvalKitã€RAGAS
    - **æ€§èƒ½ç›‘æ§**: æ”¯æŒå¤šç§æ¨¡å‹æœåŠ¡ API å’Œæ•°æ®æ ¼å¼ï¼Œè¿½è¸ª TTFT/TPOP ç­‰æŒ‡æ ‡
    - **å·¥å…·æ‰©å±•**: é›†æˆ Tool-Bench Needle-in-a-Haystack ç­‰

3.  **è¾“å‡ºå±‚**
    - **ç»“æ„åŒ–æŠ¥å‘Š**: æ”¯æŒ JSON Table Logs
    - **å¯è§†åŒ–å¹³å°**: æ”¯æŒ Gradio Wandb SwanLab

</details>

## ğŸ‰ å†…å®¹æ›´æ–°

> [!IMPORTANT]
> **ç‰ˆæœ¬ 1.0 é‡æ„**
>
> ç‰ˆæœ¬ 1.0 å¯¹è¯„æµ‹æ¡†æ¶è¿›è¡Œäº†é‡å¤§é‡æ„ï¼Œåœ¨ `evalscope/api` ä¸‹å»ºç«‹äº†å…¨æ–°çš„ã€æ›´æ¨¡å—åŒ–ä¸”æ˜“æ‰©å±•çš„ API å±‚ã€‚ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼šä¸ºåŸºå‡†ã€æ ·æœ¬å’Œç»“æœå¼•å…¥äº†æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹ï¼›å¯¹åŸºå‡†å’ŒæŒ‡æ ‡ç­‰ç»„ä»¶é‡‡ç”¨æ³¨å†Œè¡¨å¼è®¾è®¡ï¼›å¹¶é‡å†™äº†æ ¸å¿ƒè¯„æµ‹å™¨ä»¥ååŒæ–°æ¶æ„ã€‚ç°æœ‰çš„åŸºå‡†å·²è¿ç§»åˆ°è¿™ä¸€ APIï¼Œå®ç°æ›´åŠ ç®€æ´ã€ä¸€è‡´ä¸”æ˜“äºç»´æŠ¤ã€‚

- ğŸ”¥ **[2025.12.02]** æ”¯æŒè‡ªå®šä¹‰å¤šæ¨¡æ€VQAè¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/vlm.html) ï¼›æ”¯æŒæ¨¡å‹æœåŠ¡å‹æµ‹åœ¨ ClearML ä¸Šå¯è§†åŒ–ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#clearml)ã€‚
- ğŸ”¥ **[2025.11.26]** æ–°å¢æ”¯æŒ OpenAI-MRCRã€GSM8K-Vã€MGSMã€MicroVQAã€IFBenchã€SciCode è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.18]** æ”¯æŒè‡ªå®šä¹‰ Function-Callï¼ˆå·¥å…·è°ƒç”¨ï¼‰æ•°æ®é›†ï¼Œæ¥æµ‹è¯•æ¨¡å‹èƒ½å¦é€‚æ—¶å¹¶æ­£ç¡®è°ƒç”¨å·¥å…·ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#fc)
- ğŸ”¥ **[2025.11.14]** æ–°å¢æ”¯æŒSWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini ä»£ç è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/swe_bench.html)ã€‚
- ğŸ”¥ **[2025.11.12]** æ–°å¢`pass@k`ã€`vote@k`ã€`pass^k`ç­‰æŒ‡æ ‡èšåˆæ–¹æ³•ï¼›æ–°å¢æ”¯æŒA_OKVQA CMMU ScienceQ V*Benchç­‰å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.07]** æ–°å¢æ”¯æŒÏ„Â²-benchï¼Œæ˜¯ Ï„-bench çš„æ‰©å±•ä¸å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«ä¸€ç³»åˆ—ä»£ç ä¿®å¤ï¼Œå¹¶æ–°å¢äº†ç”µä¿¡ï¼ˆtelecomï¼‰é¢†åŸŸçš„æ•…éšœæ’æŸ¥åœºæ™¯ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/tau2_bench.html)ã€‚
- ğŸ”¥ **[2025.10.30]** æ–°å¢æ”¯æŒBFCL-v4ï¼Œæ”¯æŒagentçš„ç½‘ç»œæœç´¢å’Œé•¿æœŸè®°å¿†èƒ½åŠ›çš„è¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v4.html)ã€‚
- ğŸ”¥ **[2025.10.27]** æ–°å¢æ”¯æŒLogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQAç­‰è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.26]** æ–°å¢æ”¯æŒConll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 ç­‰å‘½åå®ä½“è¯†åˆ«è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.21]** ä¼˜åŒ–ä»£ç è¯„æµ‹ä¸­çš„æ²™ç®±ç¯å¢ƒä½¿ç”¨ï¼Œæ”¯æŒåœ¨æœ¬åœ°å’Œè¿œç¨‹ä¸¤ç§æ¨¡å¼ä¸‹è¿è¡Œï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/sandbox.html)ã€‚
- ğŸ”¥ **[2025.10.20]** æ–°å¢æ”¯æŒPolyMath SimpleVQA MathVerse MathVision AA-LCR ç­‰è¯„æµ‹åŸºå‡†ï¼›ä¼˜åŒ–evalscope perfè¡¨ç°ï¼Œå¯¹é½vLLM Benchï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/vs_vllm_bench.html)ã€‚
- ğŸ”¥ **[2025.10.14]** æ–°å¢æ”¯æŒOCRBench OCRBench-v2 DocVQA InfoVQA ChartQA BLINK ç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.22]** ä»£ç è¯„æµ‹åŸºå‡†(HumanEval LiveCodeBench)æ”¯æŒåœ¨æ²™ç®±ç¯å¢ƒä¸­è¿è¡Œï¼Œè¦ä½¿ç”¨è¯¥åŠŸèƒ½éœ€å…ˆå®‰è£…[ms-enclave](https://github.com/modelscope/ms-enclave)ã€‚
- ğŸ”¥ **[2025.09.19]** æ–°å¢æ”¯æŒRealWorldQAã€AI2Dã€MMStarã€MMBenchã€OmniBenchç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼Œå’ŒMulti-IFã€HealthBenchã€AMCç­‰çº¯æ–‡æœ¬è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.05]** æ”¯æŒè§†è§‰-è¯­è¨€å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è¯„æµ‹ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šMathVistaã€MMMUï¼Œæ›´å¤šæ”¯æŒæ•°æ®é›†è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/vlm.html)ã€‚
- ğŸ”¥ **[2025.09.04]** æ”¯æŒå›¾åƒç¼–è¾‘ä»»åŠ¡è¯„æµ‹ï¼Œæ”¯æŒ[GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/image_edit.html)ã€‚
- ğŸ”¥ **[2025.08.22]** Version 1.0 é‡æ„ï¼Œä¸å…¼å®¹çš„æ›´æ–°è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#v1-0)ã€‚
<details> <summary>æ›´å¤š</summary>

- ğŸ”¥ **[2025.07.18]** æ¨¡å‹å‹æµ‹æ”¯æŒéšæœºç”Ÿæˆå›¾æ–‡æ•°æ®ï¼Œç”¨äºå¤šæ¨¡æ€æ¨¡å‹å‹æµ‹ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#id4)ã€‚
- ğŸ”¥ **[2025.07.16]** æ”¯æŒ[Ï„-bench](https://github.com/sierra-research/tau-bench)ï¼Œç”¨äºè¯„ä¼° AI Agentåœ¨åŠ¨æ€ç”¨æˆ·å’Œå·¥å…·äº¤äº’çš„å®é™…ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#bench)ã€‚
- ğŸ”¥ **[2025.07.14]** æ”¯æŒâ€œäººç±»æœ€åçš„è€ƒè¯•â€([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle))ï¼Œè¿™ä¸€é«˜éš¾åº¦è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam)ã€‚
- ğŸ”¥ **[2025.07.03]** é‡æ„äº†ç«æŠ€åœºæ¨¡å¼ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å¯¹æˆ˜ï¼Œè¾“å‡ºæ¨¡å‹æ’è¡Œæ¦œï¼Œä»¥åŠå¯¹æˆ˜ç»“æœå¯è§†åŒ–ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚
- ğŸ”¥ **[2025.06.28]** ä¼˜åŒ–è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼Œæ”¯æŒæ— å‚è€ƒç­”æ¡ˆè¯„æµ‹ï¼›ä¼˜åŒ–LLMè£åˆ¤ä½¿ç”¨ï¼Œé¢„ç½®â€œæ— å‚è€ƒç­”æ¡ˆç›´æ¥æ‰“åˆ†â€ å’Œ â€œåˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´â€ä¸¤ç§æ¨¡å¼ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#qa)
- ğŸ”¥ **[2025.06.19]** æ–°å¢æ”¯æŒ[BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3)è¯„æµ‹åŸºå‡†ï¼Œç”¨äºè¯„æµ‹æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v3.html)ã€‚
- ğŸ”¥ **[2025.06.02]** æ–°å¢æ”¯æŒå¤§æµ·æé’ˆæµ‹è¯•ï¼ˆNeedle-in-a-Haystackï¼‰ï¼ŒæŒ‡å®š`needle_haystack`å³å¯è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åœ¨`outputs/reports`æ–‡ä»¶å¤¹ä¸‹ç”Ÿæˆå¯¹åº”çš„heatmapï¼Œç›´è§‚å±•ç°æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/needle_haystack.html)ã€‚
- ğŸ”¥ **[2025.05.29]** æ–°å¢æ”¯æŒ[DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary)å’Œ[FRAMES](https://modelscope.cn/datasets/iic/frames/summary)ä¸¤ä¸ªé•¿æ–‡æ¡£è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.05.16]** æ¨¡å‹æœåŠ¡æ€§èƒ½å‹æµ‹æ”¯æŒè®¾ç½®å¤šç§å¹¶å‘ï¼Œå¹¶è¾“å‡ºæ€§èƒ½å‹æµ‹æŠ¥å‘Šï¼Œ[å‚è€ƒç¤ºä¾‹](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/quick_start.html#id3)ã€‚
- ğŸ”¥ **[2025.05.13]** æ–°å¢æ”¯æŒ[ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static)æ•°æ®é›†ï¼Œè¯„æµ‹æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html)ï¼›æ”¯æŒ[DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview)å’Œ[Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val)è¯„æµ‹åŸºå‡†ï¼Œè¯„æµ‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
- ğŸ”¥ **[2025.04.29]** æ–°å¢Qwen3è¯„æµ‹æœ€ä½³å®è·µï¼Œ[æ¬¢è¿é˜…è¯»ğŸ“–](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** æ”¯æŒæ–‡ç”Ÿå›¾è¯„æµ‹ï¼šæ”¯æŒMPSã€HPSv2.1Scoreç­‰8ä¸ªæŒ‡æ ‡ï¼Œæ”¯æŒEvalMuseã€GenAI-Benchç­‰è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/t2i.html)
- ğŸ”¥ **[2025.04.10]** æ¨¡å‹æœåŠ¡å‹æµ‹å·¥å…·æ”¯æŒ`/v1/completions`ç«¯ç‚¹ï¼ˆä¹Ÿæ˜¯vLLMåŸºå‡†æµ‹è¯•çš„é»˜è®¤ç«¯ç‚¹ï¼‰
- ğŸ”¥ **[2025.04.08]** æ”¯æŒOpenAI APIå…¼å®¹çš„Embeddingæ¨¡å‹æœåŠ¡è¯„æµ‹ï¼ŒæŸ¥çœ‹[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters)
- ğŸ”¥ **[2025.03.27]** æ–°å¢æ”¯æŒ[AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview)å’Œ[ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary)è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** æ¨¡å‹æ¨ç†æœåŠ¡å‹æµ‹æ”¯æŒrandomç”ŸæˆæŒ‡å®šèŒƒå›´é•¿åº¦çš„promptï¼Œå‚è€ƒ[ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#random)
- ğŸ”¥ **[2025.03.13]** æ–°å¢æ”¯æŒ[LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary)ä»£ç è¯„æµ‹åŸºå‡†ï¼ŒæŒ‡å®š`live_code_bench`å³å¯ä½¿ç”¨ï¼›æ”¯æŒQwQ-32B åœ¨LiveCodeBenchä¸Šè¯„æµ‹ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.11]** æ–°å¢æ”¯æŒ[SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary)å’Œ[Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary)è¯„æµ‹åŸºå‡†ï¼Œç”¨ä¸è¯„æµ‹æ¨¡å‹çš„äº‹å®æ­£ç¡®æ€§ï¼ŒæŒ‡å®š`simple_qa`å’Œ`chinese_simpleqa`ä½¿ç”¨ã€‚åŒæ—¶æ”¯æŒæŒ‡å®šè£åˆ¤æ¨¡å‹ï¼Œå‚è€ƒ[ç›¸å…³å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚
- ğŸ”¥ **[2025.03.07]** æ–°å¢QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µï¼Œè¯„æµ‹äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»¥åŠæ¨ç†æ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.04]** æ–°å¢æ”¯æŒ[SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary)æ•°æ®é›†ï¼Œå…¶è¦†ç›– 13 ä¸ªé—¨ç±»ã€72 ä¸ªä¸€çº§å­¦ç§‘å’Œ 285 ä¸ªäºŒçº§å­¦ç§‘ï¼Œå…± 26529 ä¸ªé—®é¢˜ï¼ŒæŒ‡å®š`super_gpqa`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.03.03]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨¡å‹çš„æ™ºå•†å’Œæƒ…å•†ï¼Œå‚è€ƒ[ğŸ“–æ™ºå•†å’Œæƒ…å•†è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/iquiz.html)ï¼Œæ¥æµ‹æµ‹ä½ å®¶çš„AIæœ‰å¤šèªæ˜ï¼Ÿ
- ğŸ”¥ **[2025.02.27]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨ç†æ¨¡å‹çš„æ€è€ƒæ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–æ€è€ƒæ•ˆç‡è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/think_eval.html)ï¼Œè¯¥å®ç°å‚è€ƒäº†[Overthinking](https://doi.org/10.48550/arXiv.2412.21187) å’Œ [Underthinking](https://doi.org/10.48550/arXiv.2501.18585)ä¸¤ç¯‡å·¥ä½œã€‚
- ğŸ”¥ **[2025.02.25]** æ–°å¢æ”¯æŒ[MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR)å’Œ[ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary)ä¸¤ä¸ªæ¨¡å‹æ¨ç†ç›¸å…³è¯„æµ‹åŸºå‡†ï¼Œdatasetsåˆ†åˆ«æŒ‡å®š`musr`å’Œ`process_bench`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.02.18]** æ”¯æŒAIME25æ•°æ®é›†ï¼ŒåŒ…å«15é“é¢˜ç›®ï¼ˆGrok3 åœ¨è¯¥æ•°æ®é›†ä¸Šå¾—åˆ†ä¸º93åˆ†ï¼‰
- ğŸ”¥ **[2025.02.13]** æ”¯æŒDeepSeekè’¸é¦æ¨¡å‹è¯„æµ‹ï¼ŒåŒ…æ‹¬AIME24 MATH-500 GPQA-Diamondæ•°æ®é›†ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/deepseek_r1_distill.html)ï¼›æ”¯æŒæŒ‡å®š`eval_batch_size`å‚æ•°ï¼ŒåŠ é€Ÿæ¨¡å‹è¯„æµ‹
- ğŸ”¥ **[2025.01.20]** æ”¯æŒå¯è§†åŒ–è¯„æµ‹ç»“æœï¼ŒåŒ…æ‹¬å•æ¨¡å‹è¯„æµ‹ç»“æœå’Œå¤šæ¨¡å‹è¯„æµ‹ç»“æœå¯¹æ¯”ï¼Œå‚è€ƒ[ğŸ“–å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ï¼›æ–°å¢[`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary)è¯„æµ‹æ ·ä¾‹ï¼Œè¯„æµ‹æ¨¡å‹çš„IQå’ŒEQã€‚
- ğŸ”¥ **[2025.01.07]** Native backend: æ”¯æŒæ¨¡å‹APIè¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–æ¨¡å‹APIè¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#api)ï¼›æ–°å¢æ”¯æŒ`ifeval`è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ğŸ”¥ **[2024.12.31]** æ”¯æŒåŸºå‡†è¯„æµ‹æ·»åŠ ï¼Œå‚è€ƒ[ğŸ“–åŸºå‡†è¯„æµ‹æ·»åŠ æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ï¼›æ”¯æŒè‡ªå®šä¹‰æ··åˆæ•°æ®é›†è¯„æµ‹ï¼Œç”¨æ›´å°‘çš„æ•°æ®ï¼Œæ›´å…¨é¢çš„è¯„æµ‹æ¨¡å‹ï¼Œå‚è€ƒ[ğŸ“–æ··åˆæ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/collection/index.html)
- ğŸ”¥ **[2024.12.13]** æ¨¡å‹è¯„æµ‹ä¼˜åŒ–ï¼Œä¸å†éœ€è¦ä¼ é€’`--template-type`å‚æ•°ï¼›æ”¯æŒ`evalscope eval --args`å¯åŠ¨è¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html)
- ğŸ”¥ **[2024.11.26]** æ¨¡å‹æ¨ç†å‹æµ‹å·¥å…·é‡æ„å®Œæˆï¼šæ”¯æŒæœ¬åœ°å¯åŠ¨æ¨ç†æœåŠ¡ã€æ”¯æŒSpeed Benchmarkï¼›ä¼˜åŒ–å¼‚æ­¥è°ƒç”¨é”™è¯¯å¤„ç†ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)
- ğŸ”¥ **[2024.10.31]** å¤šæ¨¡æ€RAGè¯„æµ‹æœ€ä½³å®è·µå‘å¸ƒï¼Œå‚è€ƒ[ğŸ“–åšå®¢](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag)
- ğŸ”¥ **[2024.10.23]** æ”¯æŒå¤šæ¨¡æ€RAGè¯„æµ‹ï¼ŒåŒ…æ‹¬[CLIP_Benchmark](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/clip_benchmark.html)è¯„æµ‹å›¾æ–‡æ£€ç´¢å™¨ï¼Œä»¥åŠæ‰©å±•äº†[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)ä»¥æ”¯æŒç«¯åˆ°ç«¯å¤šæ¨¡æ€æŒ‡æ ‡è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.10.8]** æ”¯æŒRAGè¯„æµ‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨[MTEB/CMTEB](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html)è¿›è¡Œembeddingæ¨¡å‹å’Œrerankerçš„ç‹¬ç«‹è¯„æµ‹ï¼Œä»¥åŠä½¿ç”¨[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)è¿›è¡Œç«¯åˆ°ç«¯è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.09.18]** æˆ‘ä»¬çš„æ–‡æ¡£å¢åŠ äº†åšå®¢æ¨¡å—ï¼ŒåŒ…å«ä¸€äº›è¯„æµ‹ç›¸å…³çš„æŠ€æœ¯è°ƒç ”å’Œåˆ†äº«ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/blog/index.html)
- ğŸ”¥ **[2024.09.12]** æ”¯æŒ LongWriter è¯„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºå‡†æµ‹è¯• [LongBench-Write](evalscope/third_party/longbench_write/README.md) æ¥è¯„æµ‹é•¿è¾“å‡ºçš„è´¨é‡ä»¥åŠè¾“å‡ºé•¿åº¦ã€‚
- ğŸ”¥ **[2024.08.30]** æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ•°æ®é›†å’Œå¤šæ¨¡æ€å›¾æ–‡æ•°æ®é›†ã€‚
- ğŸ”¥ **[2024.08.20]** æ›´æ–°äº†å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…æ‹¬å¿«é€Ÿä¸Šæ‰‹ã€æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜ç­‰ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/)ã€‚
- ğŸ”¥ **[2024.08.09]** ç®€åŒ–å®‰è£…æ–¹å¼ï¼Œæ”¯æŒpypiå®‰è£…vlmevalç›¸å…³ä¾èµ–ï¼›ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä½“éªŒï¼ŒåŸºäºOpenAI APIæ–¹å¼çš„è¯„æµ‹é“¾è·¯ï¼Œæœ€é«˜åŠ é€Ÿ10å€ã€‚
- ğŸ”¥ **[2024.07.31]** é‡è¦ä¿®æ”¹ï¼š`llmuses`åŒ…åä¿®æ”¹ä¸º`evalscope`ï¼Œè¯·åŒæ­¥ä¿®æ”¹æ‚¨çš„ä»£ç ã€‚
- ğŸ”¥ **[2024.07.26]** æ”¯æŒ**VLMEvalKit**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œå‘èµ·å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä»»åŠ¡ã€‚
- ğŸ”¥ **[2024.06.29]** æ”¯æŒ**OpenCompass**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†é«˜çº§å°è£…ï¼Œæ”¯æŒpipæ–¹å¼å®‰è£…ï¼Œç®€åŒ–äº†è¯„æµ‹ä»»åŠ¡é…ç½®ã€‚
- ğŸ”¥ **[2024.06.13]** EvalScopeä¸å¾®è°ƒæ¡†æ¶SWIFTè¿›è¡Œæ— ç¼å¯¹æ¥ï¼Œæä¾›LLMä»è®­ç»ƒåˆ°è¯„æµ‹çš„å…¨é“¾è·¯æ”¯æŒ ã€‚
- ğŸ”¥ **[2024.06.13]** æ¥å…¥Agentè¯„æµ‹é›†ToolBenchã€‚
</details>

## â¤ï¸ ç¤¾åŒºä¸æ”¯æŒ

æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºï¼Œä¸å…¶ä»–å¼€å‘è€…äº¤æµå¹¶è·å–å¸®åŠ©ã€‚

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  å¾®ä¿¡ç¾¤ | é’‰é’‰ç¾¤
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬æ¨èä½¿ç”¨ `conda` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä½¿ç”¨ `pip` å®‰è£…ã€‚

1.  **åˆ›å»ºå¹¶æ¿€æ´» Conda ç¯å¢ƒ** (æ¨èä½¿ç”¨ Python 3.10)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **å®‰è£… EvalScope**

    - **æ–¹å¼ä¸€ï¼šé€šè¿‡ PyPI å®‰è£… (æ¨è)**
      ```shell
      pip install evalscope
      ```

    - **æ–¹å¼äºŒï¼šé€šè¿‡æºç å®‰è£… (ç”¨äºå¼€å‘)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **å®‰è£…é¢å¤–ä¾èµ–** (å¯é€‰)
    æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œå®‰è£…ç›¸åº”çš„åŠŸèƒ½æ‰©å±•ï¼š
    ```shell
    # æ€§èƒ½æµ‹è¯•
    pip install 'evalscope[perf]'

    # å¯è§†åŒ–App
    pip install 'evalscope[app]'

    # å…¶ä»–è¯„æµ‹åç«¯
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # å®‰è£…æ‰€æœ‰ä¾èµ–
    pip install 'evalscope[all]'
    ```
    > å¦‚æœæ‚¨é€šè¿‡æºç å®‰è£…ï¼Œè¯·å°† `evalscope` æ›¿æ¢ä¸º `.`ï¼Œä¾‹å¦‚ `pip install '.[perf]'`ã€‚

> [!NOTE]
> æœ¬é¡¹ç›®æ›¾ç”¨å `llmuses`ã€‚å¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ `v0.4.3` æˆ–æ›´æ—©ç‰ˆæœ¬ï¼Œè¯·è¿è¡Œ `pip install llmuses<=0.4.3` å¹¶ä½¿ç”¨ `from llmuses import ...` å¯¼å…¥ã€‚


## ğŸš€ å¿«é€Ÿå¼€å§‹

æ‚¨å¯ä»¥é€šè¿‡**å‘½ä»¤è¡Œ**æˆ– **Python ä»£ç **ä¸¤ç§æ–¹å¼å¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

### æ–¹å¼1. ä½¿ç”¨å‘½ä»¤è¡Œ

åœ¨ä»»æ„è·¯å¾„ä¸‹æ‰§è¡Œ `evalscope eval` å‘½ä»¤å³å¯å¼€å§‹è¯„æµ‹ã€‚ä»¥ä¸‹å‘½ä»¤å°†åœ¨ `gsm8k` å’Œ `arc` æ•°æ®é›†ä¸Šè¯„æµ‹ `Qwen/Qwen2.5-0.5B-Instruct` æ¨¡å‹ï¼Œæ¯ä¸ªæ•°æ®é›†åªå– 5 ä¸ªæ ·æœ¬ã€‚

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### æ–¹å¼2. ä½¿ç”¨Pythonä»£ç 

ä½¿ç”¨ `run_task` å‡½æ•°å’Œ `TaskConfig` å¯¹è±¡æ¥é…ç½®å’Œå¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

```python
from evalscope import run_task TaskConfig

# é…ç½®è¯„æµ‹ä»»åŠ¡
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# å¯åŠ¨è¯„æµ‹
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ æç¤ºï¼š</b> `run_task` è¿˜æ”¯æŒå­—å…¸ã€YAML æˆ– JSON æ–‡ä»¶ä½œä¸ºé…ç½®ã€‚</summary>

**ä½¿ç”¨ Python å­—å…¸**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**ä½¿ç”¨ YAML æ–‡ä»¶** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### è¾“å‡ºç»“æœ
è¯„æµ‹å®Œæˆåï¼Œæ‚¨å°†åœ¨ç»ˆç«¯çœ‹åˆ°å¦‚ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ è¿›é˜¶ç”¨æ³•

### è‡ªå®šä¹‰è¯„æµ‹å‚æ•°

æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ç²¾ç»†åŒ–æ§åˆ¶æ¨¡å‹åŠ è½½ã€æ¨ç†å’Œæ•°æ®é›†é…ç½®ã€‚

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: æ¨¡å‹åŠ è½½å‚æ•°ï¼Œå¦‚ `revision` `precision` ç­‰ã€‚
- `--generation-config`: æ¨¡å‹ç”Ÿæˆå‚æ•°ï¼Œå¦‚ `temperature` `max_tokens` ç­‰ã€‚
- `--dataset-args`: æ•°æ®é›†é…ç½®å‚æ•°ï¼Œå¦‚ `few_shot_num` ç­‰ã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å…¨éƒ¨å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚

### è¯„æµ‹åœ¨çº¿æ¨¡å‹ API

EvalScope æ”¯æŒè¯„æµ‹é€šè¿‡ API éƒ¨ç½²çš„æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ vLLM éƒ¨ç½²çš„æœåŠ¡ï¼‰ã€‚åªéœ€æŒ‡å®šæœåŠ¡åœ°å€å’Œ API Key å³å¯ã€‚

1.  **å¯åŠ¨æ¨¡å‹æœåŠ¡** (ä»¥ vLLM ä¸ºä¾‹)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **è¿è¡Œè¯„æµ‹**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ ç«æŠ€åœºæ¨¡å¼ (Arena)

ç«æŠ€åœºæ¨¡å¼é€šè¿‡æ¨¡å‹é—´çš„ä¸¤ä¸¤å¯¹æˆ˜ï¼ˆPairwise Battleï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ç»™å‡ºèƒœç‡å’Œæ’åï¼Œéå¸¸é€‚åˆå¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”ã€‚

```text
# è¯„æµ‹ç»“æœç¤ºä¾‹
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– ç«æŠ€åœºæ¨¡å¼ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚

### ğŸ–Šï¸ è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹

EvalScope å…è®¸æ‚¨è½»æ¾æ·»åŠ å’Œè¯„æµ‹è‡ªå·±çš„æ•°æ®é›†ã€‚è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/index.html)ã€‚


## ğŸ§ª å…¶ä»–è¯„æµ‹åç«¯
EvalScope æ”¯æŒé€šè¿‡ç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåç«¯â€ï¼‰å‘èµ·è¯„æµ‹ä»»åŠ¡ï¼Œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„è¯„æµ‹éœ€æ±‚ã€‚

- **Native**: EvalScope çš„é»˜è®¤è¯„æµ‹æ¡†æ¶ï¼ŒåŠŸèƒ½å…¨é¢ã€‚
- **OpenCompass**: ä¸“æ³¨äºçº¯æ–‡æœ¬è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: ä¸“æ³¨äºå¤šæ¨¡æ€è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: ä¸“æ³¨äº RAG è¯„æµ‹ï¼Œæ”¯æŒ Embedding å’Œ Reranker æ¨¡å‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/index.html)
- **ç¬¬ä¸‰æ–¹è¯„æµ‹å·¥å…·**: æ”¯æŒ [ToolBench](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html) ç­‰è¯„æµ‹ä»»åŠ¡ã€‚

## âš¡ æ¨ç†æ€§èƒ½è¯„æµ‹å·¥å…·
EvalScope æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„æ€§èƒ½ã€‚

- **å…³é”®æŒ‡æ ‡**: æ”¯æŒååé‡ (Tokens/s)ã€é¦–å­—å»¶è¿Ÿ (TTFT)ã€Token ç”Ÿæˆå»¶è¿Ÿ (TPOT) ç­‰ã€‚
- **ç»“æœè®°å½•**: æ”¯æŒå°†ç»“æœè®°å½•åˆ° `wandb` å’Œ `swanlab`ã€‚
- **é€Ÿåº¦åŸºå‡†**: å¯ç”Ÿæˆç±»ä¼¼å®˜æ–¹æŠ¥å‘Šçš„é€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– æ€§èƒ½æµ‹è¯•ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)ã€‚

è¾“å‡ºç¤ºä¾‹å¦‚ä¸‹ï¼š
<p align="center">
    <img src="docs/zh/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š å¯è§†åŒ–è¯„æµ‹ç»“æœ

EvalScope æä¾›äº†ä¸€ä¸ªåŸºäº Gradio çš„ WebUIï¼Œç”¨äºäº¤äº’å¼åœ°åˆ†æå’Œæ¯”è¾ƒè¯„æµ‹ç»“æœã€‚

1.  **å®‰è£…ä¾èµ–**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **å¯åŠ¨æœåŠ¡**
    ```bash
    evalscope app
    ```
    è®¿é—® `http://127.0.0.1:7861` å³å¯æ‰“å¼€å¯è§†åŒ–ç•Œé¢ã€‚

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/setting.png" alt="Setting" style="width: 90%;" />
      <p>è®¾ç½®ç•Œé¢</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>æ¨¡å‹æ¯”è¾ƒ</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>æŠ¥å‘Šæ¦‚è§ˆ</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_details.png" alt="Report Details" style="width: 91%;" />
      <p>æŠ¥å‘Šè¯¦æƒ…</p>
    </td>
  </tr>
</table>

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ã€‚

## ğŸ‘·â€â™‚ï¸ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ¥è‡ªç¤¾åŒºçš„ä»»ä½•è´¡çŒ®ï¼å¦‚æœæ‚¨å¸Œæœ›æ·»åŠ æ–°çš„è¯„æµ‹åŸºå‡†ã€æ¨¡å‹æˆ–åŠŸèƒ½ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [è´¡çŒ®æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ã€‚

æ„Ÿè°¢æ‰€æœ‰ä¸º EvalScope åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº† EvalScopeï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼š
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

# Arena Mode

Arena mode allows you to configure multiple candidate models and specify a baseline model. The evaluation is conducted through pairwise battles between each candidate model and the baseline model with the win rate and ranking of each model outputted at the end. This approach is suitable for comparative evaluation among multiple models and intuitively reflects the strengths and weaknesses of each model.

## Data Preparation

To support arena mode **all candidate models need to run inference on the same dataset**. The dataset can be a general QA dataset or a domain-specific one. Below is an example using a custom `general_qa` dataset. See the [documentation](../advanced_guides/custom_dataset/llm.md#question-answering-format-qa) for details on using this dataset.

The JSONL file for the `general_qa` dataset should be in the following format. Only the `query` field is required; no additional fields are necessary. Below are two example files:

- Example content of the `arena.jsonl` file:
    ```json
    {"query": "How can I improve my time management skills?"}
    {"query": "What are the most effective ways to deal with stress?"}
    {"query": "What are the main differences between Python and JavaScript programming languages?"}
    {"query": "How can I increase my productivity while working from home?"}
    {"query": "Can you explain the basics of quantum computing?"}
    ```

- Example content of the `example.jsonl` file (with reference answers):
    ```json
    {"query": "What is the capital of France?" "response": "The capital of France is Paris."}
    {"query": "What is the largest mammal in the world?" "response": "The largest mammal in the world is the blue whale."}
    {"query": "How does photosynthesis work?" "response": "Photosynthesis is the process by which green plants use sunlight to synthesize foods with the help of chlorophyll."}
    {"query": "What is the theory of relativity?" "response": "The theory of relativity developed by Albert Einstein describes the laws of physics in relation to observers in different frames of reference."}
    {"query": "Who wrote 'To Kill a Mockingbird'?" "response": "Harper Lee wrote 'To Kill a Mockingbird'."}
    ```

## Candidate Model Inference

After preparing the dataset you can use EvalScope's `run_task` method to perform inference with the candidate models and obtain their outputs for subsequent battles.

Below is an example of how to configure inference tasks for three candidate models: `Qwen2.5-0.5B-Instruct` `Qwen2.5-7B-Instruct` and `Qwen2.5-72B-Instruct` using the same configuration for inference.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task
from evalscope.constants import EvalType

models = ['qwen2.5-72b-instruct' 'qwen2.5-7b-instruct' 'qwen2.5-0.5b-instruct']

task_list = [TaskConfig(
    model=model
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=os.getenv('DASHSCOPE_API_KEY')
    eval_type=EvalType.SERVICE
    datasets=[
        'general_qa'
    ]
    dataset_args={
        'general_qa': {
            'dataset_id': 'custom_eval/text/qa'
            'subset_list': [
                'arena'
                'example'
            ]
        }
    }
    eval_batch_size=10
    generation_config={
        'temperature': 0
        'n': 1
        'max_tokens': 4096
    }) for model in models]

run_task(task_cfg=task_list)
```

<details><summary>Click to view inference results</summary>

Since the `arena` subset does not have reference answers no evaluation metrics are available for this subset. The `example` subset has reference answers so evaluation metrics will be output.
```text
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| Model                 | Dataset    | Metric          | Subset   |   Num |   Score | Cat.0   |
+=======================+============+=================+==========+=======+=========+=========+
| qwen2.5-0.5b-instruct | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-P       | example  |    12 |  0.1341 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-F       | example  |    12 |  0.1983 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-R       | example  |    12 |  0.55   | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-P       | example  |    12 |  0.0404 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-F       | example  |    12 |  0.0716 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-P       | example  |    12 |  0.1193 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-F       | example  |    12 |  0.1754 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-1          | example  |    12 |  0.1192 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-2          | example  |    12 |  0.0403 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-3          | example  |    12 |  0.0135 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-4          | example  |    12 |  0.0079 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-P       | example  |    12 |  0.1149 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-F       | example  |    12 |  0.1612 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-R       | example  |    12 |  0.6833 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-P       | example  |    12 |  0.0813 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-F       | example  |    12 |  0.1027 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-P       | example  |    12 |  0.101  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-F       | example  |    12 |  0.1361 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-1          | example  |    12 |  0.1009 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-2          | example  |    12 |  0.0807 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-P       | example  |    12 |  0.104  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-F       | example  |    12 |  0.1418 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-R       | example  |    12 |  0.7    | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-P       | example  |    12 |  0.078  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-F       | example  |    12 |  0.0964 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-P       | example  |    12 |  0.0942 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-F       | example  |    12 |  0.1235 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-1          | example  |    12 |  0.0939 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-2          | example  |    12 |  0.0777 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
```
</details>

## Candidate Model Battles

Next you can use EvalScope's `general_arena` method to conduct battles among candidate models and get their win rates and rankings on each subset. To achieve robust automatic battles you need to configure an LLM as the judge that compares the outputs of models.

During evaluation EvalScope will automatically parse the public evaluation set of candidate models use the judge model to compare the output of each candidate model with the baseline and determine which is better (to avoid model bias outputs are swapped for two rounds per comparison). The judge model's outputs are parsed as win draw or loss and each candidate model's **Elo score** and **win rate** are calculated.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task

task_cfg = TaskConfig(
    model_id='Arena'  # Model ID is 'Arena'; you can omit specifying model ID
    datasets=[
        'general_arena'  # Must be 'general_arena' indicating arena mode
    ]
    dataset_args={
        'general_arena': {
            # 'system_prompt': 'xxx' # Optional: customize the judge model's system prompt here
            # 'prompt_template': 'xxx' # Optional: customize the judge model's prompt template here
            'extra_params':{
                # Configure candidate model names and corresponding report paths
                # Report paths refer to the output paths from the previous step for parsing model inference results
                'models':[
                    {
                        'name': 'qwen2.5-0.5b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-0.5b-instruct'
                    }
                    {
                        'name': 'qwen2.5-7b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-7b-instruct'
                    }
                    {
                        'name': 'qwen2.5-72b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-72b-instruct'
                    }
                ]
                # Set baseline model must be one of the candidate models
                'baseline': 'qwen2.5-7b'
            }
        }
    }
    # Configure judge model parameters
    judge_model_args={
        'model_id': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': os.getenv('DASHSCOPE_API_KEY')
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 8000
        }
    }
    judge_worker_num=5
    # use_cache='outputs/xxx' # Optional: to add new candidate models to existing results specify the existing results path
)

run_task(task_cfg=task_cfg)
```

<details><summary>Click to view evaluation results</summary>

```text
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Model   | Dataset       | Metric        | Subset                                     |   Num |   Score | Cat.0   |
+=========+===============+===============+============================================+=======+=========+=========+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.5469 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.075  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.8382 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | OVERALL                                    |    44 |  0.3617 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.3906 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.025  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.7276 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | OVERALL                                    |    44 |  0.2826 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.6875 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.9412 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | OVERALL                                    |    44 |  0.4469 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+ 
```
</details>


The automatically generated model leaderboard is as follows (output file located in `outputs/xxx/reports/Arena/leaderboard.txt`):

The leaderboard is sorted by win rate in descending order. As shown the `qwen2.5-72b` model performs best across all subsets with the highest win rate while the `qwen2.5-0.5b` model performs the worst.

```text
=== OVERALL LEADERBOARD ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== DATASET LEADERBOARD: general_qa ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== SUBSET LEADERBOARD: general_qa - example ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            54.7  (-15.6 / +14.1)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            1.8  (+0.0 / +7.2)

=== SUBSET LEADERBOARD: general_qa - arena ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            83.8  (-11.1 / +10.3)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            7.5  (-5.0 / +1.6)
```

## Visualization of Battle Results

To intuitively display the results of the battles between candidate models and the baseline EvalScope provides a visualization feature allowing you to compare the results of each candidate model against the baseline model for each sample.

Run the command below to launch the visualization interface:
```shell
evalscope app
```
Open `http://localhost:7860` in your browser to view the visualization page.

Workflow:
1. Select the latest `general_arena` evaluation report and click the "Load and View" button.
2. Click dataset details and select the battle results between your candidate model and the baseline.
3. Adjust the threshold to filter battle results (normalized scores range from 0-1; 0.5 indicates a tie scores above 0.5 indicate the candidate is better than the baseline below 0.5 means worse).

Example below: a battle between `qwen2.5-72b` and `qwen2.5-7b`. The model judged the 72b as better:

![image](https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/arena_example.jpg)


# Sandbox Environment Usage

To complete LLM code capability evaluation we need to set up an independent evaluation environment to avoid executing erroneous code in the development environment and causing unavoidable losses. Currently EvalScope has integrated the [ms-enclave](https://github.com/modelscope/ms-enclave) sandbox environment allowing users to evaluate model code capabilities in a controlled environment such as using evaluation benchmarks like HumanEval and LiveCodeBench.

The following introduces two different sandbox usage methods:

- Local usage: Set up the sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine;
- Remote usage: Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

## 1. Local Usage

Use Docker to set up a sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine.

### Environment Setup

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in your local Python environment:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration
When running evaluations add the `use_sandbox` and `sandbox_type` parameters to automatically enable the sandbox environment. Other parameters remain the same as regular evaluations:

Here's a complete example code for model evaluation on HumanEval:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will automatically start and manage the sandbox environment ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] Local sandbox manager started
...
```

## 2. Remote Usage

Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

### Environment Setup

You need to install and configure separately on both the remote machine and local machine.

#### Remote Machine

The environment installation on the remote machine is similar to the local usage method described above:

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in remote Python environment:

```bash
pip install evalscope[sandbox]
```

3. **Start sandbox server**: Run the following command to start the sandbox server:

```bash
ms-enclave server --host 0.0.0.0 --port 1234
```

#### Local Machine

The local machine does not need Docker installation at this point but needs to install EvalScope:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration

When running evaluations add the `use_sandbox` parameter to automatically enable the sandbox environment and specify the remote sandbox server's API address in `sandbox_manager_config`:

Complete example code is as follows:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    sandbox_manager_config={
        'base_url': 'http://<remote_host>:1234'  # remote sandbox manager URL
    }
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will communicate with the remote sandbox server through API ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] HTTP sandbox manager started connected to http://<remote_host>:1234
...
```


# EvalScope Service Deployment

## Introduction

EvalScope service mode provides HTTP API-based evaluation and stress testing capabilities designed to address the following scenarios:

1. **Remote Invocation**: Support remote evaluation functionality through network without configuring complex evaluation environments locally
2. **Service Integration**: Easily integrate evaluation capabilities into existing workflows CI/CD pipelines or automated testing systems
3. **Multi-user Collaboration**: Support multiple users or systems calling the evaluation service simultaneously improving resource utilization
4. **Unified Management**: Centrally manage evaluation resources and configurations for easier maintenance and monitoring
5. **Flexible Deployment**: Can be deployed on dedicated servers or container environments decoupled from business systems

The Flask service encapsulates EvalScope's core evaluation (eval) and stress testing (perf) functionalities providing services through standard RESTful APIs making evaluation capabilities callable and integrable like other microservices.

## Features

- **Model Evaluation** (`/api/v1/eval`): Support evaluation of OpenAI API-compatible models
- **Performance Testing** (`/api/v1/perf`): Support performance benchmarking of OpenAI API-compatible models
- **Parameter Query**: Provide parameter description endpoints

## Environment Setup


### Full Installation (Recommended)

```bash
pip install evalscope[service]
```

### Development Environment Installation

```bash
# Clone repository
git clone https://github.com/modelscope/evalscope.git
cd evalscope

# Install development version with service
pip install -e '.[service]'
```

## Starting the Service

### Command Line Launch

```bash
# Use default configuration (host: 0.0.0.0 port: 9000)
evalscope service

# Custom host and port
evalscope service --host 127.0.0.1 --port 9000

# Enable debug mode
evalscope service --debug
```

### Python Code Launch

```python
from evalscope.service import run_service

# Start service
run_service(host='0.0.0.0' port=9000 debug=False)
```

## API Endpoints

### 1. Health Check

```bash
GET /health
```

**Response Example:**
```json
{
  "status": "ok"
  "service": "evalscope"
  "timestamp": "2025-12-04T10:00:00"
}
```

### 2. Model Evaluation

```bash
POST /api/v1/eval
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
  "api_key": "your-api-key"
  "datasets": ["gsm8k" "iquiz"]
  "limit": 10
  "generation_config": {
    "temperature": 0.0
    "max_tokens": 2048
  }
}
```

**Required Parameters:**
- `model`: Model name
- `datasets`: List of datasets
- `api_url`: API endpoint URL (OpenAI-compatible)

**Optional Parameters:**
- `api_key`: API key (default: "EMPTY")
- `limit`: Evaluation sample quantity limit
- `eval_batch_size`: Batch size (default: 1)
- `generation_config`: Generation configuration
  - `temperature`: Temperature parameter (default: 0.0)
  - `max_tokens`: Maximum generation tokens (default: 2048)
  - `top_p`: Nucleus sampling parameter
  - `top_k`: Top-k sampling parameter
- `work_dir`: Output directory
- `debug`: Debug mode
- `seed`: Random seed (default: 42)

**Response Example:**
```json
{
  "status": "success"
  "message": "Evaluation completed"
  "result": {"...": "..."}
  "output_dir": "/path/to/outputs/20251204_100000"
}
```

### 3. Performance Testing

```bash
POST /api/v1/perf
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  "api": "openai"
  "api_key": "your-api-key"
  "number": 100
  "parallel": 10
  "dataset": "openqa"
  "max_tokens": 2048
  "temperature": 0.0
}
```

**Required Parameters:**
- `model`: Model name
- `url`: Complete API endpoint URL

**Optional Parameters:**
- `api`: API type (openai/dashscope/anthropic/gemini default: "openai")
- `api_key`: API key
- `number`: Total number of requests (default: 1000)
- `parallel`: Concurrency level (default: 1)
- `rate`: Requests per second limit (default: -1 unlimited)
- `dataset`: Dataset name (default: "openqa")
- `max_tokens`: Maximum generation tokens (default: 2048)
- `temperature`: Temperature parameter (default: 0.0)
- `stream`: Whether to use streaming output (default: true)
- `debug`: Debug mode

**Response Example:**
```json
{
  "status": "success"
  "message": "Performance test completed"
  "output_dir": "/path/to/outputs"
  "results": {
    "parallel_10_number_100": {
      "metrics": {"...": "..."}
      "percentiles": {"...": "..."}
    }
  }
}
```

### 4. Get Evaluation Parameter Description

```bash
GET /api/v1/eval/params
```

Returns descriptions of all parameters supported by the evaluation endpoint.

### 5. Get Performance Test Parameter Description

```bash
GET /api/v1/perf/params
```

Returns descriptions of all parameters supported by the performance test endpoint.

## Usage Examples

### Testing Evaluation Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "your-api-key"
    "datasets": ["gsm8k"]
    "limit": 5
  }'
```

### Testing Performance Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    "api": "openai"
    "number": 50
    "parallel": 5
  }'
```

### Using Python requests

```python
import requests

# Evaluation request
eval_response = requests.post(
    'http://localhost:9000/api/v1/eval'
    json={
        'model': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': 'your-api-key'
        'datasets': ['gsm8k' 'iquiz']
        'limit': 10
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 2048
        }
    }
)
print(eval_response.json())

# Performance test request
perf_response = requests.post(
    'http://localhost:9000/api/v1/perf'
    json={
        'model': 'qwen-plus'
        'url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions'
        'api': 'openai'
        'number': 100
        'parallel': 10
        'dataset': 'openqa'
    }
)
print(perf_response.json())
```

## Important Notes

1. **OpenAI API-Compatible Models Only**: This service is designed specifically for OpenAI API-compatible models
2. **Long-Running Tasks**: Evaluation and performance testing tasks may take considerable time. We recommend setting appropriate HTTP timeout values on the client side as the API calls are synchronous and will block until completion.
3. **Output Directory**: Evaluation results are saved in the configured `work_dir` default is `outputs/`
4. **Error Handling**: The service returns detailed error messages and stack traces (in debug mode)
5. **Resource Management**: Pay attention to concurrency settings during stress testing to avoid server overload

## Error Codes

- `400`: Invalid request parameters
- `404`: Endpoint not found
- `500`: Internal server error

## Example Scenarios

### Scenario 1: Quick Evaluation of Qwen Model

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "sk-..."
    "datasets": ["gsm8k"]
    "limit": 100
  }'
```

### Scenario 2: Stress Testing Locally Deployed Model

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5"
    "url": "http://localhost:8000/v1/chat/completions"
    "api": "openai"
    "number": 1000
    "parallel": 20
    "max_tokens": 2048
  }'
```

### Scenario 3: Multi-Dataset Evaluation

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "datasets": ["gsm8k" "iquiz" "ceval"]
    "limit": 50
    "eval_batch_size": 4
  }'
```

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  <a href="README_zh.md">ä¸­æ–‡</a> &nbsp ï½œ &nbsp English &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/en/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  Chinese Documentation</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documentation</a>
<p>


> â­ If you like this project please click the "Star" button in the upper right corner to support us. Your support is our motivation to move forward!

## ğŸ“ Introduction

EvalScope is a powerful and easily extensible model evaluation framework created by the [ModelScope Community](https://modelscope.cn/) aiming to provide a one-stop evaluation solution for large model developers.

Whether you want to evaluate the general capabilities of models conduct multi-model performance comparisons or need to stress test models EvalScope can meet your needs.

## âœ¨ Key Features

- **ğŸ“š Comprehensive Evaluation Benchmarks**: Built-in multiple industry-recognized evaluation benchmarks including MMLU C-Eval GSM8K and more.
- **ğŸ§© Multi-modal and Multi-domain Support**: Supports evaluation of various model types including Large Language Models (LLM) Vision Language Models (VLM) Embedding Reranker AIGC and more.
- **ğŸš€ Multi-backend Integration**: Seamlessly integrates multiple evaluation backends including OpenCompass VLMEvalKit RAGEval to meet different evaluation needs.
- **âš¡ Inference Performance Testing**: Provides powerful model service stress testing tools supporting multiple performance metrics such as TTFT TPOT.
- **ğŸ“Š Interactive Reports**: Provides WebUI visualization interface supporting multi-dimensional model comparison report overview and detailed inspection.
- **âš”ï¸ Arena Mode**: Supports multi-model battles (Pairwise Battle) intuitively ranking and evaluating models.
- **ğŸ”§ Highly Extensible**: Developers can easily add custom datasets models and evaluation metrics.

<details><summary>ğŸ›ï¸ Overall Architecture</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope Overall Architecture.
</p>

1.  **Input Layer**
    - **Model Sources**: API models (OpenAI API) Local models (ModelScope)
    - **Datasets**: Standard evaluation benchmarks (MMLU/GSM8k etc.) Custom data (MCQ/QA)

2.  **Core Functions**
    - **Multi-backend Evaluation**: Native backend OpenCompass MTEB VLMEvalKit RAGAS
    - **Performance Monitoring**: Supports multiple model service APIs and data formats tracking TTFT/TPOP and other metrics
    - **Tool Extensions**: Integrates Tool-Bench Needle-in-a-Haystack etc.

3.  **Output Layer**
    - **Structured Reports**: Supports JSON Table Logs
    - **Visualization Platform**: Supports Gradio Wandb SwanLab

</details>

## ğŸ‰ What's New

> [!IMPORTANT]
> **Version 1.0 Refactoring**
>
> Version 1.0 introduces a major overhaul of the evaluation framework establishing a new more modular and extensible API layer under `evalscope/api`. Key improvements include standardized data models for benchmarks samples and results; a registry-based design for components such as benchmarks and metrics; and a rewritten core evaluator that orchestrates the new architecture. Existing benchmark adapters have been migrated to this API resulting in cleaner more consistent and easier-to-maintain implementations.

- ğŸ”¥ **[2025.12.02]** Added support for custom multimodal VQA evaluation; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/vlm.html). Added support for visualizing model service stress testing in ClearML; refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#clearml).
- ğŸ”¥ **[2025.11.26]** Added support for OpenAI-MRCR GSM8K-V MGSM MicroVQA IFBench SciCode benchmarks.
- ğŸ”¥ **[2025.11.18]** Added support for custom Function-Call (tool invocation) datasets to test whether models can timely and correctly call tools. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#function-calling-format-fc).
- ğŸ”¥ **[2025.11.14]** Added support for SWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini code evaluation benchmarks. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/swe_bench.html).
- ğŸ”¥ **[2025.11.12]** Added `pass@k` `vote@k` `pass^k` and other metric aggregation methods; added support for multimodal evaluation benchmarks such as A_OKVQA CMMU ScienceQA V*Bench.
- ğŸ”¥ **[2025.11.07]** Added support for Ï„Â²-bench an extended and enhanced version of Ï„-bench that includes a series of code fixes and adds telecom domain troubleshooting scenarios. Refer to the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/tau2_bench.html).
- ğŸ”¥ **[2025.10.30]** Added support for BFCL-v4 enabling evaluation of agent capabilities including web search and long-term memory. See the [usage documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v4.html).
- ğŸ”¥ **[2025.10.27]** Added support for LogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQA and other evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.26]** Added support for Conll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 and other Named Entity Recognition evaluation benchmarks. Thanks to @[penguinwang96825](https://github.com/penguinwang96825) for the code implementation.
- ğŸ”¥ **[2025.10.21]** Optimized sandbox environment usage in code evaluation supporting both local and remote operation modes. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/sandbox.html).
- ğŸ”¥ **[2025.10.20]** Added support for evaluation benchmarks including PolyMath SimpleVQA MathVerse MathVision AA-LCR; optimized evalscope perf performance to align with vLLM Bench. For details refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/vs_vllm_bench.html).
- ğŸ”¥ **[2025.10.14]** Added support for OCRBench OCRBench-v2 DocVQA InfoVQA ChartQA and BLINK multimodal image-text evaluation benchmarks.
- ğŸ”¥ **[2025.09.22]** Code evaluation benchmarks (HumanEval LiveCodeBench) now support running in a sandbox environment. To use this feature please install [ms-enclave](https://github.com/modelscope/ms-enclave) first.
- ğŸ”¥ **[2025.09.19]** Added support for multimodal image-text evaluation benchmarks including RealWorldQA AI2D MMStar MMBench and OmniBench as well as pure text evaluation benchmarks such as Multi-IF HealthBench and AMC.
- ğŸ”¥ **[2025.09.05]** Added support for vision-language multimodal model evaluation tasks such as MathVista and MMMU. For more supported datasets please [refer to the documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/vlm.html).
- ğŸ”¥ **[2025.09.04]** Added support for image editing task evaluation including the [GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/image_edit.html).
- ğŸ”¥ **[2025.08.22]** Version 1.0 Refactoring. Break changes please [refer to](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#switching-to-version-v1-0).
<details><summary>More</summary>

- ğŸ”¥ **[2025.07.18]** The model stress testing now supports randomly generating image-text data for multimodal model evaluation. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#id4).
- ğŸ”¥ **[2025.07.16]** Support for [Ï„-bench](https://github.com/sierra-research/tau-bench) has been added enabling the evaluation of AI Agent performance and reliability in real-world scenarios involving dynamic user and tool interactions. For usage instructions please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#bench).
- ğŸ”¥ **[2025.07.14]** Support for "Humanity's Last Exam" ([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle)) a highly challenging evaluation benchmark. For usage instructions refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam).
- ğŸ”¥ **[2025.07.03]** Refactored Arena Mode: now supports custom model battles outputs a model leaderboard and provides battle result visualization. See [reference](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html) for details.
- ğŸ”¥ **[2025.06.28]** Optimized custom dataset evaluation: now supports evaluation without reference answers. Enhanced LLM judge usage with built-in modes for "scoring directly without reference answers" and "checking answer consistency with reference answers". See [reference](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/llm.html#qa) for details.
- ğŸ”¥ **[2025.06.19]** Added support for the [BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3) benchmark designed to evaluate model function-calling capabilities across various scenarios. For more information refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/bfcl_v3.html).
- ğŸ”¥ **[2025.06.02]** Added support for the Needle-in-a-Haystack test. Simply specify `needle_haystack` to conduct the test and a corresponding heatmap will be generated in the `outputs/reports` folder providing a visual representation of the model's performance. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/needle_haystack.html) for more details.
- ğŸ”¥ **[2025.05.29]** Added support for two long document evaluation benchmarks: [DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary) and [FRAMES](https://modelscope.cn/datasets/iic/frames/summary). For usage guidelines please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html).
- ğŸ”¥ **[2025.05.16]** Model service performance stress testing now supports setting various levels of concurrency and outputs a performance test report. [Reference example](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/quick_start.html#id3).
- ğŸ”¥ **[2025.05.13]** Added support for the [ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static) dataset to evaluate model's tool-calling capabilities. Refer to the [documentation](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html) for usage instructions. Also added support for the [DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview) and [Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val) benchmarks to assess the reasoning capabilities of models.
- ğŸ”¥ **[2025.04.29]** Added Qwen3 Evaluation Best Practices [welcome to read ğŸ“–](https://evalscope.readthedocs.io/en/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** Support for text-to-image evaluation: Supports 8 metrics including MPS HPSv2.1Score etc. and evaluation benchmarks such as EvalMuse GenAI-Bench. Refer to the [user documentation](https://evalscope.readthedocs.io/en/latest/user_guides/aigc/t2i.html) for more details.
- ğŸ”¥ **[2025.04.10]** Model service stress testing tool now supports the `/v1/completions` endpoint (the default endpoint for vLLM benchmarking)
- ğŸ”¥ **[2025.04.08]** Support for evaluating embedding model services compatible with the OpenAI API has been added. For more details check the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters).
- ğŸ”¥ **[2025.03.27]** Added support for [AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview) and [ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary) evaluation benchmarks. For usage notes please refer to the [documentation](https://evalscope.readthedocs.io/en/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** The model inference service stress testing now supports generating prompts of specified length using random values. Refer to the [user guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/examples.html#using-the-random-dataset) for more details.
- ğŸ”¥ **[2025.03.13]** Added support for the [LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary) code evaluation benchmark which can be used by specifying `live_code_bench`. Supports evaluating QwQ-32B on LiveCodeBench refer to the [best practices](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html).
- ğŸ”¥ **[2025.03.11]** Added support for the [SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary) and [Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary) evaluation benchmarks. These are used to assess the factual accuracy of models and you can specify `simple_qa` and `chinese_simpleqa` for use. Support for specifying a judge model is also available. For more details refer to the [relevant parameter documentation](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).
- ğŸ”¥ **[2025.03.07]** Added support for the [QwQ-32B](https://modelscope.cn/models/Qwen/QwQ-32B/summary) model evaluate the model's reasoning ability and reasoning efficiency refer to [ğŸ“– Best Practices for QwQ-32B Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/eval_qwq.html) for more details.
- ğŸ”¥ **[2025.03.04]** Added support for the [SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary) dataset which covers 13 categories 72 first-level disciplines and 285 second-level disciplines totaling 26529 questions. You can use it by specifying `super_gpqa`.
- ğŸ”¥ **[2025.03.03]** Added support for evaluating the IQ and EQ of models. Refer to [ğŸ“– Best Practices for IQ and EQ Evaluation](https://evalscope.readthedocs.io/en/latest/best_practice/iquiz.html) to find out how smart your AI is!
- ğŸ”¥ **[2025.02.27]** Added support for evaluating the reasoning efficiency of models. Refer to [ğŸ“– Best Practices for Evaluating Thinking Efficiency](https://evalscope.readthedocs.io/en/latest/best_practice/think_eval.html). This implementation is inspired by the works [Overthinking](https://doi.org/10.48550/arXiv.2412.21187) and [Underthinking](https://doi.org/10.48550/arXiv.2501.18585).
- ğŸ”¥ **[2025.02.25]** Added support for two model inference-related evaluation benchmarks: [MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR) and [ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary). To use them simply specify `musr` and `process_bench` respectively in the datasets parameter.
- ğŸ”¥ **[2025.02.18]** Supports the AIME25 dataset which contains 15 questions (Grok3 scored 93 on this dataset).
- ğŸ”¥ **[2025.02.13]** Added support for evaluating DeepSeek distilled models including AIME24 MATH-500 and GPQA-Diamond datasetsï¼Œrefer to [best practice](https://evalscope.readthedocs.io/en/latest/best_practice/deepseek_r1_distill.html); Added support for specifying the `eval_batch_size` parameter to accelerate model evaluation.
- ğŸ”¥ **[2025.01.20]** Support for visualizing evaluation results including single model evaluation results and multi-model comparison refer to the [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html) for more details; Added [`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary) evaluation example evaluating the IQ and EQ of the model.
- ğŸ”¥ **[2025.01.07]** Native backend: Support for model API evaluation is now available. Refer to the [ğŸ“– Model API Evaluation Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html#api) for more details. Additionally support for the `ifeval` evaluation benchmark has been added.
- ğŸ”¥ğŸ”¥ **[2024.12.31]** Support for adding benchmark evaluations refer to the [ğŸ“– Benchmark Evaluation Addition Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html); support for custom mixed dataset evaluations allowing for more comprehensive model evaluations with less data refer to the [ğŸ“– Mixed Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/collection/index.html).
- ğŸ”¥ **[2024.12.13]** Model evaluation optimization: no need to pass the `--template-type` parameter anymore; supports starting evaluation with `evalscope eval --args`. Refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/get_started/basic_usage.html) for more details.
- ğŸ”¥ **[2024.11.26]** The model inference service performance evaluator has been completely refactored: it now supports local inference service startup and Speed Benchmark; asynchronous call error handling has been optimized. For more details refer to the [ğŸ“– User Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).
- ğŸ”¥ **[2024.10.31]** The best practice for evaluating Multimodal-RAG has been updated please check the [ğŸ“– Blog](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag) for more details.
- ğŸ”¥ **[2024.10.23]** Supports multimodal RAG evaluation including the assessment of image-text retrieval using [CLIP_Benchmark](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/clip_benchmark.html) and extends [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html) to support end-to-end multimodal metrics evaluation.
- ğŸ”¥ **[2024.10.8]** Support for RAG evaluation including independent evaluation of embedding models and rerankers using [MTEB/CMTEB](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/mteb.html) as well as end-to-end evaluation using [RAGAS](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/ragas.html).
- ğŸ”¥ **[2024.09.18]** Our documentation has been updated to include a blog module featuring some technical research and discussions related to evaluations. We invite you to [ğŸ“– read it](https://evalscope.readthedocs.io/en/refact_readme/blog/index.html).
- ğŸ”¥ **[2024.09.12]** Support for LongWriter evaluation which supports 10000+ word generation. You can use the benchmark [LongBench-Write](evalscope/third_party/longbench_write/README.md) to measure the long output quality as well as the output length.
- ğŸ”¥ **[2024.08.30]** Support for custom dataset evaluations including text datasets and multimodal image-text datasets.
- ğŸ”¥ **[2024.08.20]** Updated the official documentation including getting started guides best practices and FAQs. Feel free to [ğŸ“–read it here](https://evalscope.readthedocs.io/en/latest/)!
- ğŸ”¥ **[2024.08.09]** Simplified the installation process allowing for pypi installation of vlmeval dependencies; optimized the multimodal model evaluation experience achieving up to 10x acceleration based on the OpenAI API evaluation chain.
- ğŸ”¥ **[2024.07.31]** Important change: The package name `llmuses` has been changed to `evalscope`. Please update your code accordingly.
- ğŸ”¥ **[2024.07.26]** Support for **VLMEvalKit** as a third-party evaluation framework to initiate multimodal model evaluation tasks.
- ğŸ”¥ **[2024.06.29]** Support for **OpenCompass** as a third-party evaluation framework which we have encapsulated at a higher level supporting pip installation and simplifying evaluation task configuration.
- ğŸ”¥ **[2024.06.13]** EvalScope seamlessly integrates with the fine-tuning framework SWIFT providing full-chain support from LLM training to evaluation.
- ğŸ”¥ **[2024.06.13]** Integrated the Agent evaluation dataset ToolBench.

</details>

## â¤ï¸ Community & Support

Welcome to join our community to communicate with other developers and get help.

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  WeChat Group | DingTalk Group
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ Environment Setup

We recommend using `conda` to create a virtual environment and install with `pip`.

1.  **Create and Activate Conda Environment** (Python 3.10 recommended)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **Install EvalScope**

    - **Method 1: Install via PyPI (Recommended)**
      ```shell
      pip install evalscope
      ```

    - **Method 2: Install from Source (For Development)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **Install Additional Dependencies** (Optional)
    Install corresponding feature extensions according to your needs:
    ```shell
    # Performance testing
    pip install 'evalscope[perf]'

    # Visualization App
    pip install 'evalscope[app]'

    # Other evaluation backends
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # Install all dependencies
    pip install 'evalscope[all]'
    ```
    > If you installed from source please replace `evalscope` with `.` for example `pip install '.[perf]'`.

> [!NOTE]
> This project was formerly known as `llmuses`. If you need to use `v0.4.3` or earlier versions please run `pip install llmuses<=0.4.3` and use `from llmuses import ...` for imports.


## ğŸš€ Quick Start

You can start evaluation tasks in two ways: **command line** or **Python code**.

### Method 1. Using Command Line

Execute the `evalscope eval` command in any path to start evaluation. The following command will evaluate the `Qwen/Qwen2.5-0.5B-Instruct` model on `gsm8k` and `arc` datasets taking only 5 samples from each dataset.

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### Method 2. Using Python Code

Use the `run_task` function and `TaskConfig` object to configure and start evaluation tasks.

```python
from evalscope import run_task TaskConfig

# Configure evaluation task
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# Start evaluation
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ Tip:</b> `run_task` also supports dictionaries YAML or JSON files as configuration.</summary>

**Using Python Dictionary**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**Using YAML File** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### Output Results
After evaluation completion you will see a report in the terminal in the following format:
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ Advanced Usage

### Custom Evaluation Parameters

You can fine-tune model loading inference and dataset configuration through command line parameters.

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: Model loading parameters such as `revision` `precision` etc.
- `--generation-config`: Model generation parameters such as `temperature` `max_tokens` etc.
- `--dataset-args`: Dataset configuration parameters such as `few_shot_num` etc.

For details please refer to [ğŸ“– Complete Parameter Guide](https://evalscope.readthedocs.io/en/latest/get_started/parameters.html).

### Evaluating Online Model APIs

EvalScope supports evaluating model services deployed via APIs (such as services deployed with vLLM). Simply specify the service address and API Key.

1.  **Start Model Service** (using vLLM as example)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **Run Evaluation**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ Arena Mode

Arena mode evaluates model performance through pairwise battles between models providing win rates and rankings perfect for horizontal comparison of multiple models.

```text
# Example evaluation results
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
For details please refer to [ğŸ“– Arena Mode Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/arena.html).

### ğŸ–Šï¸ Custom Dataset Evaluation

EvalScope allows you to easily add and evaluate your own datasets. For details please refer to [ğŸ“– Custom Dataset Evaluation Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/custom_dataset/index.html).


## ğŸ§ª Other Evaluation Backends
EvalScope supports launching evaluation tasks through third-party evaluation frameworks (we call them "backends") to meet diverse evaluation needs.

- **Native**: EvalScope's default evaluation framework with comprehensive functionality.
- **OpenCompass**: Focuses on text-only evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: Focuses on multi-modal evaluation. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: Focuses on RAG evaluation supporting Embedding and Reranker models. [ğŸ“– Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/backend/rageval_backend/index.html)
- **Third-party Evaluation Tools**: Supports evaluation tasks like [ToolBench](https://evalscope.readthedocs.io/en/latest/third_party/toolbench.html).

## âš¡ Inference Performance Evaluation Tool
EvalScope provides a powerful stress testing tool for evaluating the performance of large language model services.

- **Key Metrics**: Supports throughput (Tokens/s) first token latency (TTFT) token generation latency (TPOT) etc.
- **Result Recording**: Supports recording results to `wandb` and `swanlab`.
- **Speed Benchmarks**: Can generate speed benchmark results similar to official reports.

For details please refer to [ğŸ“– Performance Testing Usage Guide](https://evalscope.readthedocs.io/en/latest/user_guides/stress_test/index.html).

Example output is shown below:
<p align="center">
    <img src="docs/en/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š Visualizing Evaluation Results

EvalScope provides a Gradio-based WebUI for interactive analysis and comparison of evaluation results.

1.  **Install Dependencies**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **Start Service**
    ```bash
    evalscope app
    ```
    Visit `http://127.0.0.1:7861` to open the visualization interface.

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/setting.png" alt="Setting" style="width: 85%;" />
      <p>Settings Interface</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>Model Comparison</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>Report Overview</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/en/get_started/images/report_details.png" alt="Report Details" style="width: 85%;" />
      <p>Report Details</p>
    </td>
  </tr>
</table>

For details please refer to [ğŸ“– Visualizing Evaluation Results](https://evalscope.readthedocs.io/en/latest/get_started/visualization.html).

## ğŸ‘·â€â™‚ï¸ Contributing

We welcome any contributions from the community! If you want to add new evaluation benchmarks models or features please refer to our [Contributing Guide](https://evalscope.readthedocs.io/en/latest/advanced_guides/add_benchmark.html).

Thanks to all developers who have contributed to EvalScope!

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š Citation

If you use EvalScope in your research please cite our work:
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

<p align="center">
    <br>
    <img src="docs/en/_static/images/evalscope_logo.png"/>
    <br>
<p>

<p align="center">
  ä¸­æ–‡ &nbsp ï½œ &nbsp <a href="evalscope.md">English</a> &nbsp
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.10-5be.svg">
<a href="https://badge.fury.io/py/evalscope"><img src="https://badge.fury.io/py/evalscope.svg" alt="PyPI version" height="18"></a>
<a href="https://pypi.org/project/evalscope"><img alt="PyPI - Downloads" src="https://static.pepy.tech/badge/evalscope"></a>
<a href="https://github.com/modelscope/evalscope/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
<a href='https://evalscope.readthedocs.io/zh-cn/latest/?badge=latest'><img src='https://readthedocs.org/projects/evalscope/badge/?version=latest' alt='Documentation Status' /></a>
<p>

<p align="center">
<a href="https://evalscope.readthedocs.io/zh-cn/latest/"> ğŸ“–  ä¸­æ–‡æ–‡æ¡£</a> &nbsp ï½œ &nbsp <a href="https://evalscope.readthedocs.io/en/latest/"> ğŸ“–  English Documents</a>
<p>


> â­ å¦‚æœä½ å–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·ç‚¹å‡»å³ä¸Šè§’çš„ "Star" æŒ‰é’®æ”¯æŒæˆ‘ä»¬ã€‚ä½ çš„æ”¯æŒæ˜¯æˆ‘ä»¬å‰è¿›çš„åŠ¨åŠ›ï¼

## ğŸ“ ç®€ä»‹

EvalScope æ˜¯ç”±[é­”æ­ç¤¾åŒº](https://modelscope.cn/)æ‰“é€ çš„ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§ã€æ˜“äºæ‰©å±•çš„æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ºå¤§æ¨¡å‹å¼€å‘è€…æä¾›ä¸€ç«™å¼è¯„æµ‹è§£å†³æ–¹æ¡ˆã€‚

æ— è®ºæ‚¨æ˜¯æƒ³è¯„ä¼°æ¨¡å‹çš„é€šç”¨èƒ½åŠ›ã€è¿›è¡Œå¤šæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼Œè¿˜æ˜¯éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼ŒEvalScope éƒ½èƒ½æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- **ğŸ“š å…¨é¢çš„è¯„æµ‹åŸºå‡†**: å†…ç½® MMLU C-Eval GSM8K ç­‰å¤šä¸ªä¸šç•Œå…¬è®¤çš„è¯„æµ‹åŸºå‡†ã€‚
- **ğŸ§© å¤šæ¨¡æ€ä¸å¤šé¢†åŸŸæ”¯æŒ**: æ”¯æŒå¤§è¯­è¨€æ¨¡å‹ (LLM)ã€å¤šæ¨¡æ€ (VLM)ã€Embeddingã€Rerankerã€AIGC ç­‰å¤šç§æ¨¡å‹çš„è¯„æµ‹ã€‚
- **ğŸš€ å¤šåç«¯é›†æˆ**: æ— ç¼é›†æˆ OpenCompass VLMEvalKit RAGEval ç­‰å¤šç§è¯„æµ‹åç«¯ï¼Œæ»¡è¶³ä¸åŒè¯„æµ‹éœ€æ±‚ã€‚
- **âš¡ æ¨ç†æ€§èƒ½æµ‹è¯•**: æä¾›å¼ºå¤§çš„æ¨¡å‹æœåŠ¡å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œæ”¯æŒ TTFT TPOT ç­‰å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ã€‚
- **ğŸ“Š äº¤äº’å¼æŠ¥å‘Š**: æä¾› WebUI å¯è§†åŒ–ç•Œé¢ï¼Œæ”¯æŒå¤šç»´åº¦æ¨¡å‹å¯¹æ¯”ã€æŠ¥å‘Šæ¦‚è§ˆå’Œè¯¦æƒ…æŸ¥é˜…ã€‚
- **âš”ï¸ ç«æŠ€åœºæ¨¡å¼**: æ”¯æŒå¤šæ¨¡å‹å¯¹æˆ˜ (Pairwise Battle)ï¼Œç›´è§‚åœ°å¯¹æ¨¡å‹è¿›è¡Œæ’åå’Œè¯„ä¼°ã€‚
- **ğŸ”§ é«˜åº¦å¯æ‰©å±•**: å¼€å‘è€…å¯ä»¥è½»æ¾æ·»åŠ è‡ªå®šä¹‰æ•°æ®é›†ã€æ¨¡å‹å’Œè¯„æµ‹æŒ‡æ ‡ã€‚

<details><summary>ğŸ›ï¸ æ•´ä½“æ¶æ„</summary>

<p align="center">
    <img src="https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/EvalScope%E6%9E%B6%E6%9E%84%E5%9B%BE.png" style="width: 70%;">
    <br>EvalScope æ•´ä½“æ¶æ„å›¾.
</p>

1.  **è¾“å…¥å±‚**
    - **æ¨¡å‹æ¥æº**: APIæ¨¡å‹ï¼ˆOpenAI APIï¼‰ã€æœ¬åœ°æ¨¡å‹ï¼ˆModelScopeï¼‰
    - **æ•°æ®é›†**: æ ‡å‡†è¯„æµ‹åŸºå‡†ï¼ˆMMLU/GSM8kç­‰ï¼‰ã€è‡ªå®šä¹‰æ•°æ®ï¼ˆMCQ/QAï¼‰

2.  **æ ¸å¿ƒåŠŸèƒ½**
    - **å¤šåç«¯è¯„ä¼°**: åŸç”Ÿåç«¯ã€OpenCompassã€MTEBã€VLMEvalKitã€RAGAS
    - **æ€§èƒ½ç›‘æ§**: æ”¯æŒå¤šç§æ¨¡å‹æœåŠ¡ API å’Œæ•°æ®æ ¼å¼ï¼Œè¿½è¸ª TTFT/TPOP ç­‰æŒ‡æ ‡
    - **å·¥å…·æ‰©å±•**: é›†æˆ Tool-Bench Needle-in-a-Haystack ç­‰

3.  **è¾“å‡ºå±‚**
    - **ç»“æ„åŒ–æŠ¥å‘Š**: æ”¯æŒ JSON Table Logs
    - **å¯è§†åŒ–å¹³å°**: æ”¯æŒ Gradio Wandb SwanLab

</details>

## ğŸ‰ å†…å®¹æ›´æ–°

> [!IMPORTANT]
> **ç‰ˆæœ¬ 1.0 é‡æ„**
>
> ç‰ˆæœ¬ 1.0 å¯¹è¯„æµ‹æ¡†æ¶è¿›è¡Œäº†é‡å¤§é‡æ„ï¼Œåœ¨ `evalscope/api` ä¸‹å»ºç«‹äº†å…¨æ–°çš„ã€æ›´æ¨¡å—åŒ–ä¸”æ˜“æ‰©å±•çš„ API å±‚ã€‚ä¸»è¦æ”¹è¿›åŒ…æ‹¬ï¼šä¸ºåŸºå‡†ã€æ ·æœ¬å’Œç»“æœå¼•å…¥äº†æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹ï¼›å¯¹åŸºå‡†å’ŒæŒ‡æ ‡ç­‰ç»„ä»¶é‡‡ç”¨æ³¨å†Œè¡¨å¼è®¾è®¡ï¼›å¹¶é‡å†™äº†æ ¸å¿ƒè¯„æµ‹å™¨ä»¥ååŒæ–°æ¶æ„ã€‚ç°æœ‰çš„åŸºå‡†å·²è¿ç§»åˆ°è¿™ä¸€ APIï¼Œå®ç°æ›´åŠ ç®€æ´ã€ä¸€è‡´ä¸”æ˜“äºç»´æŠ¤ã€‚

- ğŸ”¥ **[2025.12.02]** æ”¯æŒè‡ªå®šä¹‰å¤šæ¨¡æ€VQAè¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/vlm.html) ï¼›æ”¯æŒæ¨¡å‹æœåŠ¡å‹æµ‹åœ¨ ClearML ä¸Šå¯è§†åŒ–ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#clearml)ã€‚
- ğŸ”¥ **[2025.11.26]** æ–°å¢æ”¯æŒ OpenAI-MRCRã€GSM8K-Vã€MGSMã€MicroVQAã€IFBenchã€SciCode è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.18]** æ”¯æŒè‡ªå®šä¹‰ Function-Callï¼ˆå·¥å…·è°ƒç”¨ï¼‰æ•°æ®é›†ï¼Œæ¥æµ‹è¯•æ¨¡å‹èƒ½å¦é€‚æ—¶å¹¶æ­£ç¡®è°ƒç”¨å·¥å…·ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#fc)
- ğŸ”¥ **[2025.11.14]** æ–°å¢æ”¯æŒSWE-bench_Verified SWE-bench_Lite SWE-bench_Verified_mini ä»£ç è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/swe_bench.html)ã€‚
- ğŸ”¥ **[2025.11.12]** æ–°å¢`pass@k`ã€`vote@k`ã€`pass^k`ç­‰æŒ‡æ ‡èšåˆæ–¹æ³•ï¼›æ–°å¢æ”¯æŒA_OKVQA CMMU ScienceQ V*Benchç­‰å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.11.07]** æ–°å¢æ”¯æŒÏ„Â²-benchï¼Œæ˜¯ Ï„-bench çš„æ‰©å±•ä¸å¢å¼ºç‰ˆæœ¬ï¼ŒåŒ…å«ä¸€ç³»åˆ—ä»£ç ä¿®å¤ï¼Œå¹¶æ–°å¢äº†ç”µä¿¡ï¼ˆtelecomï¼‰é¢†åŸŸçš„æ•…éšœæ’æŸ¥åœºæ™¯ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/tau2_bench.html)ã€‚
- ğŸ”¥ **[2025.10.30]** æ–°å¢æ”¯æŒBFCL-v4ï¼Œæ”¯æŒagentçš„ç½‘ç»œæœç´¢å’Œé•¿æœŸè®°å¿†èƒ½åŠ›çš„è¯„æµ‹ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v4.html)ã€‚
- ğŸ”¥ **[2025.10.27]** æ–°å¢æ”¯æŒLogiQA HaluEval MathQA MRI-QA PIQA QASC CommonsenseQAç­‰è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.26]** æ–°å¢æ”¯æŒConll-2003 CrossNER Copious GeniaNER HarveyNER MIT-Movie-Trivia MIT-Restaurant OntoNotes5 WNUT2017 ç­‰å‘½åå®ä½“è¯†åˆ«è¯„æµ‹åŸºå‡†ã€‚æ„Ÿè°¢ @[penguinwang96825](https://github.com/penguinwang96825) æä¾›ä»£ç å®ç°ã€‚
- ğŸ”¥ **[2025.10.21]** ä¼˜åŒ–ä»£ç è¯„æµ‹ä¸­çš„æ²™ç®±ç¯å¢ƒä½¿ç”¨ï¼Œæ”¯æŒåœ¨æœ¬åœ°å’Œè¿œç¨‹ä¸¤ç§æ¨¡å¼ä¸‹è¿è¡Œï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/sandbox.html)ã€‚
- ğŸ”¥ **[2025.10.20]** æ–°å¢æ”¯æŒPolyMath SimpleVQA MathVerse MathVision AA-LCR ç­‰è¯„æµ‹åŸºå‡†ï¼›ä¼˜åŒ–evalscope perfè¡¨ç°ï¼Œå¯¹é½vLLM Benchï¼Œå…·ä½“å‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/vs_vllm_bench.html)ã€‚
- ğŸ”¥ **[2025.10.14]** æ–°å¢æ”¯æŒOCRBench OCRBench-v2 DocVQA InfoVQA ChartQA BLINK ç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.22]** ä»£ç è¯„æµ‹åŸºå‡†(HumanEval LiveCodeBench)æ”¯æŒåœ¨æ²™ç®±ç¯å¢ƒä¸­è¿è¡Œï¼Œè¦ä½¿ç”¨è¯¥åŠŸèƒ½éœ€å…ˆå®‰è£…[ms-enclave](https://github.com/modelscope/ms-enclave)ã€‚
- ğŸ”¥ **[2025.09.19]** æ–°å¢æ”¯æŒRealWorldQAã€AI2Dã€MMStarã€MMBenchã€OmniBenchç­‰å›¾æ–‡å¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼Œå’ŒMulti-IFã€HealthBenchã€AMCç­‰çº¯æ–‡æœ¬è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ **[2025.09.05]** æ”¯æŒè§†è§‰-è¯­è¨€å¤šæ¨¡æ€å¤§æ¨¡å‹çš„è¯„æµ‹ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šMathVistaã€MMMUï¼Œæ›´å¤šæ”¯æŒæ•°æ®é›†è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/vlm.html)ã€‚
- ğŸ”¥ **[2025.09.04]** æ”¯æŒå›¾åƒç¼–è¾‘ä»»åŠ¡è¯„æµ‹ï¼Œæ”¯æŒ[GEdit-Bench](https://modelscope.cn/datasets/stepfun-ai/GEdit-Bench) è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/image_edit.html)ã€‚
- ğŸ”¥ **[2025.08.22]** Version 1.0 é‡æ„ï¼Œä¸å…¼å®¹çš„æ›´æ–°è¯·[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#v1-0)ã€‚
<details> <summary>æ›´å¤š</summary>

- ğŸ”¥ **[2025.07.18]** æ¨¡å‹å‹æµ‹æ”¯æŒéšæœºç”Ÿæˆå›¾æ–‡æ•°æ®ï¼Œç”¨äºå¤šæ¨¡æ€æ¨¡å‹å‹æµ‹ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#id4)ã€‚
- ğŸ”¥ **[2025.07.16]** æ”¯æŒ[Ï„-bench](https://github.com/sierra-research/tau-bench)ï¼Œç”¨äºè¯„ä¼° AI Agentåœ¨åŠ¨æ€ç”¨æˆ·å’Œå·¥å…·äº¤äº’çš„å®é™…ç¯å¢ƒä¸­çš„æ€§èƒ½å’Œå¯é æ€§ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#bench)ã€‚
- ğŸ”¥ **[2025.07.14]** æ”¯æŒâ€œäººç±»æœ€åçš„è€ƒè¯•â€([Humanity's-Last-Exam](https://modelscope.cn/datasets/cais/hle))ï¼Œè¿™ä¸€é«˜éš¾åº¦è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ–¹æ³•[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/llm.html#humanity-s-last-exam)ã€‚
- ğŸ”¥ **[2025.07.03]** é‡æ„äº†ç«æŠ€åœºæ¨¡å¼ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹å¯¹æˆ˜ï¼Œè¾“å‡ºæ¨¡å‹æ’è¡Œæ¦œï¼Œä»¥åŠå¯¹æˆ˜ç»“æœå¯è§†åŒ–ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚
- ğŸ”¥ **[2025.06.28]** ä¼˜åŒ–è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼Œæ”¯æŒæ— å‚è€ƒç­”æ¡ˆè¯„æµ‹ï¼›ä¼˜åŒ–LLMè£åˆ¤ä½¿ç”¨ï¼Œé¢„ç½®â€œæ— å‚è€ƒç­”æ¡ˆç›´æ¥æ‰“åˆ†â€ å’Œ â€œåˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´â€ä¸¤ç§æ¨¡å¼ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/llm.html#qa)
- ğŸ”¥ **[2025.06.19]** æ–°å¢æ”¯æŒ[BFCL-v3](https://modelscope.cn/datasets/AI-ModelScope/bfcl_v3)è¯„æµ‹åŸºå‡†ï¼Œç”¨äºè¯„æµ‹æ¨¡å‹åœ¨å¤šç§åœºæ™¯ä¸‹çš„å‡½æ•°è°ƒç”¨èƒ½åŠ›ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/bfcl_v3.html)ã€‚
- ğŸ”¥ **[2025.06.02]** æ–°å¢æ”¯æŒå¤§æµ·æé’ˆæµ‹è¯•ï¼ˆNeedle-in-a-Haystackï¼‰ï¼ŒæŒ‡å®š`needle_haystack`å³å¯è¿›è¡Œæµ‹è¯•ï¼Œå¹¶åœ¨`outputs/reports`æ–‡ä»¶å¤¹ä¸‹ç”Ÿæˆå¯¹åº”çš„heatmapï¼Œç›´è§‚å±•ç°æ¨¡å‹æ€§èƒ½ï¼Œä½¿ç”¨[å‚è€ƒ](https://evalscope.readthedocs.io/zh-cn/latest/third_party/needle_haystack.html)ã€‚
- ğŸ”¥ **[2025.05.29]** æ–°å¢æ”¯æŒ[DocMath](https://modelscope.cn/datasets/yale-nlp/DocMath-Eval/summary)å’Œ[FRAMES](https://modelscope.cn/datasets/iic/frames/summary)ä¸¤ä¸ªé•¿æ–‡æ¡£è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.05.16]** æ¨¡å‹æœåŠ¡æ€§èƒ½å‹æµ‹æ”¯æŒè®¾ç½®å¤šç§å¹¶å‘ï¼Œå¹¶è¾“å‡ºæ€§èƒ½å‹æµ‹æŠ¥å‘Šï¼Œ[å‚è€ƒç¤ºä¾‹](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/quick_start.html#id3)ã€‚
- ğŸ”¥ **[2025.05.13]** æ–°å¢æ”¯æŒ[ToolBench-Static](https://modelscope.cn/datasets/AI-ModelScope/ToolBench-Static)æ•°æ®é›†ï¼Œè¯„æµ‹æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html)ï¼›æ”¯æŒ[DROP](https://modelscope.cn/datasets/AI-ModelScope/DROP/dataPeview)å’Œ[Winogrande](https://modelscope.cn/datasets/AI-ModelScope/winogrande_val)è¯„æµ‹åŸºå‡†ï¼Œè¯„æµ‹æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
- ğŸ”¥ **[2025.04.29]** æ–°å¢Qwen3è¯„æµ‹æœ€ä½³å®è·µï¼Œ[æ¬¢è¿é˜…è¯»ğŸ“–](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/qwen3.html)
- ğŸ”¥ **[2025.04.27]** æ”¯æŒæ–‡ç”Ÿå›¾è¯„æµ‹ï¼šæ”¯æŒMPSã€HPSv2.1Scoreç­‰8ä¸ªæŒ‡æ ‡ï¼Œæ”¯æŒEvalMuseã€GenAI-Benchç­‰è¯„æµ‹åŸºå‡†ï¼Œå‚è€ƒ[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/aigc/t2i.html)
- ğŸ”¥ **[2025.04.10]** æ¨¡å‹æœåŠ¡å‹æµ‹å·¥å…·æ”¯æŒ`/v1/completions`ç«¯ç‚¹ï¼ˆä¹Ÿæ˜¯vLLMåŸºå‡†æµ‹è¯•çš„é»˜è®¤ç«¯ç‚¹ï¼‰
- ğŸ”¥ **[2025.04.08]** æ”¯æŒOpenAI APIå…¼å®¹çš„Embeddingæ¨¡å‹æœåŠ¡è¯„æµ‹ï¼ŒæŸ¥çœ‹[ä½¿ç”¨æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html#configure-evaluation-parameters)
- ğŸ”¥ **[2025.03.27]** æ–°å¢æ”¯æŒ[AlpacaEval](https://www.modelscope.cn/datasets/AI-ModelScope/alpaca_eval/dataPeview)å’Œ[ArenaHard](https://modelscope.cn/datasets/AI-ModelScope/arena-hard-auto-v0.1/summary)è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨æ³¨æ„äº‹é¡¹è¯·æŸ¥çœ‹[æ–‡æ¡£](https://evalscope.readthedocs.io/zh-cn/latest/get_started/supported_dataset/index.html)
- ğŸ”¥ **[2025.03.20]** æ¨¡å‹æ¨ç†æœåŠ¡å‹æµ‹æ”¯æŒrandomç”ŸæˆæŒ‡å®šèŒƒå›´é•¿åº¦çš„promptï¼Œå‚è€ƒ[ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/examples.html#random)
- ğŸ”¥ **[2025.03.13]** æ–°å¢æ”¯æŒ[LiveCodeBench](https://www.modelscope.cn/datasets/AI-ModelScope/code_generation_lite/summary)ä»£ç è¯„æµ‹åŸºå‡†ï¼ŒæŒ‡å®š`live_code_bench`å³å¯ä½¿ç”¨ï¼›æ”¯æŒQwQ-32B åœ¨LiveCodeBenchä¸Šè¯„æµ‹ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.11]** æ–°å¢æ”¯æŒ[SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/SimpleQA/summary)å’Œ[Chinese SimpleQA](https://modelscope.cn/datasets/AI-ModelScope/Chinese-SimpleQA/summary)è¯„æµ‹åŸºå‡†ï¼Œç”¨ä¸è¯„æµ‹æ¨¡å‹çš„äº‹å®æ­£ç¡®æ€§ï¼ŒæŒ‡å®š`simple_qa`å’Œ`chinese_simpleqa`ä½¿ç”¨ã€‚åŒæ—¶æ”¯æŒæŒ‡å®šè£åˆ¤æ¨¡å‹ï¼Œå‚è€ƒ[ç›¸å…³å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚
- ğŸ”¥ **[2025.03.07]** æ–°å¢QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µï¼Œè¯„æµ‹äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ä»¥åŠæ¨ç†æ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–QwQ-32Bæ¨¡å‹è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/eval_qwq.html)ã€‚
- ğŸ”¥ **[2025.03.04]** æ–°å¢æ”¯æŒ[SuperGPQA](https://modelscope.cn/datasets/m-a-p/SuperGPQA/summary)æ•°æ®é›†ï¼Œå…¶è¦†ç›– 13 ä¸ªé—¨ç±»ã€72 ä¸ªä¸€çº§å­¦ç§‘å’Œ 285 ä¸ªäºŒçº§å­¦ç§‘ï¼Œå…± 26529 ä¸ªé—®é¢˜ï¼ŒæŒ‡å®š`super_gpqa`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.03.03]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨¡å‹çš„æ™ºå•†å’Œæƒ…å•†ï¼Œå‚è€ƒ[ğŸ“–æ™ºå•†å’Œæƒ…å•†è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/iquiz.html)ï¼Œæ¥æµ‹æµ‹ä½ å®¶çš„AIæœ‰å¤šèªæ˜ï¼Ÿ
- ğŸ”¥ **[2025.02.27]** æ–°å¢æ”¯æŒè¯„æµ‹æ¨ç†æ¨¡å‹çš„æ€è€ƒæ•ˆç‡ï¼Œå‚è€ƒ[ğŸ“–æ€è€ƒæ•ˆç‡è¯„æµ‹æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/think_eval.html)ï¼Œè¯¥å®ç°å‚è€ƒäº†[Overthinking](https://doi.org/10.48550/arXiv.2412.21187) å’Œ [Underthinking](https://doi.org/10.48550/arXiv.2501.18585)ä¸¤ç¯‡å·¥ä½œã€‚
- ğŸ”¥ **[2025.02.25]** æ–°å¢æ”¯æŒ[MuSR](https://modelscope.cn/datasets/AI-ModelScope/MuSR)å’Œ[ProcessBench](https://www.modelscope.cn/datasets/Qwen/ProcessBench/summary)ä¸¤ä¸ªæ¨¡å‹æ¨ç†ç›¸å…³è¯„æµ‹åŸºå‡†ï¼Œdatasetsåˆ†åˆ«æŒ‡å®š`musr`å’Œ`process_bench`å³å¯ä½¿ç”¨ã€‚
- ğŸ”¥ **[2025.02.18]** æ”¯æŒAIME25æ•°æ®é›†ï¼ŒåŒ…å«15é“é¢˜ç›®ï¼ˆGrok3 åœ¨è¯¥æ•°æ®é›†ä¸Šå¾—åˆ†ä¸º93åˆ†ï¼‰
- ğŸ”¥ **[2025.02.13]** æ”¯æŒDeepSeekè’¸é¦æ¨¡å‹è¯„æµ‹ï¼ŒåŒ…æ‹¬AIME24 MATH-500 GPQA-Diamondæ•°æ®é›†ï¼Œå‚è€ƒ[æœ€ä½³å®è·µ](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/deepseek_r1_distill.html)ï¼›æ”¯æŒæŒ‡å®š`eval_batch_size`å‚æ•°ï¼ŒåŠ é€Ÿæ¨¡å‹è¯„æµ‹
- ğŸ”¥ **[2025.01.20]** æ”¯æŒå¯è§†åŒ–è¯„æµ‹ç»“æœï¼ŒåŒ…æ‹¬å•æ¨¡å‹è¯„æµ‹ç»“æœå’Œå¤šæ¨¡å‹è¯„æµ‹ç»“æœå¯¹æ¯”ï¼Œå‚è€ƒ[ğŸ“–å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ï¼›æ–°å¢[`iquiz`](https://modelscope.cn/datasets/AI-ModelScope/IQuiz/summary)è¯„æµ‹æ ·ä¾‹ï¼Œè¯„æµ‹æ¨¡å‹çš„IQå’ŒEQã€‚
- ğŸ”¥ **[2025.01.07]** Native backend: æ”¯æŒæ¨¡å‹APIè¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–æ¨¡å‹APIè¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html#api)ï¼›æ–°å¢æ”¯æŒ`ifeval`è¯„æµ‹åŸºå‡†ã€‚
- ğŸ”¥ğŸ”¥ **[2024.12.31]** æ”¯æŒåŸºå‡†è¯„æµ‹æ·»åŠ ï¼Œå‚è€ƒ[ğŸ“–åŸºå‡†è¯„æµ‹æ·»åŠ æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ï¼›æ”¯æŒè‡ªå®šä¹‰æ··åˆæ•°æ®é›†è¯„æµ‹ï¼Œç”¨æ›´å°‘çš„æ•°æ®ï¼Œæ›´å…¨é¢çš„è¯„æµ‹æ¨¡å‹ï¼Œå‚è€ƒ[ğŸ“–æ··åˆæ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/collection/index.html)
- ğŸ”¥ **[2024.12.13]** æ¨¡å‹è¯„æµ‹ä¼˜åŒ–ï¼Œä¸å†éœ€è¦ä¼ é€’`--template-type`å‚æ•°ï¼›æ”¯æŒ`evalscope eval --args`å¯åŠ¨è¯„æµ‹ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/get_started/basic_usage.html)
- ğŸ”¥ **[2024.11.26]** æ¨¡å‹æ¨ç†å‹æµ‹å·¥å…·é‡æ„å®Œæˆï¼šæ”¯æŒæœ¬åœ°å¯åŠ¨æ¨ç†æœåŠ¡ã€æ”¯æŒSpeed Benchmarkï¼›ä¼˜åŒ–å¼‚æ­¥è°ƒç”¨é”™è¯¯å¤„ç†ï¼Œå‚è€ƒ[ğŸ“–ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)
- ğŸ”¥ **[2024.10.31]** å¤šæ¨¡æ€RAGè¯„æµ‹æœ€ä½³å®è·µå‘å¸ƒï¼Œå‚è€ƒ[ğŸ“–åšå®¢](https://evalscope.readthedocs.io/zh-cn/latest/blog/RAG/multimodal_RAG.html#multimodal-rag)
- ğŸ”¥ **[2024.10.23]** æ”¯æŒå¤šæ¨¡æ€RAGè¯„æµ‹ï¼ŒåŒ…æ‹¬[CLIP_Benchmark](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/clip_benchmark.html)è¯„æµ‹å›¾æ–‡æ£€ç´¢å™¨ï¼Œä»¥åŠæ‰©å±•äº†[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)ä»¥æ”¯æŒç«¯åˆ°ç«¯å¤šæ¨¡æ€æŒ‡æ ‡è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.10.8]** æ”¯æŒRAGè¯„æµ‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨[MTEB/CMTEB](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/mteb.html)è¿›è¡Œembeddingæ¨¡å‹å’Œrerankerçš„ç‹¬ç«‹è¯„æµ‹ï¼Œä»¥åŠä½¿ç”¨[RAGAS](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html)è¿›è¡Œç«¯åˆ°ç«¯è¯„æµ‹ã€‚
- ğŸ”¥ **[2024.09.18]** æˆ‘ä»¬çš„æ–‡æ¡£å¢åŠ äº†åšå®¢æ¨¡å—ï¼ŒåŒ…å«ä¸€äº›è¯„æµ‹ç›¸å…³çš„æŠ€æœ¯è°ƒç ”å’Œåˆ†äº«ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/blog/index.html)
- ğŸ”¥ **[2024.09.12]** æ”¯æŒ LongWriter è¯„æµ‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åŸºå‡†æµ‹è¯• [LongBench-Write](evalscope/third_party/longbench_write/README.md) æ¥è¯„æµ‹é•¿è¾“å‡ºçš„è´¨é‡ä»¥åŠè¾“å‡ºé•¿åº¦ã€‚
- ğŸ”¥ **[2024.08.30]** æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹ï¼ŒåŒ…æ‹¬æ–‡æœ¬æ•°æ®é›†å’Œå¤šæ¨¡æ€å›¾æ–‡æ•°æ®é›†ã€‚
- ğŸ”¥ **[2024.08.20]** æ›´æ–°äº†å®˜æ–¹æ–‡æ¡£ï¼ŒåŒ…æ‹¬å¿«é€Ÿä¸Šæ‰‹ã€æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜ç­‰ï¼Œæ¬¢è¿[ğŸ“–é˜…è¯»](https://evalscope.readthedocs.io/zh-cn/latest/)ã€‚
- ğŸ”¥ **[2024.08.09]** ç®€åŒ–å®‰è£…æ–¹å¼ï¼Œæ”¯æŒpypiå®‰è£…vlmevalç›¸å…³ä¾èµ–ï¼›ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä½“éªŒï¼ŒåŸºäºOpenAI APIæ–¹å¼çš„è¯„æµ‹é“¾è·¯ï¼Œæœ€é«˜åŠ é€Ÿ10å€ã€‚
- ğŸ”¥ **[2024.07.31]** é‡è¦ä¿®æ”¹ï¼š`llmuses`åŒ…åä¿®æ”¹ä¸º`evalscope`ï¼Œè¯·åŒæ­¥ä¿®æ”¹æ‚¨çš„ä»£ç ã€‚
- ğŸ”¥ **[2024.07.26]** æ”¯æŒ**VLMEvalKit**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œå‘èµ·å¤šæ¨¡æ€æ¨¡å‹è¯„æµ‹ä»»åŠ¡ã€‚
- ğŸ”¥ **[2024.06.29]** æ”¯æŒ**OpenCompass**ä½œä¸ºç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œäº†é«˜çº§å°è£…ï¼Œæ”¯æŒpipæ–¹å¼å®‰è£…ï¼Œç®€åŒ–äº†è¯„æµ‹ä»»åŠ¡é…ç½®ã€‚
- ğŸ”¥ **[2024.06.13]** EvalScopeä¸å¾®è°ƒæ¡†æ¶SWIFTè¿›è¡Œæ— ç¼å¯¹æ¥ï¼Œæä¾›LLMä»è®­ç»ƒåˆ°è¯„æµ‹çš„å…¨é“¾è·¯æ”¯æŒ ã€‚
- ğŸ”¥ **[2024.06.13]** æ¥å…¥Agentè¯„æµ‹é›†ToolBenchã€‚
</details>

## â¤ï¸ ç¤¾åŒºä¸æ”¯æŒ

æ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºï¼Œä¸å…¶ä»–å¼€å‘è€…äº¤æµå¹¶è·å–å¸®åŠ©ã€‚

[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  å¾®ä¿¡ç¾¤ | é’‰é’‰ç¾¤
:-------------------------:|:-------------------------:|:-------------------------:
<img src="docs/asset/discord_qr.jpg" width="160" height="160">  |  <img src="docs/asset/wechat.png" width="160" height="160"> | <img src="docs/asset/dingding.png" width="160" height="160">



## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬æ¨èä½¿ç”¨ `conda` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶ä½¿ç”¨ `pip` å®‰è£…ã€‚

1.  **åˆ›å»ºå¹¶æ¿€æ´» Conda ç¯å¢ƒ** (æ¨èä½¿ç”¨ Python 3.10)
    ```shell
    conda create -n evalscope python=3.10
    conda activate evalscope
    ```

2.  **å®‰è£… EvalScope**

    - **æ–¹å¼ä¸€ï¼šé€šè¿‡ PyPI å®‰è£… (æ¨è)**
      ```shell
      pip install evalscope
      ```

    - **æ–¹å¼äºŒï¼šé€šè¿‡æºç å®‰è£… (ç”¨äºå¼€å‘)**
      ```shell
      git clone https://github.com/modelscope/evalscope.git
      cd evalscope
      pip install -e .
      ```

3.  **å®‰è£…é¢å¤–ä¾èµ–** (å¯é€‰)
    æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œå®‰è£…ç›¸åº”çš„åŠŸèƒ½æ‰©å±•ï¼š
    ```shell
    # æ€§èƒ½æµ‹è¯•
    pip install 'evalscope[perf]'

    # å¯è§†åŒ–App
    pip install 'evalscope[app]'

    # å…¶ä»–è¯„æµ‹åç«¯
    pip install 'evalscope[opencompass]'
    pip install 'evalscope[vlmeval]'
    pip install 'evalscope[rag]'

    # å®‰è£…æ‰€æœ‰ä¾èµ–
    pip install 'evalscope[all]'
    ```
    > å¦‚æœæ‚¨é€šè¿‡æºç å®‰è£…ï¼Œè¯·å°† `evalscope` æ›¿æ¢ä¸º `.`ï¼Œä¾‹å¦‚ `pip install '.[perf]'`ã€‚

> [!NOTE]
> æœ¬é¡¹ç›®æ›¾ç”¨å `llmuses`ã€‚å¦‚æœæ‚¨éœ€è¦ä½¿ç”¨ `v0.4.3` æˆ–æ›´æ—©ç‰ˆæœ¬ï¼Œè¯·è¿è¡Œ `pip install llmuses<=0.4.3` å¹¶ä½¿ç”¨ `from llmuses import ...` å¯¼å…¥ã€‚


## ğŸš€ å¿«é€Ÿå¼€å§‹

æ‚¨å¯ä»¥é€šè¿‡**å‘½ä»¤è¡Œ**æˆ– **Python ä»£ç **ä¸¤ç§æ–¹å¼å¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

### æ–¹å¼1. ä½¿ç”¨å‘½ä»¤è¡Œ

åœ¨ä»»æ„è·¯å¾„ä¸‹æ‰§è¡Œ `evalscope eval` å‘½ä»¤å³å¯å¼€å§‹è¯„æµ‹ã€‚ä»¥ä¸‹å‘½ä»¤å°†åœ¨ `gsm8k` å’Œ `arc` æ•°æ®é›†ä¸Šè¯„æµ‹ `Qwen/Qwen2.5-0.5B-Instruct` æ¨¡å‹ï¼Œæ¯ä¸ªæ•°æ®é›†åªå– 5 ä¸ªæ ·æœ¬ã€‚

```bash
evalscope eval \
 --model Qwen/Qwen2.5-0.5B-Instruct \
 --datasets gsm8k arc \
 --limit 5
```

### æ–¹å¼2. ä½¿ç”¨Pythonä»£ç 

ä½¿ç”¨ `run_task` å‡½æ•°å’Œ `TaskConfig` å¯¹è±¡æ¥é…ç½®å’Œå¯åŠ¨è¯„æµ‹ä»»åŠ¡ã€‚

```python
from evalscope import run_task TaskConfig

# é…ç½®è¯„æµ‹ä»»åŠ¡
task_cfg = TaskConfig(
    model='Qwen/Qwen2.5-0.5B-Instruct'
    datasets=['gsm8k' 'arc']
    limit=5
)

# å¯åŠ¨è¯„æµ‹
run_task(task_cfg)
```

<details><summary><b>ğŸ’¡ æç¤ºï¼š</b> `run_task` è¿˜æ”¯æŒå­—å…¸ã€YAML æˆ– JSON æ–‡ä»¶ä½œä¸ºé…ç½®ã€‚</summary>

**ä½¿ç”¨ Python å­—å…¸**

```python
from evalscope.run import run_task

task_cfg = {
    'model': 'Qwen/Qwen2.5-0.5B-Instruct'
    'datasets': ['gsm8k' 'arc']
    'limit': 5
}
run_task(task_cfg=task_cfg)
```

**ä½¿ç”¨ YAML æ–‡ä»¶** (`config.yaml`)
```yaml
model: Qwen/Qwen2.5-0.5B-Instruct
datasets:
  - gsm8k
  - arc
limit: 5
```
```python
from evalscope.run import run_task

run_task(task_cfg="config.yaml")
```
</details>

### è¾“å‡ºç»“æœ
è¯„æµ‹å®Œæˆåï¼Œæ‚¨å°†åœ¨ç»ˆç«¯çœ‹åˆ°å¦‚ä¸‹æ ¼å¼çš„æŠ¥å‘Šï¼š
```text
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Model Name            | Dataset Name   | Metric Name     | Category Name   | Subset Name   |   Num |   Score |
+=======================+================+=================+=================+===============+=======+=========+
| Qwen2.5-0.5B-Instruct | gsm8k          | AverageAccuracy | default         | main          |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Easy      |     5 |     0.8 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
| Qwen2.5-0.5B-Instruct | ai2_arc        | AverageAccuracy | default         | ARC-Challenge |     5 |     0.4 |
+-----------------------+----------------+-----------------+-----------------+---------------+-------+---------+
```

## ğŸ“ˆ è¿›é˜¶ç”¨æ³•

### è‡ªå®šä¹‰è¯„æµ‹å‚æ•°

æ‚¨å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ç²¾ç»†åŒ–æ§åˆ¶æ¨¡å‹åŠ è½½ã€æ¨ç†å’Œæ•°æ®é›†é…ç½®ã€‚

```shell
evalscope eval \
 --model Qwen/Qwen3-0.6B \
 --model-args '{"revision": "master" "precision": "torch.float16" "device_map": "auto"}' \
 --generation-config '{"do_sample":true"temperature":0.6"max_tokens":512}' \
 --dataset-args '{"gsm8k": {"few_shot_num": 0 "few_shot_random": false}}' \
 --datasets gsm8k \
 --limit 10
```

- `--model-args`: æ¨¡å‹åŠ è½½å‚æ•°ï¼Œå¦‚ `revision` `precision` ç­‰ã€‚
- `--generation-config`: æ¨¡å‹ç”Ÿæˆå‚æ•°ï¼Œå¦‚ `temperature` `max_tokens` ç­‰ã€‚
- `--dataset-args`: æ•°æ®é›†é…ç½®å‚æ•°ï¼Œå¦‚ `few_shot_num` ç­‰ã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å…¨éƒ¨å‚æ•°è¯´æ˜](https://evalscope.readthedocs.io/zh-cn/latest/get_started/parameters.html)ã€‚

### è¯„æµ‹åœ¨çº¿æ¨¡å‹ API

EvalScope æ”¯æŒè¯„æµ‹é€šè¿‡ API éƒ¨ç½²çš„æ¨¡å‹æœåŠ¡ï¼ˆå¦‚ vLLM éƒ¨ç½²çš„æœåŠ¡ï¼‰ã€‚åªéœ€æŒ‡å®šæœåŠ¡åœ°å€å’Œ API Key å³å¯ã€‚

1.  **å¯åŠ¨æ¨¡å‹æœåŠ¡** (ä»¥ vLLM ä¸ºä¾‹)
    ```shell
    export VLLM_USE_MODELSCOPE=True
    python -m vllm.entrypoints.openai.api_server \
      --model Qwen/Qwen2.5-0.5B-Instruct \
      --served-model-name qwen2.5 \
      --port 8801
    ```

2.  **è¿è¡Œè¯„æµ‹**
    ```shell
    evalscope eval \
     --model qwen2.5 \
     --eval-type openai_api \
     --api-url http://127.0.0.1:8801/v1 \
     --api-key EMPTY \
     --datasets gsm8k \
     --limit 10
    ```

### âš”ï¸ ç«æŠ€åœºæ¨¡å¼ (Arena)

ç«æŠ€åœºæ¨¡å¼é€šè¿‡æ¨¡å‹é—´çš„ä¸¤ä¸¤å¯¹æˆ˜ï¼ˆPairwise Battleï¼‰æ¥è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ç»™å‡ºèƒœç‡å’Œæ’åï¼Œéå¸¸é€‚åˆå¤šæ¨¡å‹æ¨ªå‘å¯¹æ¯”ã€‚

```text
# è¯„æµ‹ç»“æœç¤ºä¾‹
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)
```
è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– ç«æŠ€åœºæ¨¡å¼ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/arena.html)ã€‚

### ğŸ–Šï¸ è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹

EvalScope å…è®¸æ‚¨è½»æ¾æ·»åŠ å’Œè¯„æµ‹è‡ªå·±çš„æ•°æ®é›†ã€‚è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/custom_dataset/index.html)ã€‚


## ğŸ§ª å…¶ä»–è¯„æµ‹åç«¯
EvalScope æ”¯æŒé€šè¿‡ç¬¬ä¸‰æ–¹è¯„æµ‹æ¡†æ¶ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œåç«¯â€ï¼‰å‘èµ·è¯„æµ‹ä»»åŠ¡ï¼Œä»¥æ»¡è¶³å¤šæ ·åŒ–çš„è¯„æµ‹éœ€æ±‚ã€‚

- **Native**: EvalScope çš„é»˜è®¤è¯„æµ‹æ¡†æ¶ï¼ŒåŠŸèƒ½å…¨é¢ã€‚
- **OpenCompass**: ä¸“æ³¨äºçº¯æ–‡æœ¬è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/opencompass_backend.html)
- **VLMEvalKit**: ä¸“æ³¨äºå¤šæ¨¡æ€è¯„æµ‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/vlmevalkit_backend.html)
- **RAGEval**: ä¸“æ³¨äº RAG è¯„æµ‹ï¼Œæ”¯æŒ Embedding å’Œ Reranker æ¨¡å‹ã€‚ [ğŸ“– ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/index.html)
- **ç¬¬ä¸‰æ–¹è¯„æµ‹å·¥å…·**: æ”¯æŒ [ToolBench](https://evalscope.readthedocs.io/zh-cn/latest/third_party/toolbench.html) ç­‰è¯„æµ‹ä»»åŠ¡ã€‚

## âš¡ æ¨ç†æ€§èƒ½è¯„æµ‹å·¥å…·
EvalScope æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹æœåŠ¡çš„æ€§èƒ½ã€‚

- **å…³é”®æŒ‡æ ‡**: æ”¯æŒååé‡ (Tokens/s)ã€é¦–å­—å»¶è¿Ÿ (TTFT)ã€Token ç”Ÿæˆå»¶è¿Ÿ (TPOT) ç­‰ã€‚
- **ç»“æœè®°å½•**: æ”¯æŒå°†ç»“æœè®°å½•åˆ° `wandb` å’Œ `swanlab`ã€‚
- **é€Ÿåº¦åŸºå‡†**: å¯ç”Ÿæˆç±»ä¼¼å®˜æ–¹æŠ¥å‘Šçš„é€Ÿåº¦åŸºå‡†æµ‹è¯•ç»“æœã€‚

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– æ€§èƒ½æµ‹è¯•ä½¿ç”¨æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/index.html)ã€‚

è¾“å‡ºç¤ºä¾‹å¦‚ä¸‹ï¼š
<p align="center">
    <img src="docs/zh/user_guides/stress_test/images/multi_perf.png" style="width: 80%;">
</p>


## ğŸ“Š å¯è§†åŒ–è¯„æµ‹ç»“æœ

EvalScope æä¾›äº†ä¸€ä¸ªåŸºäº Gradio çš„ WebUIï¼Œç”¨äºäº¤äº’å¼åœ°åˆ†æå’Œæ¯”è¾ƒè¯„æµ‹ç»“æœã€‚

1.  **å®‰è£…ä¾èµ–**
    ```bash
    pip install 'evalscope[app]'
    ```

2.  **å¯åŠ¨æœåŠ¡**
    ```bash
    evalscope app
    ```
    è®¿é—® `http://127.0.0.1:7861` å³å¯æ‰“å¼€å¯è§†åŒ–ç•Œé¢ã€‚

<table>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/setting.png" alt="Setting" style="width: 90%;" />
      <p>è®¾ç½®ç•Œé¢</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/model_compare.png" alt="Model Compare" style="width: 100%;" />
      <p>æ¨¡å‹æ¯”è¾ƒ</p>
    </td>
  </tr>
  <tr>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_overview.png" alt="Report Overview" style="width: 100%;" />
      <p>æŠ¥å‘Šæ¦‚è§ˆ</p>
    </td>
    <td style="text-align: center;">
      <img src="docs/zh/get_started/images/report_details.png" alt="Report Details" style="width: 91%;" />
      <p>æŠ¥å‘Šè¯¦æƒ…</p>
    </td>
  </tr>
</table>

è¯¦æƒ…è¯·å‚è€ƒ [ğŸ“– å¯è§†åŒ–è¯„æµ‹ç»“æœ](https://evalscope.readthedocs.io/zh-cn/latest/get_started/visualization.html)ã€‚

## ğŸ‘·â€â™‚ï¸ è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿æ¥è‡ªç¤¾åŒºçš„ä»»ä½•è´¡çŒ®ï¼å¦‚æœæ‚¨å¸Œæœ›æ·»åŠ æ–°çš„è¯„æµ‹åŸºå‡†ã€æ¨¡å‹æˆ–åŠŸèƒ½ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„ [è´¡çŒ®æŒ‡å—](https://evalscope.readthedocs.io/zh-cn/latest/advanced_guides/add_benchmark.html)ã€‚

æ„Ÿè°¢æ‰€æœ‰ä¸º EvalScope åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ï¼

<a href="https://github.com/modelscope/evalscope/graphs/contributors" target="_blank">
  <table>
    <tr>
      <th colspan="2">
        <br><img src="https://contrib.rocks/image?repo=modelscope/evalscope"><br><br>
      </th>
    </tr>
  </table>
</a>


## ğŸ“š å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº† EvalScopeï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œï¼š
```bibtex
@misc{evalscope_2024
    title={{EvalScope}: Evaluation Framework for Large Models}
    author={ModelScope Team}
    year={2024}
    url={https://github.com/modelscope/evalscope}
}
```


## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/evalscope&type=Date)](https://star-history.com/#modelscope/evalscope&Date)

# Arena Mode

Arena mode allows you to configure multiple candidate models and specify a baseline model. The evaluation is conducted through pairwise battles between each candidate model and the baseline model with the win rate and ranking of each model outputted at the end. This approach is suitable for comparative evaluation among multiple models and intuitively reflects the strengths and weaknesses of each model.

## Data Preparation

To support arena mode **all candidate models need to run inference on the same dataset**. The dataset can be a general QA dataset or a domain-specific one. Below is an example using a custom `general_qa` dataset. See the [documentation](../advanced_guides/custom_dataset/llm.md#question-answering-format-qa) for details on using this dataset.

The JSONL file for the `general_qa` dataset should be in the following format. Only the `query` field is required; no additional fields are necessary. Below are two example files:

- Example content of the `arena.jsonl` file:
    ```json
    {"query": "How can I improve my time management skills?"}
    {"query": "What are the most effective ways to deal with stress?"}
    {"query": "What are the main differences between Python and JavaScript programming languages?"}
    {"query": "How can I increase my productivity while working from home?"}
    {"query": "Can you explain the basics of quantum computing?"}
    ```

- Example content of the `example.jsonl` file (with reference answers):
    ```json
    {"query": "What is the capital of France?" "response": "The capital of France is Paris."}
    {"query": "What is the largest mammal in the world?" "response": "The largest mammal in the world is the blue whale."}
    {"query": "How does photosynthesis work?" "response": "Photosynthesis is the process by which green plants use sunlight to synthesize foods with the help of chlorophyll."}
    {"query": "What is the theory of relativity?" "response": "The theory of relativity developed by Albert Einstein describes the laws of physics in relation to observers in different frames of reference."}
    {"query": "Who wrote 'To Kill a Mockingbird'?" "response": "Harper Lee wrote 'To Kill a Mockingbird'."}
    ```

## Candidate Model Inference

After preparing the dataset you can use EvalScope's `run_task` method to perform inference with the candidate models and obtain their outputs for subsequent battles.

Below is an example of how to configure inference tasks for three candidate models: `Qwen2.5-0.5B-Instruct` `Qwen2.5-7B-Instruct` and `Qwen2.5-72B-Instruct` using the same configuration for inference.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task
from evalscope.constants import EvalType

models = ['qwen2.5-72b-instruct' 'qwen2.5-7b-instruct' 'qwen2.5-0.5b-instruct']

task_list = [TaskConfig(
    model=model
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=os.getenv('DASHSCOPE_API_KEY')
    eval_type=EvalType.SERVICE
    datasets=[
        'general_qa'
    ]
    dataset_args={
        'general_qa': {
            'dataset_id': 'custom_eval/text/qa'
            'subset_list': [
                'arena'
                'example'
            ]
        }
    }
    eval_batch_size=10
    generation_config={
        'temperature': 0
        'n': 1
        'max_tokens': 4096
    }) for model in models]

run_task(task_cfg=task_list)
```

<details><summary>Click to view inference results</summary>

Since the `arena` subset does not have reference answers no evaluation metrics are available for this subset. The `example` subset has reference answers so evaluation metrics will be output.
```text
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| Model                 | Dataset    | Metric          | Subset   |   Num |   Score | Cat.0   |
+=======================+============+=================+==========+=======+=========+=========+
| qwen2.5-0.5b-instruct | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-P       | example  |    12 |  0.1341 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-1-F       | example  |    12 |  0.1983 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-R       | example  |    12 |  0.55   | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-P       | example  |    12 |  0.0404 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-2-F       | example  |    12 |  0.0716 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-R       | example  |    12 |  0.8611 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-P       | example  |    12 |  0.1193 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | Rouge-L-F       | example  |    12 |  0.1754 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-1          | example  |    12 |  0.1192 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-2          | example  |    12 |  0.0403 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-3          | example  |    12 |  0.0135 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-0.5b-instruct | general_qa | bleu-4          | example  |    12 |  0.0079 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-P       | example  |    12 |  0.1149 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-1-F       | example  |    12 |  0.1612 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-R       | example  |    12 |  0.6833 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-P       | example  |    12 |  0.0813 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-2-F       | example  |    12 |  0.1027 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-P       | example  |    12 |  0.101  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | Rouge-L-F       | example  |    12 |  0.1361 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-1          | example  |    12 |  0.1009 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-2          | example  |    12 |  0.0807 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-72b-instruct  | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | AverageAccuracy | arena    |    10 | -1      | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-P       | example  |    12 |  0.104  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-1-F       | example  |    12 |  0.1418 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-R       | example  |    12 |  0.7    | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-P       | example  |    12 |  0.078  | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-2-F       | example  |    12 |  0.0964 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-R       | example  |    12 |  0.9722 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-P       | example  |    12 |  0.0942 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | Rouge-L-F       | example  |    12 |  0.1235 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-1          | example  |    12 |  0.0939 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-2          | example  |    12 |  0.0777 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-3          | example  |    12 |  0.0625 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
| qwen2.5-7b-instruct   | general_qa | bleu-4          | example  |    12 |  0.0556 | default |
+-----------------------+------------+-----------------+----------+-------+---------+---------+
```
</details>

## Candidate Model Battles

Next you can use EvalScope's `general_arena` method to conduct battles among candidate models and get their win rates and rankings on each subset. To achieve robust automatic battles you need to configure an LLM as the judge that compares the outputs of models.

During evaluation EvalScope will automatically parse the public evaluation set of candidate models use the judge model to compare the output of each candidate model with the baseline and determine which is better (to avoid model bias outputs are swapped for two rounds per comparison). The judge model's outputs are parsed as win draw or loss and each candidate model's **Elo score** and **win rate** are calculated.

Run the following code:
```python
import os
from evalscope import TaskConfig run_task

task_cfg = TaskConfig(
    model_id='Arena'  # Model ID is 'Arena'; you can omit specifying model ID
    datasets=[
        'general_arena'  # Must be 'general_arena' indicating arena mode
    ]
    dataset_args={
        'general_arena': {
            # 'system_prompt': 'xxx' # Optional: customize the judge model's system prompt here
            # 'prompt_template': 'xxx' # Optional: customize the judge model's prompt template here
            'extra_params':{
                # Configure candidate model names and corresponding report paths
                # Report paths refer to the output paths from the previous step for parsing model inference results
                'models':[
                    {
                        'name': 'qwen2.5-0.5b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-0.5b-instruct'
                    }
                    {
                        'name': 'qwen2.5-7b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-7b-instruct'
                    }
                    {
                        'name': 'qwen2.5-72b'
                        'report_path': 'outputs/20250702_204346/reports/qwen2.5-72b-instruct'
                    }
                ]
                # Set baseline model must be one of the candidate models
                'baseline': 'qwen2.5-7b'
            }
        }
    }
    # Configure judge model parameters
    judge_model_args={
        'model_id': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': os.getenv('DASHSCOPE_API_KEY')
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 8000
        }
    }
    judge_worker_num=5
    # use_cache='outputs/xxx' # Optional: to add new candidate models to existing results specify the existing results path
)

run_task(task_cfg=task_cfg)
```

<details><summary>Click to view evaluation results</summary>

```text
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Model   | Dataset       | Metric        | Subset                                     |   Num |   Score | Cat.0   |
+=========+===============+===============+============================================+=======+=========+=========+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.5469 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.075  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.8382 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate       | OVERALL                                    |    44 |  0.3617 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0185 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.3906 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.025  | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.7276 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_lower | OVERALL                                    |    44 |  0.2826 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-0.5b&qwen2.5-7b |    12 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&example@qwen2.5-72b&qwen2.5-7b  |    12 |  0.6875 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-0.5b&qwen2.5-7b   |    10 |  0.0909 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | general_qa&arena@qwen2.5-72b&qwen2.5-7b    |    10 |  0.9412 | default |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+
| Arena   | general_arena | winrate_upper | OVERALL                                    |    44 |  0.4469 | -       |
+---------+---------------+---------------+--------------------------------------------+-------+---------+---------+ 
```
</details>


The automatically generated model leaderboard is as follows (output file located in `outputs/xxx/reports/Arena/leaderboard.txt`):

The leaderboard is sorted by win rate in descending order. As shown the `qwen2.5-72b` model performs best across all subsets with the highest win rate while the `qwen2.5-0.5b` model performs the worst.

```text
=== OVERALL LEADERBOARD ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== DATASET LEADERBOARD: general_qa ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            69.3  (-13.3 / +12.2)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            4.7  (-2.5 / +4.4)

=== SUBSET LEADERBOARD: general_qa - example ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            54.7  (-15.6 / +14.1)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            1.8  (+0.0 / +7.2)

=== SUBSET LEADERBOARD: general_qa - arena ===
Model           WinRate (%)  CI (%)
------------  -------------  ---------------
qwen2.5-72b            83.8  (-11.1 / +10.3)
qwen2.5-7b             50    (+0.0 / +0.0)
qwen2.5-0.5b            7.5  (-5.0 / +1.6)
```

## Visualization of Battle Results

To intuitively display the results of the battles between candidate models and the baseline EvalScope provides a visualization feature allowing you to compare the results of each candidate model against the baseline model for each sample.

Run the command below to launch the visualization interface:
```shell
evalscope app
```
Open `http://localhost:7860` in your browser to view the visualization page.

Workflow:
1. Select the latest `general_arena` evaluation report and click the "Load and View" button.
2. Click dataset details and select the battle results between your candidate model and the baseline.
3. Adjust the threshold to filter battle results (normalized scores range from 0-1; 0.5 indicates a tie scores above 0.5 indicate the candidate is better than the baseline below 0.5 means worse).

Example below: a battle between `qwen2.5-72b` and `qwen2.5-7b`. The model judged the 72b as better:

![image](https://sail-moe.oss-cn-hangzhou.aliyuncs.com/yunlin/images/evalscope/doc/arena_example.jpg)


# Sandbox Environment Usage

To complete LLM code capability evaluation we need to set up an independent evaluation environment to avoid executing erroneous code in the development environment and causing unavoidable losses. Currently EvalScope has integrated the [ms-enclave](https://github.com/modelscope/ms-enclave) sandbox environment allowing users to evaluate model code capabilities in a controlled environment such as using evaluation benchmarks like HumanEval and LiveCodeBench.

The following introduces two different sandbox usage methods:

- Local usage: Set up the sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine;
- Remote usage: Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

## 1. Local Usage

Use Docker to set up a sandbox environment on a local machine and conduct evaluation locally requiring Docker support on the local machine.

### Environment Setup

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in your local Python environment:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration
When running evaluations add the `use_sandbox` and `sandbox_type` parameters to automatically enable the sandbox environment. Other parameters remain the same as regular evaluations:

Here's a complete example code for model evaluation on HumanEval:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will automatically start and manage the sandbox environment ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] Local sandbox manager started
...
```

## 2. Remote Usage

Set up the sandbox environment on a remote server and conduct evaluation through API interfaces requiring Docker support on the remote machine.

### Environment Setup

You need to install and configure separately on both the remote machine and local machine.

#### Remote Machine

The environment installation on the remote machine is similar to the local usage method described above:

1. **Install Docker**: Please ensure Docker is installed on your machine. You can download and install Docker from the [Docker official website](https://www.docker.com/get-started).

2. **Install sandbox environment dependencies**: Install packages like `ms-enclave` in remote Python environment:

```bash
pip install evalscope[sandbox]
```

3. **Start sandbox server**: Run the following command to start the sandbox server:

```bash
ms-enclave server --host 0.0.0.0 --port 1234
```

#### Local Machine

The local machine does not need Docker installation at this point but needs to install EvalScope:

```bash
pip install evalscope[sandbox]
```

### Parameter Configuration

When running evaluations add the `use_sandbox` parameter to automatically enable the sandbox environment and specify the remote sandbox server's API address in `sandbox_manager_config`:

Complete example code is as follows:
```python
from dotenv import dotenv_values
env = dotenv_values('.env')
from evalscope import TaskConfig run_task

task_config = TaskConfig(
    model='qwen-plus'
    datasets=['humaneval']
    api_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
    api_key=env.get('DASHSCOPE_API_KEY')
    eval_type='openai_api'
    eval_batch_size=5
    limit=5
    generation_config={
        'max_tokens': 4096
        'temperature': 0.0
        'seed': 42
    }
    use_sandbox=True # enable sandbox
    sandbox_type='docker' # specify sandbox type
    sandbox_manager_config={
        'base_url': 'http://<remote_host>:1234'  # remote sandbox manager URL
    }
    judge_worker_num=5 # specify number of sandbox workers during evaluation
)

run_task(task_config)
```

During model evaluation EvalScope will communicate with the remote sandbox server through API ensuring code runs in an isolated environment. The console will display output like:
```text
[INFO:ms_enclave] HTTP sandbox manager started connected to http://<remote_host>:1234
...
```


# EvalScope Service Deployment

## Introduction

EvalScope service mode provides HTTP API-based evaluation and stress testing capabilities designed to address the following scenarios:

1. **Remote Invocation**: Support remote evaluation functionality through network without configuring complex evaluation environments locally
2. **Service Integration**: Easily integrate evaluation capabilities into existing workflows CI/CD pipelines or automated testing systems
3. **Multi-user Collaboration**: Support multiple users or systems calling the evaluation service simultaneously improving resource utilization
4. **Unified Management**: Centrally manage evaluation resources and configurations for easier maintenance and monitoring
5. **Flexible Deployment**: Can be deployed on dedicated servers or container environments decoupled from business systems

The Flask service encapsulates EvalScope's core evaluation (eval) and stress testing (perf) functionalities providing services through standard RESTful APIs making evaluation capabilities callable and integrable like other microservices.

## Features

- **Model Evaluation** (`/api/v1/eval`): Support evaluation of OpenAI API-compatible models
- **Performance Testing** (`/api/v1/perf`): Support performance benchmarking of OpenAI API-compatible models
- **Parameter Query**: Provide parameter description endpoints

## Environment Setup


### Full Installation (Recommended)

```bash
pip install evalscope[service]
```

### Development Environment Installation

```bash
# Clone repository
git clone https://github.com/modelscope/evalscope.git
cd evalscope

# Install development version with service
pip install -e '.[service]'
```

## Starting the Service

### Command Line Launch

```bash
# Use default configuration (host: 0.0.0.0 port: 9000)
evalscope service

# Custom host and port
evalscope service --host 127.0.0.1 --port 9000

# Enable debug mode
evalscope service --debug
```

### Python Code Launch

```python
from evalscope.service import run_service

# Start service
run_service(host='0.0.0.0' port=9000 debug=False)
```

## API Endpoints

### 1. Health Check

```bash
GET /health
```

**Response Example:**
```json
{
  "status": "ok"
  "service": "evalscope"
  "timestamp": "2025-12-04T10:00:00"
}
```

### 2. Model Evaluation

```bash
POST /api/v1/eval
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
  "api_key": "your-api-key"
  "datasets": ["gsm8k" "iquiz"]
  "limit": 10
  "generation_config": {
    "temperature": 0.0
    "max_tokens": 2048
  }
}
```

**Required Parameters:**
- `model`: Model name
- `datasets`: List of datasets
- `api_url`: API endpoint URL (OpenAI-compatible)

**Optional Parameters:**
- `api_key`: API key (default: "EMPTY")
- `limit`: Evaluation sample quantity limit
- `eval_batch_size`: Batch size (default: 1)
- `generation_config`: Generation configuration
  - `temperature`: Temperature parameter (default: 0.0)
  - `max_tokens`: Maximum generation tokens (default: 2048)
  - `top_p`: Nucleus sampling parameter
  - `top_k`: Top-k sampling parameter
- `work_dir`: Output directory
- `debug`: Debug mode
- `seed`: Random seed (default: 42)

**Response Example:**
```json
{
  "status": "success"
  "message": "Evaluation completed"
  "result": {"...": "..."}
  "output_dir": "/path/to/outputs/20251204_100000"
}
```

### 3. Performance Testing

```bash
POST /api/v1/perf
```

**Request Body Example:**
```json
{
  "model": "qwen-plus"
  "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
  "api": "openai"
  "api_key": "your-api-key"
  "number": 100
  "parallel": 10
  "dataset": "openqa"
  "max_tokens": 2048
  "temperature": 0.0
}
```

**Required Parameters:**
- `model`: Model name
- `url`: Complete API endpoint URL

**Optional Parameters:**
- `api`: API type (openai/dashscope/anthropic/gemini default: "openai")
- `api_key`: API key
- `number`: Total number of requests (default: 1000)
- `parallel`: Concurrency level (default: 1)
- `rate`: Requests per second limit (default: -1 unlimited)
- `dataset`: Dataset name (default: "openqa")
- `max_tokens`: Maximum generation tokens (default: 2048)
- `temperature`: Temperature parameter (default: 0.0)
- `stream`: Whether to use streaming output (default: true)
- `debug`: Debug mode

**Response Example:**
```json
{
  "status": "success"
  "message": "Performance test completed"
  "output_dir": "/path/to/outputs"
  "results": {
    "parallel_10_number_100": {
      "metrics": {"...": "..."}
      "percentiles": {"...": "..."}
    }
  }
}
```

### 4. Get Evaluation Parameter Description

```bash
GET /api/v1/eval/params
```

Returns descriptions of all parameters supported by the evaluation endpoint.

### 5. Get Performance Test Parameter Description

```bash
GET /api/v1/perf/params
```

Returns descriptions of all parameters supported by the performance test endpoint.

## Usage Examples

### Testing Evaluation Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "your-api-key"
    "datasets": ["gsm8k"]
    "limit": 5
  }'
```

### Testing Performance Endpoint with curl

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    "api": "openai"
    "number": 50
    "parallel": 5
  }'
```

### Using Python requests

```python
import requests

# Evaluation request
eval_response = requests.post(
    'http://localhost:9000/api/v1/eval'
    json={
        'model': 'qwen-plus'
        'api_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
        'api_key': 'your-api-key'
        'datasets': ['gsm8k' 'iquiz']
        'limit': 10
        'generation_config': {
            'temperature': 0.0
            'max_tokens': 2048
        }
    }
)
print(eval_response.json())

# Performance test request
perf_response = requests.post(
    'http://localhost:9000/api/v1/perf'
    json={
        'model': 'qwen-plus'
        'url': 'https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions'
        'api': 'openai'
        'number': 100
        'parallel': 10
        'dataset': 'openqa'
    }
)
print(perf_response.json())
```

## Important Notes

1. **OpenAI API-Compatible Models Only**: This service is designed specifically for OpenAI API-compatible models
2. **Long-Running Tasks**: Evaluation and performance testing tasks may take considerable time. We recommend setting appropriate HTTP timeout values on the client side as the API calls are synchronous and will block until completion.
3. **Output Directory**: Evaluation results are saved in the configured `work_dir` default is `outputs/`
4. **Error Handling**: The service returns detailed error messages and stack traces (in debug mode)
5. **Resource Management**: Pay attention to concurrency settings during stress testing to avoid server overload

## Error Codes

- `400`: Invalid request parameters
- `404`: Endpoint not found
- `500`: Internal server error

## Example Scenarios

### Scenario 1: Quick Evaluation of Qwen Model

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "api_key": "sk-..."
    "datasets": ["gsm8k"]
    "limit": 100
  }'
```

### Scenario 2: Stress Testing Locally Deployed Model

```bash
curl -X POST http://localhost:9000/api/v1/perf \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5"
    "url": "http://localhost:8000/v1/chat/completions"
    "api": "openai"
    "number": 1000
    "parallel": 20
    "max_tokens": 2048
  }'
```

### Scenario 3: Multi-Dataset Evaluation

```bash
curl -X POST http://localhost:9000/api/v1/eval \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen-plus"
    "api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"
    "datasets": ["gsm8k" "iquiz" "ceval"]
    "limit": 50
    "eval_batch_size": 4
  }'
```
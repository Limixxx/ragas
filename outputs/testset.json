[
    {
        "user_input":"Where Unionville mentioned in the paper?",
        "reference_contexts":[
            "Fine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n 1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n"
        ],
        "reference":"Unionville is mentioned in the affiliation of the paper's author, Sarah Ling, as part of her location: Markville Secondary School, Ontario, L3P7P5, Unionville, Canada.",
        "persona_name":"Arjun Mehta",
        "query_style":"POOR_GRAMMAR",
        "query_length":"SHORT",
        "synthesizer_name":"single_hop_specific_query_synthesizer"
    },
    {
        "user_input":"I’m Riya, NLP researcher that do sentiment classification tasks using preprocessed IMDB dataset and fine-tune transformer models like DistilBERT and LLaMa3, but where exactly in the provided context is the term ISSN mentioned and what’s the specific ISSN number given there, you know?",
        "reference_contexts":[
            "1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press"
        ],
        "reference":"The term ISSN is mentioned in the context in association with the Journal of Big Data and Computing, and the ISSN number provided is 2959-0590.",
        "persona_name":"Riya Patel",
        "query_style":"POOR_GRAMMAR",
        "query_length":"LONG",
        "synthesizer_name":"single_hop_specific_query_synthesizer"
    },
    {
        "user_input":"What details we have about LLaMa38B model—like who developed it, what parameter sizes it come in, what tasks it can perform well, and how we using it in our work comparing sentiment classification performance with DistilBERT model?",
        "reference_contexts":[
            "grammatical rules, semantics, and contextual\nnuances.\nThrough probabilistic methods, LLMs predict\nand generate coherent text without requiring\nhuman supervision during training. This enables\nthem to perform a variety of natural language\nprocessing tasks, such as summariza- tion,\ntranslation, and sentiment analysis. The\ntransformer-based approach significantly\nenhances their ability to process language\nefficiently and accurately, making LLMs a\npowerfultoolforadvancingAIapplications.\n 1.2 Models\nThe first model used is the Meta LLaMa 3 8B,\nwhich is the next generation of Meta’s state-of-\nthe-art open-source large language model[11].\nLlama3 comesin configurationsrangingfrom8\nbillion to 70 billion parameters, making it a\nhighly scalable and powerful model capable of\nprocessing large amounts of data for diverse\napplications[3]. It is designed to compete with\nand surpass existing models in terms of\nperformance across various tasks, including\nlanguage understanding, coding, reasoning, and\nmore.\nThe second model used is distilbert-base-\nuncasedwhichisadistilledversionoftheBERT\nbase model [8]. DistilBERT is a transformers\nmodel,smallerandfasterthanBERT,whichwas\npretrained on the same corpus in a self-\nsupervised fashion, using the BERT base model\nasateacher.Themodelwastrainedtoreturnthe\nsame probabilities as the BERT base model.\nDistilBERT has about 66 million parameters,\nwhich makes it smaller and more lightweight\nthantheoriginalBERTmodelandisdesignedto\nretain around 97% of BERT’s performance\nwhilebeing60%smallerandfaster.\nThe DistilBERT model was selected for this\ninvestigation since it is primarily used for tasks\nthat requiretransformermodelsbut requirefine-\ntuning. It has been widely used for fine-tuning\ntasks that use the whole sentence to make\ndecisions, such as sequence classification, token\nclassification,orquestionanswering.\nBoth models’ checkpoints hosted on\nhuggingfaceareusedfortheinference.\n 1.3 Fine-Tuning\nLanguage models are often further trained via a\nprocess named fine-tuning. Fine-tuning in\nmachine learning is the process of adapting a\npre-trainedmodelforspecifictasksorusecases.\nIt has become a fundamental deep learning\ntechnique, particularly in the training process of\nfoundation models used for generative AI. Fine-\ntuning for specific tasks such as interpreting\nquestions and generating responses, or\ntranslatingtextfromonelanguagetoanotherare\ncommon. In this investigation, we finetune the\ndistilbert-base-uncased and Llama 3 models and\ncompare their performance for the task of\nsentimentclassification.\n"
        ],
        "reference":"Meta developed the LLaMa38B model, which is the next generation of Meta’s state-of-the-art open-source large language model. It comes in configurations ranging from 8 billion to 70 billion parameters, making it highly scalable and powerful for processing large amounts of data for diverse applications including language understanding, coding, reasoning, and more. In the investigation, LLaMa38B is used along with distilbert-base-uncased for fine-tuning and comparing their performance on sentiment classification tasks, and its checkpoints hosted on huggingface are used for inference.",
        "persona_name":"Riya Patel",
        "query_style":"POOR_GRAMMAR",
        "query_length":"LONG",
        "synthesizer_name":"single_hop_specific_query_synthesizer"
    },
    {
        "user_input":"What we do to IMDB dataset before train our models?",
        "reference_contexts":[
            "2. System Design\n 2.1 Overview\nFigure 1. System Overview\nFigure1providesanoutlineofthestepstakento\nfinetuneamodel.\n 2.2 Data Pre-Processing\nThe firststage of our processinvolves preparing\nthe IMDB dataset for use with the transformer\nmodel. Since transformer-based models like\nDistilBERT require tokenized input, we\npreprocess the data by converting text into a\nformatthatthemodelcaninterpret.\nTokenization: Each movie review is tokenized\nusing a pre-trained tokenizer from the\nDistilBERT model. This tokenizer splits text\ninto subword units and converts them into\ninteger token IDs. The tokenizer also handles\npunctuation, case normalization (lowercasing),\nand truncation to ensure that the input sequence\nfitsthemodel’smaximuminputlength.\nPadding:Toensureuniforminputlengthsduring\nbatching, tokenized sequences are padded.\nPaddingaddsspecialtokenstoshortersequences\nso that they match the maximum sequence\nlengthineachbatch.Oncetokenizedandpadded,\nthe preprocessed dataset is ready for input into\nthemodel.\n3. Model Definition\nThe distilBERT model is initialized with\nweightspre-trainedonalargecorpusoftextdata,\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n109\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com"
        ],
        "reference":"We preprocess the IMDB dataset by tokenizing each movie review using DistilBERT's pre-trained tokenizer—splitting text into subword units, converting to integer token IDs, handling punctuation, lowercasing, truncating to fit the model's maximum input length—then padding tokenized sequences to uniform lengths for batching.",
        "persona_name":"Riya Patel",
        "query_style":"POOR_GRAMMAR",
        "query_length":"SHORT",
        "synthesizer_name":"single_hop_specific_query_synthesizer"
    },
    {
        "user_input":"In the binary sentiment classification task using the IMDB dataset, which model—DistilBERT-base-uncased or Meta-Llama-3-8B—had higher accuracy, and what was its accuracy score?",
        "reference_contexts":[
            "<1-hop>\n\nFine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n 1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n",
            "<2-hop>\n\n1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press",
            "<3-hop>\n\nThe evaluation of the model’s performance was\nbased on standard metrics including precision,\nrecall, and F1-score. These metrics were\ncomputed for each sentiment class (positive and\nnegative) as well as for the overall model\nperformance. The formulas for these metrics are\nasfollows.\nTP+TN\nTP+TN+FN+FP\nTP\nTP+FP\nTP\nTP+FN\nPrecision×Recall\nPrecision+Recall\n Table 1. Comparison of Model Performance\nModel Accuracy Precision Recall F1Score\ndistilBERT-base-\nuncased\nMeta-Llama-3-8B\n0.88\n0.66\n0.86075\n0.63793\n0.90666\n0.74000\n0.88311\n0.68518\n 4.2 Latency\nDistilBERT-base-uncased is much faster, with\nlow latency suitable for simple real-time\napplications such as sentiment classification due\nto its smaller size and efficient architecture. For\nthis task of sentiment classification, it took\napproximately2:23mintoruneachepoch.\nMeta-Llama-3-8B provides more powerful\ncapabilities and nuanced understanding but\ncomeswithhigher\nlatency, making it more suitable for applications\nwhere processingspeed is less critical compared\ntooutputquality.\n 5. Discussion\nThis study compared the performance of\nDistilBERT and LLaMA 3 on sentiment\nclassification using the IMDB dataset. Our\nfindings indicate that both models have their\nstrengths and weaknesses, and understanding\nthese can provide insights into optimizing\nsentiment analysis systems in practical\napplications.\nModel Performance:The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset.\nGeneralization Across Domains: The models\nperformed well on the IMDB dataset but may\nstruggle to generalize to different domains,\nwhere sentiment expressions vary significantly.\nImplementingdomainadaptationtechniquesand\nfine-tuning on diverse datasets could enhance\nmodel robustness. Highly polar movie reviews\nwere used, and both models’ ability to detect\nnuanced sentiments was not tested. Future work\ncould explore specialized training datasets to\nimprovetherecognitionofsubtlesentiments.\nPromising avenues for future research include\ndeveloping hybrid models that combine the\nstrengths of both LLaMA 3 and DistilBERT.\nEnhancing training techniques, such as few-shot\nand zero-shot learning, will also be essential for\nimproving performance across diverse contexts.\nAddressing the limitations identified in this\nstudy will be crucial for creating more accurate\nand efficient sentiment classification systems\nthat effectively meet user needs. These findings\nunderscore the importance of model selection\nbasedonapplicationrequirements.\n",
            "<4-hop>\n\nFuture Work: Several avenues for future\nexplorationremain:\n• Leveraging hybrid models combining\nefficiencyandnuancedunderstanding[5].\n• Expanding experiments to include\ndomain-specific datasets to enhance\ngeneralization.\n• Investigating advanced fine-tuning\ntechniques, such as adapters and LoRA\nlayers,forfurtheroptimization[11].\n 6. Conclusion\nIn this study, we conduct an evaluation of\nsentiment classification by comparing the\nperformance of a large language model and a\nsmall language model. The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset. These findings suggest that model size\nalone does not guarantee better performance,\nemphasizing the importance of selecting the\nappropriate model for specific tasks. Future\nworkcouldexplore theinfluenceof datasetsize,\nfine-tuning strategies, and domain-specific\ntraining to further optimize sentiment\nclassificationmodels.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran,\nRonen Eldan, Johannes Gehrke, Eric\nHorvitz,EceKamar,PeterLee,YinTatLee,\nYuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence:\nEarlyexperimentswithgpt-4.arXivpreprint\narXiv:2303.12712(2023).\nAccuracy=\nRecall=\nPrecision=\nF1=2 ×\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n111\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com"
        ],
        "reference":"DistilBERT-base-uncased had a higher accuracy than Meta-Llama-3-8B in the binary sentiment classification task using the IMDB dataset, with an accuracy score of 0.88.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_abstract_query_synthesizer"
    },
    {
        "user_input":"What preprocessing and fine-tuning steps were applied to the DistilBERT model for IMDB sentiment classification, and which evaluation metrics including recall were used to assess its performance after fine-tuning?",
        "reference_contexts":[
            "<1-hop>\n\nallowing it to already understand general\nlanguage features. We then modify the pre-\ntrained model for the specific task of sentiment\nclassificationby:\nAdding a Classification Layer: The pre-trained\nDistilBERT model outputs a contextualized\nrepresentation for each token in the input\nsequence.A fully connectedlayer (classification\nhead) is added on top of the model, with two\noutputnodescorrespondingtothetwosentiment\nlabels: positive and negative.Load model: We\nload pre-trained DistilBERT model then add the\nclassificationlayerwith2outputlabels:positive,\nnegative.Label Mapping: A mapping between\nsentiment labels and numerical IDs is defined\n(e.g., 0 for negative and 1 for positive). This\nensures that predictions made by the model are\ninterpretable.\n 3.1 Finetuning\nThe fine-tuning process involves training the\npre-trained DistilBERT model on the IMDB\ndataset while updating its weights to specialize\nin sentiment analysis. Fine-tuning is a crucial\nstep because it allows the model to transfer its\ngeneral language understanding to the specific\ntask of classifying movie reviews. We set the\nseed to a random fixed number to ensure fair\ncomparison and reproductibility. A smaller\nsubset of 3000 reviews was created for training\nand testing for the distilBERT model. A subset\nof 100 reviews was used for the LLaMa 3 8B\nmodel due to constraints on available RAM for\nthe free version of Google Colab. This smaller\nsubset may affect the finetuning capabilities of\nthe LLaMa 3 8B model since it is not given as\nmuch training data as the distilBERT model.\nThisdifferenceintrainingdatawillbetakeninto\nconsideration for comparing the models’\nperformance.\nTraining: The model is trained using a\nsupervised learning approach. The tokenized\nIMDB dataset is split into training and\nevaluation sets. During training, the model\nlearns to minimize the classification error by\nadjusting its weights via backpropagation. A\nlearning rate scheduler and optimizer (AdamW)\nare used to ensure smooth convergence and\npreventoverfitting.\n 3.2 Training Parameters Were Configured\n• output_dir:Directorytosavethemodel.\n• num_train_epochs:Numberoftraining\nepochs,setto2.\n• per_device_train_batch_size:Batchsizeper\ndevice,setto16.•learning_rate:Setto2×\n10−5\n• weight_decay:Setto0.01\n• optimizer:PagedAdamWwith32-bit\nprecision.\nAfter fine-tuning, the model’s performance\nimproved significantly, achieving an overall\naccuracy of 0.88. This result demonstrates the\neffectiveness of the fine-tuning process. By the\nend of the fine-tuning stage, the model is well-\nadjusted to the task of sentiment classification\nand can be used to make predictions on unseen\nmoviereviews.\n",
            "<2-hop>\n\n4. Evaluation\nThis section presents the experimental results\nobtained from evaluating the model after fine-\ntuning. The perfor- mance was assessed using\nprecision, recall, and F1-score metrics for each\nsentimentclass(positiveandnegative)aswellas\ntheoverallaccuracyofthemodel.\n 4.1 Performance\nThe confusion matrixes below provides a\nbreakdown of the model’s predictions across all\nsentiment classes, as shown in Figures 2 and 3.\nThis analysis helps in identifying common\nmisclassifications and understanding the\nmodel’sstrengthsandweaknesses.\nFigure 2. DistilBERT-Base-Uncased\nFigure 3. LLaMa-3-8B\n110\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press",
            "<3-hop>\n\n2. System Design\n 2.1 Overview\nFigure 1. System Overview\nFigure1providesanoutlineofthestepstakento\nfinetuneamodel.\n 2.2 Data Pre-Processing\nThe firststage of our processinvolves preparing\nthe IMDB dataset for use with the transformer\nmodel. Since transformer-based models like\nDistilBERT require tokenized input, we\npreprocess the data by converting text into a\nformatthatthemodelcaninterpret.\nTokenization: Each movie review is tokenized\nusing a pre-trained tokenizer from the\nDistilBERT model. This tokenizer splits text\ninto subword units and converts them into\ninteger token IDs. The tokenizer also handles\npunctuation, case normalization (lowercasing),\nand truncation to ensure that the input sequence\nfitsthemodel’smaximuminputlength.\nPadding:Toensureuniforminputlengthsduring\nbatching, tokenized sequences are padded.\nPaddingaddsspecialtokenstoshortersequences\nso that they match the maximum sequence\nlengthineachbatch.Oncetokenizedandpadded,\nthe preprocessed dataset is ready for input into\nthemodel.\n3. Model Definition\nThe distilBERT model is initialized with\nweightspre-trainedonalargecorpusoftextdata,\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n109\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com"
        ],
        "reference":"For the DistilBERT model's sentiment classification task on the IMDB dataset: \n1. Data Preprocessing Steps: The dataset was preprocessed by tokenizing each movie review using a pre-trained DistilBERT tokenizer (splitting text into subword units, converting to integer token IDs, handling punctuation, case normalization to lowercase, and truncating sequences to fit the model’s maximum input length). Tokenized sequences were padded to ensure uniform lengths across batches.\n2. Fine-tuning Steps: \n   a. Model Modification: A classification layer (fully connected) with two output nodes (corresponding to positive and negative sentiment labels) was added on top of the pre-trained DistilBERT model.\n   b. Label Mapping: A mapping between sentiment labels and numerical IDs was defined (0 for negative, 1 for positive).\n   c. Dataset Subset: A subset of 3000 reviews was used for training and testing the DistilBERT model.\n   d. Training Parameters: The model was trained with 2 epochs, a per-device training batch size of 16, a learning rate of 2×10⁻⁵, weight decay of 0.01, and the PagedAdamW optimizer with 32-bit precision. It was trained using supervised learning to minimize classification error via backpropagation, with a learning rate scheduler and optimizer to prevent overfitting.\n3. Evaluation Metrics: After fine-tuning, the model’s performance was assessed using metrics including precision, recall, F1-score (for each sentiment class: positive and negative) and overall accuracy (which reached 0.88).",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_abstract_query_synthesizer"
    },
    {
        "user_input":"Which movie review dataset was used for few-shot training to compare DistilBERT and LLaMA3 models for sentiment classification, and how many reviews does this dataset contain?",
        "reference_contexts":[
            "<1-hop>\n\nFine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n 1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n",
            "<2-hop>\n\n1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press"
        ],
        "reference":"The movie review dataset used for few-shot training in comparing DistilBERT and LLaMA3 models for sentiment classification is the IMDB dataset. This dataset consists of a total of 50,000 movie reviews, with 25,000 highly polar reviews for training and another 25,000 for testing.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_abstract_query_synthesizer"
    },
    {
        "user_input":"How does fine-tuning help DistilBERT and Llama3 models perform binary sentiment classification of IMDB reviews, and what overall accuracy did the fine-tuned DistilBERT model achieve?",
        "reference_contexts":[
            "<1-hop>\n\nFine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n 1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n",
            "<2-hop>\n\n1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press",
            "<3-hop>\n\nallowing it to already understand general\nlanguage features. We then modify the pre-\ntrained model for the specific task of sentiment\nclassificationby:\nAdding a Classification Layer: The pre-trained\nDistilBERT model outputs a contextualized\nrepresentation for each token in the input\nsequence.A fully connectedlayer (classification\nhead) is added on top of the model, with two\noutputnodescorrespondingtothetwosentiment\nlabels: positive and negative.Load model: We\nload pre-trained DistilBERT model then add the\nclassificationlayerwith2outputlabels:positive,\nnegative.Label Mapping: A mapping between\nsentiment labels and numerical IDs is defined\n(e.g., 0 for negative and 1 for positive). This\nensures that predictions made by the model are\ninterpretable.\n 3.1 Finetuning\nThe fine-tuning process involves training the\npre-trained DistilBERT model on the IMDB\ndataset while updating its weights to specialize\nin sentiment analysis. Fine-tuning is a crucial\nstep because it allows the model to transfer its\ngeneral language understanding to the specific\ntask of classifying movie reviews. We set the\nseed to a random fixed number to ensure fair\ncomparison and reproductibility. A smaller\nsubset of 3000 reviews was created for training\nand testing for the distilBERT model. A subset\nof 100 reviews was used for the LLaMa 3 8B\nmodel due to constraints on available RAM for\nthe free version of Google Colab. This smaller\nsubset may affect the finetuning capabilities of\nthe LLaMa 3 8B model since it is not given as\nmuch training data as the distilBERT model.\nThisdifferenceintrainingdatawillbetakeninto\nconsideration for comparing the models’\nperformance.\nTraining: The model is trained using a\nsupervised learning approach. The tokenized\nIMDB dataset is split into training and\nevaluation sets. During training, the model\nlearns to minimize the classification error by\nadjusting its weights via backpropagation. A\nlearning rate scheduler and optimizer (AdamW)\nare used to ensure smooth convergence and\npreventoverfitting.\n 3.2 Training Parameters Were Configured\n• output_dir:Directorytosavethemodel.\n• num_train_epochs:Numberoftraining\nepochs,setto2.\n• per_device_train_batch_size:Batchsizeper\ndevice,setto16.•learning_rate:Setto2×\n10−5\n• weight_decay:Setto0.01\n• optimizer:PagedAdamWwith32-bit\nprecision.\nAfter fine-tuning, the model’s performance\nimproved significantly, achieving an overall\naccuracy of 0.88. This result demonstrates the\neffectiveness of the fine-tuning process. By the\nend of the fine-tuning stage, the model is well-\nadjusted to the task of sentiment classification\nand can be used to make predictions on unseen\nmoviereviews.\n",
            "<4-hop>\n\n4. Evaluation\nThis section presents the experimental results\nobtained from evaluating the model after fine-\ntuning. The perfor- mance was assessed using\nprecision, recall, and F1-score metrics for each\nsentimentclass(positiveandnegative)aswellas\ntheoverallaccuracyofthemodel.\n 4.1 Performance\nThe confusion matrixes below provides a\nbreakdown of the model’s predictions across all\nsentiment classes, as shown in Figures 2 and 3.\nThis analysis helps in identifying common\nmisclassifications and understanding the\nmodel’sstrengthsandweaknesses.\nFigure 2. DistilBERT-Base-Uncased\nFigure 3. LLaMa-3-8B\n110\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press",
            "<5-hop>\n\ngrammatical rules, semantics, and contextual\nnuances.\nThrough probabilistic methods, LLMs predict\nand generate coherent text without requiring\nhuman supervision during training. This enables\nthem to perform a variety of natural language\nprocessing tasks, such as summariza- tion,\ntranslation, and sentiment analysis. The\ntransformer-based approach significantly\nenhances their ability to process language\nefficiently and accurately, making LLMs a\npowerfultoolforadvancingAIapplications.\n 1.2 Models\nThe first model used is the Meta LLaMa 3 8B,\nwhich is the next generation of Meta’s state-of-\nthe-art open-source large language model[11].\nLlama3 comesin configurationsrangingfrom8\nbillion to 70 billion parameters, making it a\nhighly scalable and powerful model capable of\nprocessing large amounts of data for diverse\napplications[3]. It is designed to compete with\nand surpass existing models in terms of\nperformance across various tasks, including\nlanguage understanding, coding, reasoning, and\nmore.\nThe second model used is distilbert-base-\nuncasedwhichisadistilledversionoftheBERT\nbase model [8]. DistilBERT is a transformers\nmodel,smallerandfasterthanBERT,whichwas\npretrained on the same corpus in a self-\nsupervised fashion, using the BERT base model\nasateacher.Themodelwastrainedtoreturnthe\nsame probabilities as the BERT base model.\nDistilBERT has about 66 million parameters,\nwhich makes it smaller and more lightweight\nthantheoriginalBERTmodelandisdesignedto\nretain around 97% of BERT’s performance\nwhilebeing60%smallerandfaster.\nThe DistilBERT model was selected for this\ninvestigation since it is primarily used for tasks\nthat requiretransformermodelsbut requirefine-\ntuning. It has been widely used for fine-tuning\ntasks that use the whole sentence to make\ndecisions, such as sequence classification, token\nclassification,orquestionanswering.\nBoth models’ checkpoints hosted on\nhuggingfaceareusedfortheinference.\n 1.3 Fine-Tuning\nLanguage models are often further trained via a\nprocess named fine-tuning. Fine-tuning in\nmachine learning is the process of adapting a\npre-trainedmodelforspecifictasksorusecases.\nIt has become a fundamental deep learning\ntechnique, particularly in the training process of\nfoundation models used for generative AI. Fine-\ntuning for specific tasks such as interpreting\nquestions and generating responses, or\ntranslatingtextfromonelanguagetoanotherare\ncommon. In this investigation, we finetune the\ndistilbert-base-uncased and Llama 3 models and\ncompare their performance for the task of\nsentimentclassification.\n",
            "<6-hop>\n\n2. System Design\n 2.1 Overview\nFigure 1. System Overview\nFigure1providesanoutlineofthestepstakento\nfinetuneamodel.\n 2.2 Data Pre-Processing\nThe firststage of our processinvolves preparing\nthe IMDB dataset for use with the transformer\nmodel. Since transformer-based models like\nDistilBERT require tokenized input, we\npreprocess the data by converting text into a\nformatthatthemodelcaninterpret.\nTokenization: Each movie review is tokenized\nusing a pre-trained tokenizer from the\nDistilBERT model. This tokenizer splits text\ninto subword units and converts them into\ninteger token IDs. The tokenizer also handles\npunctuation, case normalization (lowercasing),\nand truncation to ensure that the input sequence\nfitsthemodel’smaximuminputlength.\nPadding:Toensureuniforminputlengthsduring\nbatching, tokenized sequences are padded.\nPaddingaddsspecialtokenstoshortersequences\nso that they match the maximum sequence\nlengthineachbatch.Oncetokenizedandpadded,\nthe preprocessed dataset is ready for input into\nthemodel.\n3. Model Definition\nThe distilBERT model is initialized with\nweightspre-trainedonalargecorpusoftextdata,\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n109\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com"
        ],
        "reference":"Fine-tuning adapts pre-trained DistilBERT and Llama3 models to the specific task of binary sentiment classification (classifying IMDB movie reviews as positive or negative). For this task, Llama3 8B excels in capturing complex sentiments while DistilBERT-base-uncased offers a more efficient solution for simpler tasks. The fine-tuned DistilBERT model achieved an overall accuracy of 0.88.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_abstract_query_synthesizer"
    },
    {
        "user_input":"What data preprocessing steps for the IMDB dataset are described in the work copyrighted by STEMM Institute Press?",
        "reference_contexts":[
            "<1-hop>\n\n1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press",
            "<2-hop>\n\n2. System Design\n 2.1 Overview\nFigure 1. System Overview\nFigure1providesanoutlineofthestepstakento\nfinetuneamodel.\n 2.2 Data Pre-Processing\nThe firststage of our processinvolves preparing\nthe IMDB dataset for use with the transformer\nmodel. Since transformer-based models like\nDistilBERT require tokenized input, we\npreprocess the data by converting text into a\nformatthatthemodelcaninterpret.\nTokenization: Each movie review is tokenized\nusing a pre-trained tokenizer from the\nDistilBERT model. This tokenizer splits text\ninto subword units and converts them into\ninteger token IDs. The tokenizer also handles\npunctuation, case normalization (lowercasing),\nand truncation to ensure that the input sequence\nfitsthemodel’smaximuminputlength.\nPadding:Toensureuniforminputlengthsduring\nbatching, tokenized sequences are padded.\nPaddingaddsspecialtokenstoshortersequences\nso that they match the maximum sequence\nlengthineachbatch.Oncetokenizedandpadded,\nthe preprocessed dataset is ready for input into\nthemodel.\n3. Model Definition\nThe distilBERT model is initialized with\nweightspre-trainedonalargecorpusoftextdata,\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n109\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com"
        ],
        "reference":"The work copyrighted by STEMM Institute Press details two key data preprocessing steps for the IMDB dataset: tokenization and padding. Tokenization uses a pre-trained DistilBERT tokenizer to split text into subword units, convert them into integer token IDs, handle punctuation, normalize text to lowercase, and truncate sequences to fit the model's maximum input length. Padding adds special tokens to shorter sequences to ensure uniform input lengths during batching, matching the maximum sequence length in each batch.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_specific_query_synthesizer"
    },
    {
        "user_input":"Why DistilBERT outperformed LLaMa3 on IMDB sentiment task even LLaMa3 excels at complex sentiments?",
        "reference_contexts":[
            "<1-hop>\n\nFuture Work: Several avenues for future\nexplorationremain:\n• Leveraging hybrid models combining\nefficiencyandnuancedunderstanding[5].\n• Expanding experiments to include\ndomain-specific datasets to enhance\ngeneralization.\n• Investigating advanced fine-tuning\ntechniques, such as adapters and LoRA\nlayers,forfurtheroptimization[11].\n 6. Conclusion\nIn this study, we conduct an evaluation of\nsentiment classification by comparing the\nperformance of a large language model and a\nsmall language model. The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset. These findings suggest that model size\nalone does not guarantee better performance,\nemphasizing the importance of selecting the\nappropriate model for specific tasks. Future\nworkcouldexplore theinfluenceof datasetsize,\nfine-tuning strategies, and domain-specific\ntraining to further optimize sentiment\nclassificationmodels.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran,\nRonen Eldan, Johannes Gehrke, Eric\nHorvitz,EceKamar,PeterLee,YinTatLee,\nYuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence:\nEarlyexperimentswithgpt-4.arXivpreprint\narXiv:2303.12712(2023).\nAccuracy=\nRecall=\nPrecision=\nF1=2 ×\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n111\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com",
            "<2-hop>\n\nFine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n 1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n"
        ],
        "reference":"DistilBERT outperformed LLaMa3 on the IMDB sentiment classification task because it had higher accuracy, precision, recall, and F1 score. Even though LLaMa3 excels in capturing complex sentiments, DistilBERT is a more efficient solution for simpler tasks like the IMDB sentiment task here.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_specific_query_synthesizer"
    },
    {
        "user_input":"Which model did better than Llama3 on IMDB sentiment classification task, and Llama3 have what parameter sizes?",
        "reference_contexts":[
            "<1-hop>\n\nFuture Work: Several avenues for future\nexplorationremain:\n• Leveraging hybrid models combining\nefficiencyandnuancedunderstanding[5].\n• Expanding experiments to include\ndomain-specific datasets to enhance\ngeneralization.\n• Investigating advanced fine-tuning\ntechniques, such as adapters and LoRA\nlayers,forfurtheroptimization[11].\n 6. Conclusion\nIn this study, we conduct an evaluation of\nsentiment classification by comparing the\nperformance of a large language model and a\nsmall language model. The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset. These findings suggest that model size\nalone does not guarantee better performance,\nemphasizing the importance of selecting the\nappropriate model for specific tasks. Future\nworkcouldexplore theinfluenceof datasetsize,\nfine-tuning strategies, and domain-specific\ntraining to further optimize sentiment\nclassificationmodels.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran,\nRonen Eldan, Johannes Gehrke, Eric\nHorvitz,EceKamar,PeterLee,YinTatLee,\nYuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence:\nEarlyexperimentswithgpt-4.arXivpreprint\narXiv:2303.12712(2023).\nAccuracy=\nRecall=\nPrecision=\nF1=2 ×\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n111\nCopyright @ STEMM Institute Press\nhttp:\/\/www.stemmpress.com",
            "<2-hop>\n\ngrammatical rules, semantics, and contextual\nnuances.\nThrough probabilistic methods, LLMs predict\nand generate coherent text without requiring\nhuman supervision during training. This enables\nthem to perform a variety of natural language\nprocessing tasks, such as summariza- tion,\ntranslation, and sentiment analysis. The\ntransformer-based approach significantly\nenhances their ability to process language\nefficiently and accurately, making LLMs a\npowerfultoolforadvancingAIapplications.\n 1.2 Models\nThe first model used is the Meta LLaMa 3 8B,\nwhich is the next generation of Meta’s state-of-\nthe-art open-source large language model[11].\nLlama3 comesin configurationsrangingfrom8\nbillion to 70 billion parameters, making it a\nhighly scalable and powerful model capable of\nprocessing large amounts of data for diverse\napplications[3]. It is designed to compete with\nand surpass existing models in terms of\nperformance across various tasks, including\nlanguage understanding, coding, reasoning, and\nmore.\nThe second model used is distilbert-base-\nuncasedwhichisadistilledversionoftheBERT\nbase model [8]. DistilBERT is a transformers\nmodel,smallerandfasterthanBERT,whichwas\npretrained on the same corpus in a self-\nsupervised fashion, using the BERT base model\nasateacher.Themodelwastrainedtoreturnthe\nsame probabilities as the BERT base model.\nDistilBERT has about 66 million parameters,\nwhich makes it smaller and more lightweight\nthantheoriginalBERTmodelandisdesignedto\nretain around 97% of BERT’s performance\nwhilebeing60%smallerandfaster.\nThe DistilBERT model was selected for this\ninvestigation since it is primarily used for tasks\nthat requiretransformermodelsbut requirefine-\ntuning. It has been widely used for fine-tuning\ntasks that use the whole sentence to make\ndecisions, such as sequence classification, token\nclassification,orquestionanswering.\nBoth models’ checkpoints hosted on\nhuggingfaceareusedfortheinference.\n 1.3 Fine-Tuning\nLanguage models are often further trained via a\nprocess named fine-tuning. Fine-tuning in\nmachine learning is the process of adapting a\npre-trainedmodelforspecifictasksorusecases.\nIt has become a fundamental deep learning\ntechnique, particularly in the training process of\nfoundation models used for generative AI. Fine-\ntuning for specific tasks such as interpreting\nquestions and generating responses, or\ntranslatingtextfromonelanguagetoanotherare\ncommon. In this investigation, we finetune the\ndistilbert-base-uncased and Llama 3 models and\ncompare their performance for the task of\nsentimentclassification.\n"
        ],
        "reference":"DistilBERT outperformed Llama3 on the IMDB dataset's sentiment classification task. Llama3 comes in parameter configurations ranging from 8 billion to 70 billion.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_specific_query_synthesizer"
    },
    {
        "user_input":"What distilBERT-base-uncased accuracy is, and why confusion matrix used for it?",
        "reference_contexts":[
            "<1-hop>\n\nThe evaluation of the model’s performance was\nbased on standard metrics including precision,\nrecall, and F1-score. These metrics were\ncomputed for each sentiment class (positive and\nnegative) as well as for the overall model\nperformance. The formulas for these metrics are\nasfollows.\nTP+TN\nTP+TN+FN+FP\nTP\nTP+FP\nTP\nTP+FN\nPrecision×Recall\nPrecision+Recall\n Table 1. Comparison of Model Performance\nModel Accuracy Precision Recall F1Score\ndistilBERT-base-\nuncased\nMeta-Llama-3-8B\n0.88\n0.66\n0.86075\n0.63793\n0.90666\n0.74000\n0.88311\n0.68518\n 4.2 Latency\nDistilBERT-base-uncased is much faster, with\nlow latency suitable for simple real-time\napplications such as sentiment classification due\nto its smaller size and efficient architecture. For\nthis task of sentiment classification, it took\napproximately2:23mintoruneachepoch.\nMeta-Llama-3-8B provides more powerful\ncapabilities and nuanced understanding but\ncomeswithhigher\nlatency, making it more suitable for applications\nwhere processingspeed is less critical compared\ntooutputquality.\n 5. Discussion\nThis study compared the performance of\nDistilBERT and LLaMA 3 on sentiment\nclassification using the IMDB dataset. Our\nfindings indicate that both models have their\nstrengths and weaknesses, and understanding\nthese can provide insights into optimizing\nsentiment analysis systems in practical\napplications.\nModel Performance:The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset.\nGeneralization Across Domains: The models\nperformed well on the IMDB dataset but may\nstruggle to generalize to different domains,\nwhere sentiment expressions vary significantly.\nImplementingdomainadaptationtechniquesand\nfine-tuning on diverse datasets could enhance\nmodel robustness. Highly polar movie reviews\nwere used, and both models’ ability to detect\nnuanced sentiments was not tested. Future work\ncould explore specialized training datasets to\nimprovetherecognitionofsubtlesentiments.\nPromising avenues for future research include\ndeveloping hybrid models that combine the\nstrengths of both LLaMA 3 and DistilBERT.\nEnhancing training techniques, such as few-shot\nand zero-shot learning, will also be essential for\nimproving performance across diverse contexts.\nAddressing the limitations identified in this\nstudy will be crucial for creating more accurate\nand efficient sentiment classification systems\nthat effectively meet user needs. These findings\nunderscore the importance of model selection\nbasedonapplicationrequirements.\n",
            "<2-hop>\n\n4. Evaluation\nThis section presents the experimental results\nobtained from evaluating the model after fine-\ntuning. The perfor- mance was assessed using\nprecision, recall, and F1-score metrics for each\nsentimentclass(positiveandnegative)aswellas\ntheoverallaccuracyofthemodel.\n 4.1 Performance\nThe confusion matrixes below provides a\nbreakdown of the model’s predictions across all\nsentiment classes, as shown in Figures 2 and 3.\nThis analysis helps in identifying common\nmisclassifications and understanding the\nmodel’sstrengthsandweaknesses.\nFigure 2. DistilBERT-Base-Uncased\nFigure 3. LLaMa-3-8B\n110\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp:\/\/www.stemmpress.com\nCopyright @ STEMM Institute Press"
        ],
        "reference":"distilBERT-base-uncased has an accuracy of 0.88. Confusion matrices were used for this model to provide a breakdown of its predictions across all sentiment classes, identify common misclassifications, and understand its strengths and weaknesses.",
        "persona_name":null,
        "query_style":null,
        "query_length":null,
        "synthesizer_name":"multi_hop_specific_query_synthesizer"
    }
]
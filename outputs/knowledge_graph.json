{
  "nodes": [
    {
      "id": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "properties": {
        "page_content": "Fine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
        "document_metadata": {
          "producer": "Adobe Acrobat Pro 10.0.0",
          "creator": "Adobe Acrobat Pro 10.0.0",
          "creationdate": "2025-01-14T22:56:27+08:00",
          "author": "",
          "comments": "",
          "company": "",
          "keywords": "",
          "moddate": "2025-01-14T23:00:11+08:00",
          "subject": "",
          "title": "",
          "trapped": "/False",
          "source": "doc/Fine-Tuning_distilBERT_for_Enhanced_Sentiment_Clas.pdf",
          "total_pages": 5,
          "page": 0,
          "page_label": "1"
        },
        "headlines": [
          "1. Introduction",
          "1.1 Background Dataset",
          "Large Language Models (LLMs)",
          "2. Methodology",
          "3. Results and Discussion"
        ],
        "summary": "This research fine-tunes the DistilBERT model for sentiment classification using the IMDB movie review dataset. It compares DistilBERT with the larger LLaMA 3 model on their ability to classify reviews as positive or negative. The study finds that while LLaMA 3 excels at capturing complex sentiments, DistilBERT offers a more efficient solution for simpler classification tasks. The results highlight the effectiveness of fine-tuning transformer models for this purpose. The paper contributes to optimizing sentiment analysis models and suggests future research directions like hybrid models.",
        "keyphrases": [
          "Fine-Tuning distilBERT",
          "Sentiment Classification",
          "Natural Language Processing",
          "Large Language Models",
          "IMDB dataset"
        ],
        "summary_embedding": [
          -0.06421570479869843,
          0.01199241355061531,
          -0.0281485877931118,
          -0.043522585183382034,
          -0.01709967851638794,
          -0.04326030611991882,
          -0.0626106858253479,
          0.028192471712827682,
          0.02942674048244953,
          -0.03073275089263916,
          0.12325315177440643,
          -0.3350124955177307,
          -0.02119228057563305,
          0.09120173752307892,
          -0.045540280640125275,
          -0.045877184718847275,
          -0.015077144838869572,
          -0.00637450348585844,
          0.05278943479061127,
          -0.009236876852810383,
          0.06593398004770279,
          -0.010865841992199421,
          0.03700193017721176,
          0.042406659573316574,
          -0.024610156193375587,
          0.018733056262135506,
          -0.016488362103700638,
          0.03069973737001419,
          0.007626369595527649,
          -0.0210268571972847,
          -0.06061122193932533,
          0.05405682325363159,
          0.03530396893620491,
          -0.0195210762321949,
          -0.02239612676203251,
          -0.02904757671058178,
          -0.004968908149749041,
          -0.06103211268782616,
          -0.02603350579738617,
          0.07942831516265869,
          0.03571612015366554,
          -0.0036751346196979284,
          0.0009114104905165732,
          0.026302188634872437,
          0.006594482343643904,
          -0.05752973258495331,
          -0.003556555602699518,
          0.016556698828935623,
          -0.026742853224277496,
          -0.016756318509578705,
          -0.0031740302219986916,
          0.04957131668925285,
          0.03990285471081734,
          0.02377544529736042,
          0.043335579335689545,
          -0.07180074602365494,
          0.049263205379247665,
          0.023502016440033913,
          -0.08119712024927139,
          -0.01067274995148182,
          0.07408061623573303,
          -0.03383314609527588,
          -0.03815614432096481,
          0.0410119853913784,
          -0.006618108134716749,
          0.023433899506926537,
          -0.026380307972431183,
          0.04564380273222923,
          -0.003939338959753513,
          0.00779834296554327,
          -0.061926670372486115,
          -0.011079962365329266,
          0.037781137973070145,
          -0.05785771459341049,
          0.03257916495203972,
          -0.06406496465206146,
          0.004047904163599014,
          -0.059983186423778534,
          0.013427535071969032,
          0.0123423608019948,
          -0.0477353073656559,
          -0.027008503675460815,
          -0.0359463170170784,
          0.09584776312112808,
          -0.03130260854959488,
          -0.028776289895176888,
          -0.005353474989533424,
          0.0022337944246828556,
          -0.01625645160675049,
          -0.09779386967420578,
          0.046248167753219604,
          -0.01883697509765625,
          0.02774110436439514,
          0.044444043189287186,
          -0.01820901781320572,
          0.03241470456123352,
          -0.08912339806556702,
          0.07483173906803131,
          0.011911621317267418,
          -0.013061597011983395,
          0.013014545664191246,
          -0.02022198773920536,
          -0.025777557864785194,
          -0.03259739279747009,
          -0.04602305218577385,
          0.054678745567798615,
          0.08498380333185196,
          -0.0416310578584671,
          0.03923613205552101,
          0.07364330440759659,
          -0.017211439087986946,
          -0.0139284897595644,
          0.017026592046022415,
          0.01762550324201584,
          0.006641826126724482,
          -0.055768389254808426,
          -0.00588319031521678,
          0.01531777624040842,
          0.006330450065433979,
          0.03876440227031708,
          0.09956289082765579,
          0.07351253181695938,
          -0.007385523524135351,
          -0.011147800832986832,
          -0.04847538471221924,
          -0.01812830939888954,
          0.0006544436328113079,
          -0.0011343719670549035,
          -0.02471059560775757,
          -0.01736298017203808,
          0.005022578872740269,
          -0.03488914296030998,
          0.04114001989364624,
          0.03483916446566582,
          0.024000629782676697,
          -0.006641846150159836,
          0.010542272590100765,
          -0.057421620935201645,
          -0.04755432531237602,
          -0.03032997064292431,
          0.04391102492809296,
          0.006536791101098061,
          -0.05366983264684677,
          -0.04858264699578285,
          0.002704574726521969,
          0.03545152395963669,
          -0.03967020660638809,
          0.021928096190094948,
          -0.02158385142683983,
          0.03566645830869675,
          0.02991274744272232,
          -0.026328187435865402,
          -0.006101046688854694,
          -0.029137352481484413,
          -0.03912845626473427,
          0.03750994801521301,
          -0.006542347837239504,
          -0.022764073684811592,
          -0.03700454160571098,
          0.00404204661026597,
          0.05291252210736275,
          -0.019603468477725983,
          0.00887951347976923,
          0.035792212933301926,
          -0.07094880938529968,
          0.0018563239136710763,
          0.045551199465990067,
          0.03381983935832977,
          -0.08081432431936264,
          -0.007640454452484846,
          0.02311966009438038,
          -0.053302690386772156,
          0.023977262899279594,
          -0.019310640171170235,
          -0.0032446826808154583,
          -0.0027866775635629892,
          -0.02008085697889328,
          -0.07170302420854568,
          -0.03174111992120743,
          -0.08912629634141922,
          0.02493830770254135,
          0.0005541050340980291,
          -0.006082137115299702,
          0.020719552412629128,
          0.01427795272320509,
          0.03325968235731125,
          -0.017478549852967262,
          -0.043263524770736694,
          0.04137640446424484,
          0.010570758022367954,
          0.06665581464767456,
          0.001404420705512166,
          0.013371232897043228,
          0.04157095029950142,
          0.05228840187191963,
          0.016108011826872826,
          0.04623401537537575,
          0.03031160868704319,
          0.01017857901751995,
          -0.021117916330695152,
          -0.04758675768971443,
          0.03702455759048462,
          0.09000780433416367,
          -0.009061736986041069,
          -5.448152296594344e-05,
          -0.04639371857047081,
          -0.014068030752241611,
          -0.06312167644500732,
          0.011872872710227966,
          -0.04528747498989105,
          0.013538901694118977,
          0.04038657248020172,
          -0.02484637312591076,
          0.006037557031959295,
          -0.029237743467092514,
          0.013579112477600574,
          0.009465223178267479,
          -0.03749525919556618,
          -0.03829050436615944,
          -0.013180666603147984,
          -0.032586101442575455,
          0.021947864443063736,
          0.0696287453174591,
          0.024496886879205704,
          -0.05108172073960304,
          -0.02306605502963066,
          -0.005390298552811146,
          0.020674850791692734,
          0.02208366058766842,
          0.039552126079797745,
          0.0046515376307070255,
          -0.04894188791513443,
          0.05363765358924866,
          0.0037615324836224318,
          0.0444234274327755,
          -0.0055196238681674,
          -0.03641677275300026,
          0.005454768892377615,
          0.05607007071375847,
          0.03347441181540489,
          -0.03324839100241661,
          0.11939576268196106,
          0.0512193962931633,
          -0.03626804053783417,
          -0.0355442613363266,
          -0.006594301667064428,
          -0.05599837377667427,
          0.06600101292133331,
          0.00675294129177928,
          -0.0019953062292188406,
          -0.03600786253809929,
          -0.040904078632593155,
          -0.015487661585211754,
          0.01205087173730135,
          -0.05540770664811134,
          0.04581230878829956,
          -0.029551761224865913,
          -0.00655303243547678,
          -0.035348039120435715,
          0.05003166198730469,
          -0.06456474214792252,
          0.043116163462400436,
          -0.026600942015647888,
          -0.03450016677379608,
          0.0362122543156147,
          0.003198472550138831,
          -0.040589507669210434,
          0.007942408323287964,
          -0.10497444123029709,
          0.03687826916575432,
          -0.006425123196095228,
          -0.044111937284469604,
          0.013884156942367554,
          -0.007183165289461613,
          0.01347437221556902,
          0.03391534090042114,
          -0.07387731224298477,
          0.04377100244164467,
          -0.006511301267892122,
          0.004841240588575602,
          -0.03296336904168129,
          0.023436306044459343,
          -0.020900847390294075,
          0.04732630401849747,
          -0.042636483907699585,
          -0.04353395476937294,
          -0.12098641693592072,
          0.006200313568115234,
          -0.029942462220788002,
          -0.007196076214313507,
          -0.0009613164002075791,
          -0.02128579653799534,
          -0.05922415852546692,
          0.04193549603223801,
          -0.03142354637384415,
          0.039448946714401245,
          -0.036824487149715424,
          -0.02705392986536026,
          -0.042499855160713196,
          0.008984103798866272,
          0.03530992195010185,
          0.02336394041776657,
          -0.05458081513643265,
          0.013563094660639763,
          0.04519510269165039,
          0.0007351826643571258,
          0.008695841766893864,
          -0.04968487098813057,
          -0.0007659217226319015,
          0.03325088694691658,
          -0.0012836749665439129,
          -0.018916163593530655,
          -0.011252657510340214,
          -0.016218464821577072,
          0.007621759548783302,
          0.0004209894686937332,
          -0.0037771654315292835,
          0.032296814024448395,
          -0.043218258768320084,
          -0.025440862402319908,
          -0.012714308686554432,
          0.07516380399465561,
          -0.08511366695165634,
          -0.03878125175833702,
          0.019640322774648666,
          0.022692514583468437,
          0.01072598248720169,
          -0.07696586847305298,
          0.01599353738129139,
          -0.0770007073879242,
          -0.03563647344708443,
          -0.009975191205739975,
          -0.019541842862963676,
          -0.018224628642201424,
          -0.03205999359488487,
          -0.0005796724581159651,
          -0.05585116147994995,
          -0.06729788333177567,
          0.028802970424294472,
          0.0017803482478484511,
          -0.05982941389083862,
          -0.0030605257488787174,
          0.011387036181986332,
          0.03375494107604027,
          -0.06492090225219727,
          0.027338266372680664,
          0.05767764523625374,
          0.06639262288808823,
          -0.02340123802423477,
          0.014463900588452816,
          -0.01504580955952406,
          -0.014821536839008331,
          -0.028590569272637367,
          -0.05332699418067932,
          0.0031887514051049948,
          -0.0077990698628127575,
          0.0005086155724711716,
          0.039355479180812836,
          0.023801719769835472,
          0.022312059998512268,
          -0.04743869975209236,
          0.030225073918700218,
          0.0694931149482727,
          0.05779895558953285,
          -0.028019357472658157,
          -0.08845705538988113,
          0.011434134095907211,
          0.007125528063625097,
          -0.01089647226035595,
          0.05526289716362953,
          0.017695249989628792,
          -0.01613258756697178,
          0.0073739904910326,
          0.02464989386498928,
          -0.041475288569927216,
          -0.013873632997274399,
          -0.09431201219558716,
          -0.07208355516195297,
          0.006377913523465395,
          -0.012139336206018925,
          -0.06291372328996658,
          0.05262693390250206,
          0.014901475049555302,
          0.09969408065080643,
          0.0843321681022644,
          -0.05561912804841995,
          -0.003190377028658986,
          -0.005256472155451775,
          -0.03402911126613617,
          0.0466083325445652,
          0.02279806323349476,
          -0.05682612210512161,
          0.01822713203728199,
          0.055183541029691696,
          -0.01984517276287079,
          -0.11561167985200882,
          0.047175753861665726,
          -0.0065497299656271935,
          -0.02636600285768509,
          -0.04110073670744896,
          -0.002959302393719554,
          -0.010452382266521454,
          -0.029982658103108406,
          0.09115002304315567,
          0.020388714969158173,
          0.02452745847404003,
          -0.05419094115495682,
          -0.01247477252036333,
          -0.030114956200122833,
          -0.01826586201786995,
          0.25604721903800964,
          -0.004103296902030706,
          0.019478172063827515,
          0.005732015706598759,
          -0.0057186949998140335,
          0.015778569504618645,
          -0.07166019827127457,
          0.020996849983930588,
          -0.008259275928139687,
          0.05746002122759819,
          0.01552156638354063,
          -0.03136879578232765,
          -0.0267450250685215,
          0.037760622799396515,
          -0.007397380191832781,
          -0.02659880556166172,
          -0.037786297500133514,
          0.01593027636408806,
          -0.08519980311393738,
          0.028261786326766014,
          0.04928981140255928,
          -0.006060638930648565,
          0.0562041774392128,
          -0.011114235036075115,
          0.014587691985070705,
          0.04507758840918541,
          -0.059350885450839996,
          0.042821627110242844,
          1.7213524188264273e-05,
          -0.00876445323228836,
          0.027468867599964142,
          0.011848382651805878,
          0.1192636489868164,
          0.01879308745265007,
          -0.023776564747095108,
          0.02184581197798252,
          -0.04441377520561218,
          -0.021460458636283875,
          0.06801922619342804,
          0.06327374279499054,
          -0.0005573087837547064,
          0.0339219868183136,
          -0.041364721953868866,
          0.006790670566260815,
          -0.01933790184557438,
          -0.013876395300030708,
          -0.015296008437871933,
          0.014818048104643822,
          0.08539225906133652,
          -0.0409809872508049,
          -0.005850919988006353,
          -0.04349605739116669,
          -0.02659672312438488,
          -0.06813324242830276,
          -0.04298814758658409,
          0.028693787753582,
          -0.018700141459703445,
          0.03278292715549469,
          -0.039556749165058136,
          -0.03856862336397171,
          -0.02948843687772751,
          -0.07114917039871216,
          0.04700634256005287,
          0.05582021549344063,
          -0.009557959623634815,
          -0.0015170230763033032,
          -0.044512711465358734,
          0.01699848659336567,
          -0.057972583919763565,
          -0.04622916132211685,
          0.046200115233659744,
          -0.005218783859163523,
          -0.00554295489564538,
          -0.01318593043833971,
          -0.011710766702890396,
          0.03763611242175102,
          0.022435981780290604,
          0.0025918155442923307,
          -0.05372244119644165,
          -0.03904008865356445,
          0.08702337741851807,
          0.06961037218570709,
          -0.003092252416536212,
          -0.011653787456452847,
          0.01594442129135132,
          -0.008886796422302723,
          -0.0074674938805401325,
          -0.03175445273518562,
          0.04776956886053085,
          -0.01741712912917137,
          -0.043556127697229385,
          0.026383018121123314,
          -0.023042267188429832,
          0.021249713376164436,
          -0.048597726970911026,
          -0.012750895693898201,
          0.07158088684082031,
          0.0044835591688752174,
          -0.008973700925707817,
          -0.017019618302583694,
          0.004598251543939114,
          -0.006584638264030218
        ]
      },
      "type": "document"
    },
    {
      "id": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "properties": {
        "page_content": "grammatical rules, semantics, and contextual\nnuances.\nThrough probabilistic methods, LLMs predict\nand generate coherent text without requiring\nhuman supervision during training. This enables\nthem to perform a variety of natural language\nprocessing tasks, such as summariza- tion,\ntranslation, and sentiment analysis. The\ntransformer-based approach significantly\nenhances their ability to process language\nefficiently and accurately, making LLMs a\npowerfultoolforadvancingAIapplications.\n1.2 Models\nThe first model used is the Meta LLaMa 3 8B,\nwhich is the next generation of Meta’s state-of-\nthe-art open-source large language model[11].\nLlama3 comesin configurationsrangingfrom8\nbillion to 70 billion parameters, making it a\nhighly scalable and powerful model capable of\nprocessing large amounts of data for diverse\napplications[3]. It is designed to compete with\nand surpass existing models in terms of\nperformance across various tasks, including\nlanguage understanding, coding, reasoning, and\nmore.\nThe second model used is distilbert-base-\nuncasedwhichisadistilledversionoftheBERT\nbase model [8]. DistilBERT is a transformers\nmodel,smallerandfasterthanBERT,whichwas\npretrained on the same corpus in a self-\nsupervised fashion, using the BERT base model\nasateacher.Themodelwastrainedtoreturnthe\nsame probabilities as the BERT base model.\nDistilBERT has about 66 million parameters,\nwhich makes it smaller and more lightweight\nthantheoriginalBERTmodelandisdesignedto\nretain around 97% of BERT’s performance\nwhilebeing60%smallerandfaster.\nThe DistilBERT model was selected for this\ninvestigation since it is primarily used for tasks\nthat requiretransformermodelsbut requirefine-\ntuning. It has been widely used for fine-tuning\ntasks that use the whole sentence to make\ndecisions, such as sequence classification, token\nclassification,orquestionanswering.\nBoth models’ checkpoints hosted on\nhuggingfaceareusedfortheinference.\n1.3 Fine-Tuning\nLanguage models are often further trained via a\nprocess named fine-tuning. Fine-tuning in\nmachine learning is the process of adapting a\npre-trainedmodelforspecifictasksorusecases.\nIt has become a fundamental deep learning\ntechnique, particularly in the training process of\nfoundation models used for generative AI. Fine-\ntuning for specific tasks such as interpreting\nquestions and generating responses, or\ntranslatingtextfromonelanguagetoanotherare\ncommon. In this investigation, we finetune the\ndistilbert-base-uncased and Llama 3 models and\ncompare their performance for the task of\nsentimentclassification.\n2. System Design\n2.1 Overview\nFigure 1. System Overview\nFigure1providesanoutlineofthestepstakento\nfinetuneamodel.\n2.2 Data Pre-Processing\nThe firststage of our processinvolves preparing\nthe IMDB dataset for use with the transformer\nmodel. Since transformer-based models like\nDistilBERT require tokenized input, we\npreprocess the data by converting text into a\nformatthatthemodelcaninterpret.\nTokenization: Each movie review is tokenized\nusing a pre-trained tokenizer from the\nDistilBERT model. This tokenizer splits text\ninto subword units and converts them into\ninteger token IDs. The tokenizer also handles\npunctuation, case normalization (lowercasing),\nand truncation to ensure that the input sequence\nfitsthemodel’smaximuminputlength.\nPadding:Toensureuniforminputlengthsduring\nbatching, tokenized sequences are padded.\nPaddingaddsspecialtokenstoshortersequences\nso that they match the maximum sequence\nlengthineachbatch.Oncetokenizedandpadded,\nthe preprocessed dataset is ready for input into\nthemodel.\n3. Model Definition\nThe distilBERT model is initialized with\nweightspre-trainedonalargecorpusoftextdata,\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n109\nCopyright @ STEMM Institute Press\nhttp://www.stemmpress.com",
        "document_metadata": {
          "producer": "Adobe Acrobat Pro 10.0.0",
          "creator": "Adobe Acrobat Pro 10.0.0",
          "creationdate": "2025-01-14T22:56:27+08:00",
          "author": "",
          "comments": "",
          "company": "",
          "keywords": "",
          "moddate": "2025-01-14T23:00:11+08:00",
          "subject": "",
          "title": "",
          "trapped": "/False",
          "source": "doc/Fine-Tuning_distilBERT_for_Enhanced_Sentiment_Clas.pdf",
          "total_pages": 5,
          "page": 1,
          "page_label": "2"
        },
        "headlines": [
          "1.2 Models",
          "1.3 Fine-Tuning",
          "2. System Design",
          "2.1 Overview",
          "2.2 Data Pre-Processing"
        ],
        "summary": "Large Language Models (LLMs) learn grammatical rules, semantics, and context through probabilistic methods to generate coherent text without human supervision. They perform tasks like summarization, translation, and sentiment analysis using a transformer-based architecture for efficient language processing. The investigation uses two models: Meta LLaMa 3 8B, a powerful open-source model scalable from 8 to 70 billion parameters, and DistilBERT, a smaller, faster distilled version of BERT with about 66 million parameters. DistilBERT retains most of BERT's performance while being more lightweight and is selected for tasks requiring fine-tuning. Both models are fine-tuned, a process of adapting pre-trained models for specific tasks like sentiment classification. The system design involves data pre-processing of the IMDB dataset, including tokenization and padding to prepare inputs for the transformer model. Tokenization converts text into integer token IDs using DistilBERT's tokenizer, handling normalization and truncation. Padding ensures uniform input lengths for batching. The fine-tuned models are then compared for performance on the sentiment classification task.",
        "keyphrases": [
          "large language models",
          "fine-tuning",
          "DistilBERT",
          "Llama 3",
          "sentiment classification"
        ],
        "summary_embedding": [
          -0.04163885861635208,
          0.0172867588698864,
          0.02700159139931202,
          -0.019560258835554123,
          -0.010056556202471256,
          -0.058625511825084686,
          -0.061768654733896255,
          -0.02165084518492222,
          -0.004100858699530363,
          -0.024625511839985847,
          0.1155354231595993,
          -0.34276875853538513,
          0.01639309711754322,
          0.06751132011413574,
          -0.021567581221461296,
          -0.018504882231354713,
          0.03626975417137146,
          0.029643703252077103,
          0.03741394728422165,
          -0.029344592243433,
          0.07261413335800171,
          -0.01377340592443943,
          -0.0014995805686339736,
          0.009001017548143864,
          0.018822692334651947,
          0.013199123553931713,
          -0.004696174990385771,
          0.01595919020473957,
          -0.05927939713001251,
          -0.0717209056019783,
          0.001585750957019627,
          0.03365202620625496,
          0.02270607091486454,
          -0.043760642409324646,
          -0.037355635315179825,
          -0.05493057519197464,
          -0.017606237903237343,
          -0.05200137943029404,
          -0.006632275879383087,
          0.06034824997186661,
          0.040922459214925766,
          0.01565656065940857,
          0.01117145549505949,
          -0.03726864978671074,
          -0.05275348573923111,
          -0.08283346891403198,
          -0.015933599323034286,
          0.012193188071250916,
          -0.037808485329151154,
          -0.018682196736335754,
          0.0037546628154814243,
          0.07232433557510376,
          0.060628343373537064,
          0.04564696177840233,
          0.06086507812142372,
          -0.04602762684226036,
          0.04451166093349457,
          0.005090944468975067,
          -0.08443135023117065,
          0.021059367805719376,
          0.055062539875507355,
          0.0024190086405724287,
          -0.0034208158031105995,
          0.009851629845798016,
          -0.012645021080970764,
          -0.009022984653711319,
          -0.01392767857760191,
          0.0719563364982605,
          -0.016508201137185097,
          -0.0022328393533825874,
          -0.02059868536889553,
          0.017880626022815704,
          -0.04148827865719795,
          -0.04916868731379509,
          0.032160017639398575,
          -0.03105572797358036,
          -0.029838696122169495,
          -0.017895396798849106,
          -0.037519942969083786,
          0.020297862589359283,
          -0.04630723223090172,
          -0.014578452333807945,
          -0.0030285620596259832,
          0.044273559004068375,
          -0.025397364050149918,
          -0.06404741108417511,
          0.0384647436439991,
          -0.02483416721224785,
          -0.005416318774223328,
          -0.029384585097432137,
          0.00944566074758768,
          -0.025572700425982475,
          0.04270351305603981,
          0.08677390217781067,
          -0.02746519073843956,
          0.034917574375867844,
          -0.1072017177939415,
          0.05888403207063675,
          -0.005783627275377512,
          0.002502043964341283,
          0.023919451981782913,
          -0.06289415806531906,
          0.024786831811070442,
          -0.03078785166144371,
          -0.04397746920585632,
          0.04951150715351105,
          0.0661633238196373,
          -0.05423780158162117,
          0.057562410831451416,
          -0.004822778515517712,
          0.03514600545167923,
          -0.061011701822280884,
          0.04349730163812637,
          0.00020047684665769339,
          -0.030204614624381065,
          -0.05241803079843521,
          0.02929387427866459,
          0.012638995423913002,
          0.00936397910118103,
          0.051368605345487595,
          0.07273437827825546,
          0.05702528357505798,
          -0.01970227248966694,
          -0.01696624606847763,
          -0.03372326120734215,
          -0.05971492826938629,
          -0.023135263472795486,
          -0.006516421679407358,
          -0.019396182149648666,
          -0.0057525308802723885,
          0.016184674575924873,
          -0.03188953548669815,
          0.040929634124040604,
          0.010608837939798832,
          -0.01662464439868927,
          -0.003221432911232114,
          -0.0005689946701750159,
          -0.004530196078121662,
          -0.01920102909207344,
          -0.06080514192581177,
          0.07375438511371613,
          -0.030144521966576576,
          -0.030921364203095436,
          -0.05692119896411896,
          0.02497454173862934,
          0.017526842653751373,
          -0.011028993874788284,
          0.0026404631789773703,
          -0.03690285235643387,
          0.019986864179372787,
          0.011932379566133022,
          -0.048157598823308945,
          -0.023032281547784805,
          -0.05501275137066841,
          -0.045645490288734436,
          0.05926833301782608,
          -0.023772522807121277,
          -0.008817557245492935,
          -0.09065378457307816,
          0.0012990274699404836,
          0.030902154743671417,
          -0.00029294699197635055,
          0.01521843858063221,
          0.0586315281689167,
          -0.03568040579557419,
          -0.02811444364488125,
          0.01721882075071335,
          0.028306983411312103,
          -0.08287936449050903,
          0.0045097931288182735,
          0.0401826836168766,
          -0.056961413472890854,
          0.01975744031369686,
          -0.020203089341521263,
          0.059276338666677475,
          -0.016292594373226166,
          -0.052501481026411057,
          -0.0596073716878891,
          -0.03987693786621094,
          -0.020705902948975563,
          0.011973114684224129,
          -0.03503315895795822,
          0.0074034095741808414,
          0.019302895292639732,
          0.043199531733989716,
          0.08449158072471619,
          -0.040666621178388596,
          -0.033525723963975906,
          0.02493881806731224,
          -0.004105194006115198,
          0.02688748762011528,
          0.004761039279401302,
          0.030992064625024796,
          0.007531882729381323,
          0.030757123604416847,
          0.018584823235869408,
          0.0236623827368021,
          0.046063125133514404,
          -0.01069580763578415,
          -0.014632915146648884,
          -0.08278641104698181,
          0.02309543825685978,
          0.09329895675182343,
          0.033815957605838776,
          -0.02121911011636257,
          -0.06305941194295883,
          -0.0403745099902153,
          -0.029028093442320824,
          0.029324978590011597,
          -0.033002618700265884,
          0.011294198222458363,
          0.00992053747177124,
          -0.029284661635756493,
          0.0495624840259552,
          -0.05932425335049629,
          0.025640225037932396,
          -0.015050469897687435,
          -0.004188743885606527,
          -0.042669013142585754,
          0.00035294878762215376,
          -0.039427563548088074,
          -0.009543582797050476,
          0.07411954551935196,
          0.007184154819697142,
          -0.0018380340188741684,
          -0.013465126976370811,
          -0.029975704848766327,
          -0.0006985316285863519,
          0.05689103156328201,
          0.03738195821642876,
          0.02085847593843937,
          -0.03947348892688751,
          0.038266055285930634,
          -0.004895808640867472,
          0.039777979254722595,
          -0.002248534932732582,
          -0.021423684433102608,
          -0.0010019850451499224,
          0.033429186791181564,
          0.013322895392775536,
          0.010394466109573841,
          0.12159931659698486,
          0.03023374453186989,
          -0.039503127336502075,
          -0.062328264117240906,
          -0.035268090665340424,
          -0.04635421559214592,
          0.046858832240104675,
          0.0005262219929136336,
          0.014569370076060295,
          -0.058281704783439636,
          -0.07275435328483582,
          0.0029125658329576254,
          -0.0016790216322988272,
          -0.020512232556939125,
          0.028789576143026352,
          -0.025267774239182472,
          -0.004667202476412058,
          -0.048427365720272064,
          0.03371061757206917,
          -0.08195766806602478,
          0.0884646326303482,
          -0.016437292098999023,
          -0.023496950045228004,
          0.02945529855787754,
          0.02665589191019535,
          -0.02316223457455635,
          0.010204574093222618,
          -0.07335171848535538,
          -0.031402986496686935,
          -0.03636819124221802,
          -0.027565395459532738,
          0.01813199743628502,
          -0.019766872748732567,
          -0.022921105846762657,
          0.016974732279777527,
          -0.06260259449481964,
          0.043866973370313644,
          -0.00143715541344136,
          0.00836118496954441,
          -0.01518335472792387,
          0.025568636134266853,
          -0.006599206943064928,
          0.03181552141904831,
          -0.03270253166556358,
          -0.05608009174466133,
          -0.09932979196310043,
          0.0039029549807310104,
          -0.012947541661560535,
          0.0049613225273787975,
          -0.052924513816833496,
          -0.00026327368686906993,
          -0.028125381097197533,
          0.01667444035410881,
          -0.01120771560817957,
          0.029314501211047173,
          -0.008442405611276627,
          -0.050710972398519516,
          -0.009454178623855114,
          -0.01396507490426302,
          -0.0090941796079278,
          -0.004433711990714073,
          -0.06891538202762604,
          -0.04156133905053139,
          0.05380227416753769,
          0.006501341704279184,
          0.019709039479494095,
          -0.09494727849960327,
          -0.002803301904350519,
          0.020779183134436607,
          -0.03852930665016174,
          -0.0041819107718765736,
          0.0023697209544479847,
          -0.02985825203359127,
          0.0014138424303382635,
          -0.004632655531167984,
          0.001670008641667664,
          0.014885645359754562,
          -0.03248043358325958,
          -0.02500211074948311,
          -0.0061707100830972195,
          0.06710889935493469,
          -0.04418361932039261,
          -0.024670490995049477,
          0.013994128443300724,
          0.004219573922455311,
          0.02646481618285179,
          -0.07598017901182175,
          -0.0034931274130940437,
          -0.04132743179798126,
          -0.04165137559175491,
          -0.027552945539355278,
          -0.02565532922744751,
          0.02180022932589054,
          -0.020243963226675987,
          0.003409935859963298,
          -0.03320494666695595,
          -0.03877273201942444,
          0.07087204605340958,
          0.030640292912721634,
          0.005711989011615515,
          -0.0007019542972557247,
          -0.01553276740014553,
          0.06408170610666275,
          -0.041564978659152985,
          -0.020004792138934135,
          0.08070789277553558,
          0.07228539139032364,
          -0.07723849266767502,
          0.016115088015794754,
          -0.009422928094863892,
          -0.00416182167828083,
          -0.021885359659790993,
          -0.06641232967376709,
          0.0025048761162906885,
          -0.054519545286893845,
          0.08056385815143585,
          -0.0029386007227003574,
          0.002321834908798337,
          -0.014988372102379799,
          -0.026444297283887863,
          0.01899375021457672,
          0.08977644145488739,
          0.017613952979445457,
          -0.01939770020544529,
          -0.08572325110435486,
          0.0535457618534565,
          0.008468895219266415,
          0.014321721158921719,
          0.026842016726732254,
          -0.049380071461200714,
          0.03183796629309654,
          -0.053429149091243744,
          0.0393303781747818,
          -0.025828564539551735,
          -0.02674347162246704,
          -0.06537588685750961,
          -0.06037614494562149,
          -0.011368060484528542,
          0.013600416481494904,
          -0.08188265562057495,
          0.0437590517103672,
          0.01102172676473856,
          0.0717388316988945,
          0.08059383183717728,
          -0.0393189899623394,
          0.02655024267733097,
          0.016542118042707443,
          -0.031179334968328476,
          0.07671751081943512,
          0.028734760358929634,
          -0.04551190882921219,
          -0.01440322957932949,
          0.044478368014097214,
          -0.016971349716186523,
          -0.09120673686265945,
          0.09749302268028259,
          0.00044674481614492834,
          -0.02372511476278305,
          -0.018135197460651398,
          -0.03246965631842613,
          0.024002201855182648,
          0.012189015746116638,
          0.05506214126944542,
          0.022282226011157036,
          -0.0010488962288945913,
          -0.019390707835555077,
          0.0015019819838926196,
          0.03485564514994621,
          0.04544096067547798,
          0.2739608585834503,
          0.011478391475975513,
          0.01404634304344654,
          -0.012229750864207745,
          0.025093674659729004,
          0.0029012628365308046,
          -0.057263314723968506,
          -0.005086594261229038,
          -0.01196092925965786,
          0.052255306392908096,
          0.03988521173596382,
          0.006830616854131222,
          -0.007903442718088627,
          -0.026140764355659485,
          0.0019742597360163927,
          -0.010654950514435768,
          -0.03739047050476074,
          0.025408467277884483,
          -0.11377523094415665,
          0.06987830251455307,
          0.03399735316634178,
          -0.009666627272963524,
          0.04463481903076172,
          -0.013354574330151081,
          0.0009173319558613002,
          0.020971927791833878,
          -0.06424735486507416,
          0.03606418892741203,
          0.013904804363846779,
          -0.009026581421494484,
          0.03053373098373413,
          -0.039703115820884705,
          0.05913081392645836,
          0.00756549509242177,
          -0.007177606225013733,
          0.020184986293315887,
          -0.024663101881742477,
          -0.03990335762500763,
          0.06459929794073105,
          0.08119373768568039,
          -0.005201147403568029,
          0.06323785334825516,
          -0.04405343532562256,
          -0.011697035282850266,
          0.0002081280981656164,
          0.007447726558893919,
          -0.04509219899773598,
          0.014283763244748116,
          0.03556516021490097,
          -0.048350702971220016,
          0.003848049556836486,
          -0.028465474024415016,
          -0.01157787349075079,
          -0.04245628044009209,
          -0.02057337760925293,
          -0.009463300928473473,
          0.007590658497065306,
          0.05492138862609863,
          0.0017122513381764293,
          -0.0044664982706308365,
          -0.04478231444954872,
          -0.08449525386095047,
          0.052711792290210724,
          0.0311446376144886,
          -0.06374341249465942,
          -0.012906519696116447,
          -0.04664280265569687,
          0.05753009393811226,
          -0.06715364009141922,
          -0.0018930531805381179,
          0.03733291104435921,
          0.046462904661893845,
          -0.05031010881066322,
          -0.02208746038377285,
          -0.019373668357729912,
          0.07881546765565872,
          0.03965749219059944,
          0.028485912829637527,
          -0.06684692949056625,
          -0.058348149061203,
          0.029290635138750076,
          0.04039976745843887,
          -0.0017499319510534406,
          0.010398447513580322,
          0.013761205598711967,
          -0.023901890963315964,
          0.0031572976149618626,
          -0.0495857298374176,
          0.05774816498160362,
          -0.008216453716158867,
          -0.014072958379983902,
          0.014418117702007294,
          -0.025303909555077553,
          0.04321310669183731,
          -0.052870262414216995,
          0.021945811808109283,
          0.07168848067522049,
          0.01345573365688324,
          0.03289906308054924,
          -0.041598979383707047,
          -0.01998387835919857,
          0.024020526558160782
        ]
      },
      "type": "document"
    },
    {
      "id": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "properties": {
        "page_content": "allowing it to already understand general\nlanguage features. We then modify the pre-\ntrained model for the specific task of sentiment\nclassificationby:\nAdding a Classification Layer: The pre-trained\nDistilBERT model outputs a contextualized\nrepresentation for each token in the input\nsequence.A fully connectedlayer (classification\nhead) is added on top of the model, with two\noutputnodescorrespondingtothetwosentiment\nlabels: positive and negative.Load model: We\nload pre-trained DistilBERT model then add the\nclassificationlayerwith2outputlabels:positive,\nnegative.Label Mapping: A mapping between\nsentiment labels and numerical IDs is defined\n(e.g., 0 for negative and 1 for positive). This\nensures that predictions made by the model are\ninterpretable.\n3.1 Finetuning\nThe fine-tuning process involves training the\npre-trained DistilBERT model on the IMDB\ndataset while updating its weights to specialize\nin sentiment analysis. Fine-tuning is a crucial\nstep because it allows the model to transfer its\ngeneral language understanding to the specific\ntask of classifying movie reviews. We set the\nseed to a random fixed number to ensure fair\ncomparison and reproductibility. A smaller\nsubset of 3000 reviews was created for training\nand testing for the distilBERT model. A subset\nof 100 reviews was used for the LLaMa 3 8B\nmodel due to constraints on available RAM for\nthe free version of Google Colab. This smaller\nsubset may affect the finetuning capabilities of\nthe LLaMa 3 8B model since it is not given as\nmuch training data as the distilBERT model.\nThisdifferenceintrainingdatawillbetakeninto\nconsideration for comparing the models’\nperformance.\nTraining: The model is trained using a\nsupervised learning approach. The tokenized\nIMDB dataset is split into training and\nevaluation sets. During training, the model\nlearns to minimize the classification error by\nadjusting its weights via backpropagation. A\nlearning rate scheduler and optimizer (AdamW)\nare used to ensure smooth convergence and\npreventoverfitting.\n3.2 Training Parameters Were Configured\n• output_dir:Directorytosavethemodel.\n• num_train_epochs:Numberoftraining\nepochs,setto2.\n• per_device_train_batch_size:Batchsizeper\ndevice,setto16.•learning_rate:Setto2×\n10−5\n• weight_decay:Setto0.01\n• optimizer:PagedAdamWwith32-bit\nprecision.\nAfter fine-tuning, the model’s performance\nimproved significantly, achieving an overall\naccuracy of 0.88. This result demonstrates the\neffectiveness of the fine-tuning process. By the\nend of the fine-tuning stage, the model is well-\nadjusted to the task of sentiment classification\nand can be used to make predictions on unseen\nmoviereviews.\n4. Evaluation\nThis section presents the experimental results\nobtained from evaluating the model after fine-\ntuning. The perfor- mance was assessed using\nprecision, recall, and F1-score metrics for each\nsentimentclass(positiveandnegative)aswellas\ntheoverallaccuracyofthemodel.\n4.1 Performance\nThe confusion matrixes below provides a\nbreakdown of the model’s predictions across all\nsentiment classes, as shown in Figures 2 and 3.\nThis analysis helps in identifying common\nmisclassifications and understanding the\nmodel’sstrengthsandweaknesses.\nFigure 2. DistilBERT-Base-Uncased\nFigure 3. LLaMa-3-8B\n110\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
        "document_metadata": {
          "producer": "Adobe Acrobat Pro 10.0.0",
          "creator": "Adobe Acrobat Pro 10.0.0",
          "creationdate": "2025-01-14T22:56:27+08:00",
          "author": "",
          "comments": "",
          "company": "",
          "keywords": "",
          "moddate": "2025-01-14T23:00:11+08:00",
          "subject": "",
          "title": "",
          "trapped": "/False",
          "source": "doc/Fine-Tuning_distilBERT_for_Enhanced_Sentiment_Clas.pdf",
          "total_pages": 5,
          "page": 2,
          "page_label": "3"
        },
        "headlines": [
          "3.1 Finetuning",
          "3.2 Training Parameters Were Configured",
          "4. Evaluation",
          "4.1 Performance"
        ],
        "summary": "A pre-trained DistilBERT model is adapted for sentiment classification by adding a two-node classification layer for positive and negative labels. The model is fine-tuned on a subset of the IMDB dataset to specialize in analyzing movie reviews. Due to computational constraints, a smaller dataset was used for the LLaMa 3 8B model compared to DistilBERT. Training involves supervised learning with an AdamW optimizer and a specific learning rate. After fine-tuning, the DistilBERT model achieved an overall accuracy of 0.88. The model's performance was evaluated using precision, recall, F1-score, and confusion matrices. The results demonstrate the effectiveness of fine-tuning for transferring general language knowledge to a specific task. The process ensures the model can make interpretable predictions on new, unseen reviews.",
        "keyphrases": [
          "DistilBERT model",
          "sentiment classification",
          "fine-tuning process",
          "IMDB dataset",
          "classification layer"
        ],
        "summary_embedding": [
          -0.06853368878364563,
          -0.000420792872318998,
          0.009224075824022293,
          -0.04136144742369652,
          -0.018384093418717384,
          -0.05359805375337601,
          -0.0692141056060791,
          0.017866604030132294,
          0.015657827258110046,
          -0.0335996188223362,
          0.10247613489627838,
          -0.3371198773384094,
          -0.027164224535226822,
          0.048917755484580994,
          -0.05561261996626854,
          -0.01341018546372652,
          0.003142016474157572,
          0.018745461478829384,
          0.07368235290050507,
          -0.05249409377574921,
          0.06335359811782837,
          -0.0415944866836071,
          0.020166102796792984,
          0.05091339349746704,
          -0.01796118915081024,
          0.0017039431259036064,
          0.011797351762652397,
          0.06220834702253342,
          -0.013452884741127491,
          -0.04089321196079254,
          -0.036199916154146194,
          0.04558814316987991,
          0.0409204326570034,
          -0.02117173746228218,
          -0.04050495848059654,
          -0.055374857038259506,
          -0.020965950563549995,
          -0.05762592703104019,
          0.002952656941488385,
          0.061476998031139374,
          0.001796177588403225,
          -0.008624979294836521,
          -0.010333416983485222,
          -0.0072708334773778915,
          -0.013250862248241901,
          -0.0796317309141159,
          -0.01689259707927704,
          0.0194321870803833,
          -0.003310712520033121,
          -0.015757061541080475,
          0.027041120454669,
          0.05251540616154671,
          0.0226454995572567,
          0.038591526448726654,
          0.026947740465402603,
          -0.08921685814857483,
          0.03426564484834671,
          0.015589461661875248,
          -0.07127790153026581,
          -0.0002679501776583493,
          0.07459402084350586,
          -0.03162292763590813,
          -0.020255526527762413,
          0.05032150447368622,
          0.01244451105594635,
          -0.009728677570819855,
          -0.02005217969417572,
          0.07486891746520996,
          -0.007132778409868479,
          0.011355295777320862,
          -0.028078800067305565,
          -0.005024611949920654,
          0.02185959555208683,
          -0.04725448042154312,
          0.010519792325794697,
          -0.04965469241142273,
          -0.005772322881966829,
          -0.05976087972521782,
          -0.006656567566096783,
          0.006888938136398792,
          -0.0425746776163578,
          -0.03020823374390602,
          -0.0031281181145459414,
          0.04546259716153145,
          -0.025338422507047653,
          -0.03887203708291054,
          0.032699570059776306,
          0.004673548508435488,
          -0.0030523762106895447,
          -0.09679588675498962,
          0.013113847002387047,
          -0.020666832104325294,
          0.04194498062133789,
          0.06782998144626617,
          -0.0309679564088583,
          0.04566747695207596,
          -0.08984048664569855,
          0.05781405046582222,
          0.02018309012055397,
          0.008771692402660847,
          0.03025648184120655,
          -0.031015582382678986,
          0.000302814383758232,
          -0.04132625088095665,
          -0.06730704009532928,
          0.03436177596449852,
          0.05443594232201576,
          -0.03873063251376152,
          0.05764127895236015,
          0.029634488746523857,
          0.021141821518540382,
          -0.04565652832388878,
          0.025631451979279518,
          0.010766815394163132,
          -0.0016886425437405705,
          -0.05230455473065376,
          -0.009007888846099377,
          -0.01538331713527441,
          0.011185165494680405,
          0.048769012093544006,
          0.07657846063375473,
          0.05716324970126152,
          0.005788890179246664,
          -0.030459385365247726,
          -0.04615965858101845,
          -0.02948535606265068,
          0.00042909575859084725,
          -0.0035206249449402094,
          -0.028234772384166718,
          -0.025252079591155052,
          0.033238984644412994,
          -0.022613029927015305,
          0.03360392898321152,
          0.028725668787956238,
          -0.0077567147091031075,
          -0.03487037494778633,
          0.004784347955137491,
          -0.05895211920142174,
          -0.04085109382867813,
          -0.056913360953330994,
          0.05990397185087204,
          -0.011890076100826263,
          -0.0383746400475502,
          -0.03616024926304817,
          0.01135543268173933,
          0.01849428564310074,
          -0.043645959347486496,
          0.02688358910381794,
          -0.03401106968522072,
          0.04827064275741577,
          0.037884991616010666,
          -0.02657133713364601,
          -0.027693241834640503,
          -0.03887609764933586,
          0.014761464670300484,
          0.050288837403059006,
          -0.049988534301519394,
          -0.04538286104798317,
          -0.07615390419960022,
          0.000850686221383512,
          0.06370720267295837,
          -0.0054692430421710014,
          0.012185742147266865,
          0.04889865592122078,
          -0.041196566075086594,
          -0.013580147176980972,
          0.02934640645980835,
          0.02054908499121666,
          -0.09312494844198227,
          -0.014088107272982597,
          0.006841135676950216,
          -0.06865368038415909,
          0.010785792022943497,
          0.007118647452443838,
          -0.004168442450463772,
          -0.024191007018089294,
          -0.03645426407456398,
          -0.07726753503084183,
          -0.045520201325416565,
          -0.09186761826276779,
          0.035086680203676224,
          -0.009845849126577377,
          -0.009709019213914871,
          0.017160575836896896,
          0.0571119450032711,
          0.06783901900053024,
          -0.0037219147197902203,
          -0.034974005073308945,
          0.015375799499452114,
          -0.008059333078563213,
          0.06122992932796478,
          -0.015577363781630993,
          -0.010525677353143692,
          0.05806727334856987,
          0.018565822392702103,
          0.027740608900785446,
          0.01854303851723671,
          0.03748439997434616,
          0.026752902194857597,
          -0.022685445845127106,
          -0.09111084043979645,
          0.04712485522031784,
          0.0782431960105896,
          -0.006060461513698101,
          0.015803813934326172,
          -0.05651971697807312,
          -0.01992587372660637,
          -0.06180400401353836,
          0.02677934803068638,
          -0.028275761753320694,
          0.016393892467021942,
          0.025510160252451897,
          -0.05973602086305618,
          0.025187717750668526,
          -0.03035804070532322,
          0.008601211942732334,
          0.007855272851884365,
          -0.019367987290024757,
          -0.06367836147546768,
          0.002749278675764799,
          -0.025730015709996223,
          0.030148159712553024,
          0.07878914475440979,
          0.010931713506579399,
          -0.004630585201084614,
          -0.01846197247505188,
          -0.0250332523137331,
          0.03900117799639702,
          0.014807169325649738,
          0.03133823722600937,
          0.019305838271975517,
          -0.04486880823969841,
          0.0472455732524395,
          -0.008359820581972599,
          0.07036862522363663,
          -0.0069227805361151695,
          -0.0462980680167675,
          0.009595387615263462,
          0.032666467130184174,
          0.039579231292009354,
          -0.009524772875010967,
          0.14018012583255768,
          0.03859059140086174,
          -0.03275454416871071,
          -0.03576233610510826,
          -0.03377882018685341,
          -0.02357964962720871,
          0.05299384519457817,
          0.026360036805272102,
          0.004927338100969791,
          -0.05302548035979271,
          -0.028517430648207664,
          0.011744245886802673,
          0.0024009505286812782,
          0.00638255849480629,
          0.04056709632277489,
          -0.05093468725681305,
          -0.0068563115783035755,
          -0.03327810764312744,
          0.07329651713371277,
          -0.06208323314785957,
          0.04848282411694527,
          0.0007449985714629292,
          -0.031711943447589874,
          0.04626400023698807,
          0.0181760024279356,
          -0.027379104867577553,
          -0.001960322493687272,
          -0.06480935961008072,
          0.012153682298958302,
          -0.01225844956934452,
          -0.03122333437204361,
          0.014788366854190826,
          -0.03426014259457588,
          0.0006769514293409884,
          0.020544232800602913,
          -0.04393146559596062,
          0.04147947579622269,
          -0.04654671996831894,
          -0.008592004887759686,
          -0.03663066774606705,
          -0.006243892014026642,
          -0.00530562037602067,
          0.049614302814006805,
          -0.021774590015411377,
          -0.027770012617111206,
          -0.11507023870944977,
          0.013742279261350632,
          -0.012233763933181763,
          -0.04896680265665054,
          -0.027162274345755577,
          -0.007204716559499502,
          -0.0261284988373518,
          0.02088124118745327,
          -0.03098166733980179,
          0.04821113869547844,
          -0.04451233148574829,
          -0.03962964192032814,
          -0.019739503040909767,
          -0.005148961208760738,
          0.03365601971745491,
          -0.008454241789877415,
          -0.055140234529972076,
          0.014875476248562336,
          0.02444770745933056,
          0.001249524182640016,
          0.0311137642711401,
          -0.06919514387845993,
          -0.01503022201359272,
          0.013674921356141567,
          -0.005425747483968735,
          -0.0003498979494906962,
          -0.00981817115098238,
          0.03257223591208458,
          0.018747525289654732,
          0.013084691017866135,
          -0.018839247524738312,
          0.007628666236996651,
          -0.02151649445295334,
          0.007842231541872025,
          -0.010896577499806881,
          0.06580164283514023,
          -0.044933367520570755,
          -0.04759368300437927,
          0.001861801603808999,
          0.02993614412844181,
          0.02486550621688366,
          -0.07847000658512115,
          -0.025421570986509323,
          -0.041848938912153244,
          -0.059099096804857254,
          0.005130694713443518,
          -0.006836868356913328,
          -0.015625564381480217,
          -0.011400608345866203,
          -0.007364662364125252,
          -0.01930178515613079,
          -0.06984682381153107,
          0.054019782692193985,
          0.028643297031521797,
          -0.06802317500114441,
          -0.005233911797404289,
          0.004872331395745277,
          0.05884101614356041,
          -0.06004628539085388,
          0.025281082838773727,
          0.0955808237195015,
          0.04361695796251297,
          -0.03552662208676338,
          0.018590908497571945,
          -0.021960236132144928,
          -0.01027835626155138,
          -0.019923130050301552,
          -0.06579191982746124,
          -0.006153143476694822,
          -0.01744268462061882,
          0.018365388736128807,
          0.02123750001192093,
          -0.0072723813354969025,
          -0.0011740804184228182,
          -0.01457767654210329,
          -0.00724806310608983,
          0.08995383232831955,
          0.03540986031293869,
          -0.025372862815856934,
          -0.06450682133436203,
          0.023765018209815025,
          0.012816878035664558,
          0.02508646249771118,
          0.045538485050201416,
          -0.01410785224288702,
          0.012811674736440182,
          -0.005413777660578489,
          0.033404093235731125,
          -0.06322178989648819,
          -0.032507073134183884,
          -0.09276849776506424,
          -0.03616025671362877,
          0.03350610285997391,
          -0.007410279009491205,
          -0.07272797077894211,
          0.06925567239522934,
          0.016444792971014977,
          0.08420783281326294,
          0.07741373032331467,
          -0.06914238631725311,
          0.02386011928319931,
          -0.014370935037732124,
          -0.0427250899374485,
          0.06586570292711258,
          0.018111195415258408,
          -0.02830921672284603,
          0.001098725013434887,
          0.04606835916638374,
          -0.019158631563186646,
          -0.12104611098766327,
          0.0905744731426239,
          0.03163720294833183,
          -0.0333707332611084,
          -0.014276414178311825,
          -0.021792786195874214,
          -0.01937984861433506,
          -0.015384415164589882,
          0.07406280189752579,
          0.02707689255475998,
          0.0039434656500816345,
          -0.021752813830971718,
          -0.013692761771380901,
          0.02403196319937706,
          -0.016744447872042656,
          0.2637878656387329,
          0.005072378553450108,
          0.03864836320281029,
          0.0009567686356604099,
          -0.012492979876697063,
          0.0029236506670713425,
          -0.10508125275373459,
          0.003120085457339883,
          -0.005176219157874584,
          0.054047536104917526,
          -0.011602289043366909,
          -0.008589322678744793,
          -0.032775796949863434,
          -0.013481472618877888,
          -0.006706443149596453,
          -0.017002047970891,
          -0.02945094183087349,
          0.009138510562479496,
          -0.10361006110906601,
          0.05524888634681702,
          0.04586872458457947,
          -0.0065049403347074986,
          0.044475656002759933,
          0.019801925867795944,
          -0.011421434581279755,
          0.037993818521499634,
          -0.06661736220121384,
          0.03914164379239082,
          0.004438729956746101,
          0.01080547645688057,
          0.02521437779068947,
          0.008585373871028423,
          0.10133486241102219,
          0.018530843779444695,
          -0.0214606411755085,
          0.028074555099010468,
          -0.01781746931374073,
          0.001332045765593648,
          0.060340143740177155,
          0.05330432206392288,
          -0.02039700746536255,
          0.029964882880449295,
          -0.03837230056524277,
          -0.006700299680233002,
          -0.014844683930277824,
          -0.02780902199447155,
          -0.02441122569143772,
          0.001996488543227315,
          0.0526287704706192,
          -0.04856579750776291,
          0.01773666962981224,
          -0.028142482042312622,
          -0.039234697818756104,
          -0.0690290704369545,
          -0.037856847047805786,
          0.015620055608451366,
          0.010229135863482952,
          0.03531086444854736,
          -0.026523098349571228,
          -0.012243003584444523,
          -0.02402511052787304,
          -0.07022330909967422,
          0.03527994453907013,
          0.05550646781921387,
          -0.03440503776073456,
          -0.01905668154358864,
          -0.04255877435207367,
          0.04619315639138222,
          -0.01666082814335823,
          -0.0526813305914402,
          0.03561826050281525,
          0.04155841842293739,
          -0.02578672021627426,
          -0.01718456856906414,
          -0.0183443371206522,
          0.06598623842000961,
          0.024777807295322418,
          0.02689753845334053,
          -0.08045017719268799,
          -0.061155304312705994,
          0.07603852450847626,
          0.05942680686712265,
          -0.022456781938672066,
          -0.002864040667191148,
          0.0026624733582139015,
          0.0281274002045393,
          -0.004763831850141287,
          -0.05117974430322647,
          0.0489545501768589,
          -0.0034268235322088003,
          -0.04360587149858475,
          0.01848188415169716,
          -0.019293932244181633,
          0.04418665170669556,
          -0.043228816241025925,
          -0.020058322697877884,
          0.0650915876030922,
          0.03867499902844429,
          -0.006050629075616598,
          -0.043457724153995514,
          -0.0006598079344257712,
          0.017605086788535118
        ]
      },
      "type": "document"
    },
    {
      "id": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "properties": {
        "page_content": "The evaluation of the model’s performance was\nbased on standard metrics including precision,\nrecall, and F1-score. These metrics were\ncomputed for each sentiment class (positive and\nnegative) as well as for the overall model\nperformance. The formulas for these metrics are\nasfollows.\nTP+TN\nTP+TN+FN+FP\nTP\nTP+FP\nTP\nTP+FN\nPrecision×Recall\nPrecision+Recall\nTable 1. Comparison of Model Performance\nModel Accuracy Precision Recall F1Score\ndistilBERT-base-\nuncased\nMeta-Llama-3-8B\n0.88\n0.66\n0.86075\n0.63793\n0.90666\n0.74000\n0.88311\n0.68518\n4.2 Latency\nDistilBERT-base-uncased is much faster, with\nlow latency suitable for simple real-time\napplications such as sentiment classification due\nto its smaller size and efficient architecture. For\nthis task of sentiment classification, it took\napproximately2:23mintoruneachepoch.\nMeta-Llama-3-8B provides more powerful\ncapabilities and nuanced understanding but\ncomeswithhigher\nlatency, making it more suitable for applications\nwhere processingspeed is less critical compared\ntooutputquality.\n5. Discussion\nThis study compared the performance of\nDistilBERT and LLaMA 3 on sentiment\nclassification using the IMDB dataset. Our\nfindings indicate that both models have their\nstrengths and weaknesses, and understanding\nthese can provide insights into optimizing\nsentiment analysis systems in practical\napplications.\nModel Performance:The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset.\nGeneralization Across Domains: The models\nperformed well on the IMDB dataset but may\nstruggle to generalize to different domains,\nwhere sentiment expressions vary significantly.\nImplementingdomainadaptationtechniquesand\nfine-tuning on diverse datasets could enhance\nmodel robustness. Highly polar movie reviews\nwere used, and both models’ ability to detect\nnuanced sentiments was not tested. Future work\ncould explore specialized training datasets to\nimprovetherecognitionofsubtlesentiments.\nPromising avenues for future research include\ndeveloping hybrid models that combine the\nstrengths of both LLaMA 3 and DistilBERT.\nEnhancing training techniques, such as few-shot\nand zero-shot learning, will also be essential for\nimproving performance across diverse contexts.\nAddressing the limitations identified in this\nstudy will be crucial for creating more accurate\nand efficient sentiment classification systems\nthat effectively meet user needs. These findings\nunderscore the importance of model selection\nbasedonapplicationrequirements.\nFuture Work: Several avenues for future\nexplorationremain:\n• Leveraging hybrid models combining\nefficiencyandnuancedunderstanding[5].\n• Expanding experiments to include\ndomain-specific datasets to enhance\ngeneralization.\n• Investigating advanced fine-tuning\ntechniques, such as adapters and LoRA\nlayers,forfurtheroptimization[11].\n6. Conclusion\nIn this study, we conduct an evaluation of\nsentiment classification by comparing the\nperformance of a large language model and a\nsmall language model. The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset. These findings suggest that model size\nalone does not guarantee better performance,\nemphasizing the importance of selecting the\nappropriate model for specific tasks. Future\nworkcouldexplore theinfluenceof datasetsize,\nfine-tuning strategies, and domain-specific\ntraining to further optimize sentiment\nclassificationmodels.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran,\nRonen Eldan, Johannes Gehrke, Eric\nHorvitz,EceKamar,PeterLee,YinTatLee,\nYuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence:\nEarlyexperimentswithgpt-4.arXivpreprint\narXiv:2303.12712(2023).\nAccuracy=\nRecall=\nPrecision=\nF1=2 ×\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n111\nCopyright @ STEMM Institute Press\nhttp://www.stemmpress.com",
        "document_metadata": {
          "producer": "Adobe Acrobat Pro 10.0.0",
          "creator": "Adobe Acrobat Pro 10.0.0",
          "creationdate": "2025-01-14T22:56:27+08:00",
          "author": "",
          "comments": "",
          "company": "",
          "keywords": "",
          "moddate": "2025-01-14T23:00:11+08:00",
          "subject": "",
          "title": "",
          "trapped": "/False",
          "source": "doc/Fine-Tuning_distilBERT_for_Enhanced_Sentiment_Clas.pdf",
          "total_pages": 5,
          "page": 3,
          "page_label": "4"
        },
        "headlines": [
          "Table 1. Comparison of Model Performance",
          "4.2 Latency",
          "5. Discussion",
          "Future Work",
          "6. Conclusion"
        ],
        "summary": "The study compared DistilBERT and LLaMA 3 for sentiment analysis on the IMDB dataset. Performance was evaluated using accuracy, precision, recall, and F1-score. DistilBERT outperformed LLaMA 3 across all these metrics. DistilBERT also had much lower latency, making it suitable for real-time applications. LLaMA 3 offered more nuanced understanding but with higher latency. Both models may struggle to generalize to domains beyond the training data. Future work could explore hybrid models and domain adaptation techniques. The findings show model size alone does not guarantee better performance. Selecting the right model depends on specific application requirements like speed or output quality.",
        "keyphrases": [
          "sentiment classification",
          "DistilBERT model",
          "LLaMA 3",
          "model performance",
          "IMDB dataset"
        ],
        "summary_embedding": [
          -0.060175925493240356,
          -0.010661754757165909,
          -0.015903541818261147,
          -0.008053727447986603,
          -0.02508780173957348,
          -0.019880959764122963,
          -0.06876710802316666,
          0.02507156692445278,
          0.010990762151777744,
          -0.010216102935373783,
          0.12767894566059113,
          -0.3485985994338989,
          -0.005812138319015503,
          0.04768087714910507,
          -0.050146233290433884,
          0.0017506456933915615,
          -0.004882680252194405,
          -0.01107823383063078,
          0.06057606637477875,
          -0.03885059803724289,
          0.046881597489118576,
          -0.02787623181939125,
          -0.015996502712368965,
          0.033260006457567215,
          -0.02444463223218918,
          0.011274170130491257,
          0.009548833593726158,
          0.05057995393872261,
          -0.018374890089035034,
          -0.07489120960235596,
          0.0013948800042271614,
          0.038701582700014114,
          0.013758962973952293,
          0.013536150567233562,
          -0.034716058522462845,
          -0.056879837065935135,
          0.03545040264725685,
          -0.05981249734759331,
          -0.024905910715460777,
          0.05114509537816048,
          0.012724227271974087,
          0.02415572479367256,
          0.008615581318736076,
          0.022801190614700317,
          0.015424784272909164,
          -0.08740723133087158,
          0.007757676299661398,
          0.012698153033852577,
          -0.006935231853276491,
          -0.02492457814514637,
          -0.024354850873351097,
          0.05879673361778259,
          0.011907970532774925,
          0.04961811378598213,
          0.02181781642138958,
          -0.044463999569416046,
          0.033579349517822266,
          0.014465993270277977,
          -0.070571169257164,
          0.023775415495038033,
          0.05700875073671341,
          -0.025048930197954178,
          -0.04742265120148659,
          0.053615134209394455,
          0.008760214783251286,
          0.04757469892501831,
          0.0038852051366120577,
          0.07513967156410217,
          -0.002529810881242156,
          0.03462279215455055,
          -0.01960081234574318,
          -0.021312393248081207,
          0.001863742363639176,
          -0.08796476572751999,
          0.02162495069205761,
          -0.06690330803394318,
          -0.005974672269076109,
          -0.03083566576242447,
          0.003232880961149931,
          -0.014077146537601948,
          -0.07347805798053741,
          -0.013183332048356533,
          -0.027988292276859283,
          0.05398733541369438,
          -0.012860475108027458,
          -0.03287304937839508,
          0.024534696713089943,
          0.014759731478989124,
          0.023493114858865738,
          -0.10730722546577454,
          0.014480880461633205,
          -0.013348476029932499,
          0.04883356764912605,
          0.04913046583533287,
          -0.02003866620361805,
          0.03606759384274483,
          -0.09721709787845612,
          0.06554094702005386,
          0.009707768447697163,
          -0.005289807450026274,
          0.010097242891788483,
          -0.05398626998066902,
          0.001236569369211793,
          -0.004408398177474737,
          -0.0683922991156578,
          0.04434598982334137,
          0.07875557988882065,
          -0.04565088823437691,
          0.055732160806655884,
          0.06581917405128479,
          0.004996465519070625,
          -0.013744794763624668,
          -0.007425407879054546,
          0.0004601737891789526,
          0.017849160358309746,
          -0.056804198771715164,
          0.000274589256150648,
          0.03770501911640167,
          0.011378840543329716,
          0.02923586778342724,
          0.07342404872179031,
          0.06569261103868484,
          0.003295366419479251,
          -0.019355135038495064,
          -0.02390981838107109,
          -0.033551182597875595,
          -0.03701840341091156,
          0.015612510032951832,
          -0.021812517195940018,
          -0.008303320966660976,
          -0.024780064821243286,
          -0.016933714970946312,
          0.027131520211696625,
          0.014694007113575935,
          0.03925091773271561,
          -0.008555903099477291,
          0.006932306569069624,
          -0.038940660655498505,
          -0.028896097093820572,
          -0.03786733001470566,
          0.03417044132947922,
          -0.004574883263558149,
          -0.04754793271422386,
          -0.04718954861164093,
          0.031009800732135773,
          0.0484774149954319,
          -0.014915586449205875,
          0.029594533145427704,
          0.020743846893310547,
          0.03803703933954239,
          0.00927609484642744,
          -0.022235078737139702,
          0.007922333665192127,
          -0.028107361868023872,
          -0.02454378828406334,
          0.03171493113040924,
          -0.023848753422498703,
          -0.014092972502112389,
          -0.03322786092758179,
          -0.03320927172899246,
          0.032719697803258896,
          -0.022745735943317413,
          -0.00843757763504982,
          0.05899291858077049,
          -0.0500498004257679,
          -0.008219020441174507,
          0.04375185817480087,
          0.01609584502875805,
          -0.07834652066230774,
          -0.019092267379164696,
          0.011374285444617271,
          -0.06249023228883743,
          -0.01048179529607296,
          -0.028532089665532112,
          -0.016980057582259178,
          -0.0157209150493145,
          -0.023307882249355316,
          -0.04404784366488457,
          -0.015917276963591576,
          -0.06767114251852036,
          0.0026521987747401,
          0.01145720761269331,
          -0.0054706064984202385,
          0.028522487729787827,
          0.019892539829015732,
          0.03063322976231575,
          0.001265808823518455,
          -0.037109192460775375,
          0.04166116192936897,
          0.01597440242767334,
          0.07624693214893341,
          -0.0043198829516768456,
          0.018687395378947258,
          0.04114438593387604,
          0.038663968443870544,
          0.03233296051621437,
          0.05444507300853729,
          0.029672466218471527,
          0.01004779152572155,
          -0.04436744004487991,
          -0.0530681386590004,
          0.02093120850622654,
          0.1119009256362915,
          -0.015343998558819294,
          0.012889147736132145,
          -0.0621924102306366,
          -0.015966838225722313,
          -0.05073588714003563,
          0.019581211730837822,
          -0.058694928884506226,
          0.03661004453897476,
          -0.014572151936590672,
          -0.035089828073978424,
          -0.011943706311285496,
          -0.038414597511291504,
          0.007567088585346937,
          -0.02727825753390789,
          -0.03596596419811249,
          -0.04885626956820488,
          0.0004044031957164407,
          -0.05324414744973183,
          -0.0016184634296223521,
          0.07866616547107697,
          -0.008598397485911846,
          -0.02802419289946556,
          -0.011513741686940193,
          -0.002740593161433935,
          -0.009113240987062454,
          0.036762818694114685,
          0.04719763994216919,
          0.006401685066521168,
          -0.06337431073188782,
          0.029336363077163696,
          -0.01022971048951149,
          0.06349722295999527,
          -0.006532586179673672,
          -0.029952289536595345,
          0.021729489788413048,
          0.05039752274751663,
          0.04098252207040787,
          -0.028013912960886955,
          0.12857910990715027,
          0.04131831228733063,
          -0.013658493757247925,
          -0.07870613038539886,
          -0.042126864194869995,
          -0.03521065413951874,
          0.03733430430293083,
          -0.03481398895382881,
          0.009063267149031162,
          -0.022946208715438843,
          -0.03419310227036476,
          -0.012851105071604252,
          -0.03295554965734482,
          -0.002249827841296792,
          0.06059412285685539,
          0.004225379321724176,
          -0.011249793693423271,
          -0.03468380123376846,
          0.011699332855641842,
          -0.08422790467739105,
          0.04828730598092079,
          -0.038833070546388626,
          -0.04469732567667961,
          0.021876363083720207,
          -0.031863100826740265,
          -0.02410179190337658,
          -0.028514105826616287,
          -0.06809687614440918,
          0.009650275111198425,
          -0.020464720204472542,
          -0.052414670586586,
          0.011030230671167374,
          -0.014538380317389965,
          0.008875351399183273,
          0.0144407469779253,
          -0.05330659821629524,
          0.026748187839984894,
          -0.029525144025683403,
          0.028049223124980927,
          -0.029523007571697235,
          0.02473423257470131,
          -0.007779802195727825,
          0.05286987125873566,
          -0.04194101691246033,
          -0.03935403749346733,
          -0.09263364970684052,
          0.004521526396274567,
          0.0025089478585869074,
          -0.00965314544737339,
          -0.015522842295467854,
          0.002960136393085122,
          -0.03308740630745888,
          0.03023008070886135,
          -0.016450824216008186,
          0.042118024080991745,
          -0.04315205290913582,
          -0.021979697048664093,
          -0.014898869208991528,
          0.034507788717746735,
          0.024875115603208542,
          -0.008083466440439224,
          -0.045628976076841354,
          -0.01102454587817192,
          0.030756406486034393,
          0.042042821645736694,
          -0.00010831450344994664,
          -0.061005011200904846,
          0.013291116803884506,
          0.04673390835523605,
          -0.018241599202156067,
          -0.029629703611135483,
          -0.015108013525605202,
          -0.008676771074533463,
          -0.015810491517186165,
          0.007676220498979092,
          0.012405782006680965,
          -0.0024919419083744287,
          -0.03534979745745659,
          -0.03941630944609642,
          0.022274047136306763,
          0.05112485960125923,
          -0.062036678194999695,
          -0.07003693282604218,
          0.030431417748332024,
          -0.0094737084582448,
          0.0158243328332901,
          -0.08496490120887756,
          -0.02613922767341137,
          -0.04817364364862442,
          -0.04877495393157005,
          0.008469443768262863,
          -0.016791729256510735,
          -0.005034653469920158,
          -0.01976419799029827,
          0.01337067037820816,
          -0.05349146947264671,
          -0.07002314180135727,
          0.06773296743631363,
          -0.00016615138156339526,
          -0.04068263992667198,
          0.013676447793841362,
          0.00848105363547802,
          0.06294303387403488,
          -0.0011240519816055894,
          0.06405149400234222,
          0.06624874472618103,
          0.07058023661375046,
          -0.017375532537698746,
          0.002071420894935727,
          0.007068324834108353,
          -0.029087623581290245,
          -0.013558742590248585,
          -0.03078591637313366,
          0.002934029558673501,
          -0.035922784358263016,
          0.0035413142759352922,
          0.04768071696162224,
          -0.007643004413694143,
          0.009569584392011166,
          -0.014247836545109749,
          0.03071664087474346,
          0.0900002121925354,
          0.04289601743221283,
          -0.05142144858837128,
          -0.07298286259174347,
          -0.016509776934981346,
          0.029357243329286575,
          0.0008017786894924939,
          0.03522869572043419,
          -0.0014275789726525545,
          0.011903173290193081,
          -0.010329480282962322,
          -4.186744990875013e-05,
          -0.060342226177453995,
          -0.051031164824962616,
          -0.10477723181247711,
          -0.08767819404602051,
          -0.018842805176973343,
          0.00411639828234911,
          -0.07849708199501038,
          0.07885967195034027,
          -0.007350386586040258,
          0.06142585724592209,
          0.08665478229522705,
          -0.049229130148887634,
          0.00012627214891836047,
          -0.017958864569664,
          -0.017449501901865005,
          0.09175196290016174,
          0.005766598973423243,
          -0.06030432879924774,
          -0.0027451214846223593,
          0.0237727127969265,
          -0.008704871870577335,
          -0.06596989184617996,
          0.03765427693724632,
          0.01904977299273014,
          -0.0052146450616419315,
          -0.025458924472332,
          -0.04592600837349892,
          0.01955508626997471,
          -0.015227734111249447,
          0.09442543238401413,
          0.02129599079489708,
          0.004338126163929701,
          -0.04000771790742874,
          -0.010277028195559978,
          0.019198330119252205,
          -0.01550777442753315,
          0.2826104164123535,
          -0.01751500740647316,
          0.0023273304104804993,
          0.02181152254343033,
          -0.0012156472075730562,
          -0.011301873251795769,
          -0.07109016180038452,
          0.016957087442278862,
          -0.015585851855576038,
          0.06835292279720306,
          -0.003970869816839695,
          -0.0202444177120924,
          -0.008874047547578812,
          0.013187852688133717,
          -0.004289181903004646,
          -0.01844237931072712,
          -0.0400204136967659,
          0.059828322380781174,
          -0.1044364869594574,
          0.037648577243089676,
          0.06641889363527298,
          -0.012970225885510445,
          0.046071071177721024,
          -0.006875918246805668,
          0.007417894434183836,
          0.04648046940565109,
          -0.08759364485740662,
          0.05637567490339279,
          0.009817341342568398,
          0.005030066706240177,
          0.014429248869419098,
          -0.01683945022523403,
          0.09950093924999237,
          0.03397529199719429,
          -0.0018615481676533818,
          0.03265592083334923,
          -0.017454935237765312,
          -0.02595275454223156,
          0.06246466189622879,
          0.05349485203623772,
          0.0005894210189580917,
          0.004837724845856428,
          -0.010425672866404057,
          -0.029132328927516937,
          0.019048506394028664,
          -0.0048567913472652435,
          -0.02951853536069393,
          0.004674694035202265,
          0.06351262331008911,
          -0.053209807723760605,
          -0.004909643437713385,
          -0.04626254737377167,
          -0.034544650465250015,
          -0.050569646060466766,
          -0.04877123981714249,
          0.008832813240587711,
          -0.028629248961806297,
          0.03803195804357529,
          -0.01052719447761774,
          -0.012503881938755512,
          -0.014488301239907742,
          -0.06072970852255821,
          0.012220154516398907,
          0.050189949572086334,
          -0.02752271480858326,
          -0.004395595286041498,
          -0.0390552319586277,
          0.036636319011449814,
          0.00048373694880865514,
          -0.061302781105041504,
          0.05083228647708893,
          -0.006774705369025469,
          -0.04192899540066719,
          -0.017142586410045624,
          -0.02335328608751297,
          0.053974926471710205,
          0.029612857848405838,
          0.013019521720707417,
          -0.0856088250875473,
          -0.06381268054246902,
          0.09259959310293198,
          0.06692633777856827,
          -0.013817179016768932,
          -0.011010498739778996,
          -0.011832153424620628,
          -0.019287727773189545,
          -0.0019752902444452047,
          -0.06699294596910477,
          0.03097453899681568,
          -0.03816685080528259,
          -0.019034964963793755,
          0.02386404387652874,
          0.008845067583024502,
          0.01834317296743393,
          -0.04576992988586426,
          0.007390059996396303,
          0.1024685874581337,
          0.015163999050855637,
          -0.03355419263243675,
          -0.0395561084151268,
          0.011613882146775723,
          -0.0021186089143157005
        ]
      },
      "type": "document"
    },
    {
      "id": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "properties": {
        "page_content": "[2]JacobDevlin,Ming-WeiChang,KentonLee,\nand Kristina Toutanova. 2019. BERT: Pre-\ntraining of Deep Bidirectional Transformers\nforLanguageUnderstanding.InProceedings\nof the 2019 Conference of the North\nAmerican Chapter of the Association for\nComputational Linguistics: Human\nLanguage Technologies, Volume 1 (Long\nand Short Papers), Jill Burstein, Christy\nDoran, and Thamar Solorio\n(Eds.).Association for Computational\nLinguistics, Minneapolis, Minnesota,4171–\n4186.https://doi.org/10.18653/v1/N19-1423\n[3]Diefan Lin, Yi Wen, Weishi Wang, and Yan\nSu. 2024. Enhanced Sentiment Intensity\nRegression Through LoRA Fine-Tuning on\nLlama 3.IEEE Access 12 (2024), 108072–\n108087.\nhttps://doi.org/10.1109/ACCESS.2024.3438\n353\n[4] Bing Liu. 2020. Sentiment analysis: Mining\nopinions, sentiments, and emotions.\nCambridgeuniversitypress.\n[5]HaochenLiu,Sai KrishnaRallabandi,Yijing\nWu, Parag Pravin Dakle, and Preethi\nRaghavan. 2023. Self-training Strategies for\nSentiment Analysis: An Empirical Study.\narXivpreprintarXiv:2309.08777(2023).\n[6]AndrewL.Maas,RaymondE.Daly,PeterT.\nPham, Dan Huang, Andrew Y. Ng, and\nChristopher Potts. 2011. Learning Word\nVectors for Sentiment Analysis. In\nProceedings of the 49th Annual Meeting of\nthe Association for Computational\nLinguistics: HumanLanguage Technologies,\nDekang Lin, Yuji Matsumoto, and Rada\nMihalcea (Eds.). Association for\nComputational Linguistics, Portland,\nOregon, USA, 142–150.\nhttps://aclanthology.org/P11-1015Soujanya\nPoria, Devamanyu Hazarika, Navonil\nMajumder, and Rada Mihalcea. 2023.\nBeneath the Tip of the Iceberg: Current\nChallenges and New Directions in\nSentiment Analysis Research. IEEE\nTransactions on Affective Computing 14, 1\n(2023), 108–132.\nhttps://doi.org/10.1109/TAFFC.2020.30381\n67\n[7] V Sanh. 2019. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint\narXiv:1910.01108(2019).\n[8] Amira Samy Talaat. 2023. Sentiment\nanalysis classification system using hybrid\nBERT models. Journal of Big Data 10, 1\n(2023),110.\n[9] A Vaswani. 2017. Attention is all you need.\nAdvances in Neural Information Processing\nSystems(2017).\n[10] Peiyuan Zhang, Guangtao Zeng, Tianduo\nWang, and Wei Lu. 2024. Tinyllama: An\nopen-source small language model. arXiv\npreprintarXiv:2401.02385(2024).\n112\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
        "document_metadata": {
          "producer": "Adobe Acrobat Pro 10.0.0",
          "creator": "Adobe Acrobat Pro 10.0.0",
          "creationdate": "2025-01-14T22:56:27+08:00",
          "author": "",
          "comments": "",
          "company": "",
          "keywords": "",
          "moddate": "2025-01-14T23:00:11+08:00",
          "subject": "",
          "title": "",
          "trapped": "/False",
          "source": "doc/Fine-Tuning_distilBERT_for_Enhanced_Sentiment_Clas.pdf",
          "total_pages": 5,
          "page": 4,
          "page_label": "5"
        },
        "headlines": [
          "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "Enhanced Sentiment Intensity Regression Through LoRA Fine-Tuning on Llama 3",
          "Sentiment analysis: Mining opinions, sentiments, and emotions",
          "Self-training Strategies for Sentiment Analysis: An Empirical Study",
          "Learning Word Vectors for Sentiment Analysis"
        ],
        "summary": "The text lists several key research papers in natural language processing and sentiment analysis. It includes the seminal BERT model from 2019 and its distilled version, DistilBERT. Other foundational works mentioned are the \"Attention is All You Need\" paper and early sentiment analysis research from 2011. Recent studies cover enhanced sentiment intensity regression using LoRA on Llama 3 and hybrid BERT models. It also references a sentiment analysis textbook and papers on self-training strategies and current research challenges. Newer, efficient models like TinyLlama are also noted. The compilation appears to be a reference list for a journal article in big data and computing.",
        "keyphrases": [
          "BERT",
          "Sentiment Analysis",
          "Deep Bidirectional Transformers",
          "LoRA Fine-Tuning",
          "Attention is all you need"
        ],
        "summary_embedding": [
          -0.03154522553086281,
          -0.004516022279858589,
          0.022067641839385033,
          -0.04294547066092491,
          -0.009079270996153355,
          -0.0733707994222641,
          -0.07030638307332993,
          0.017882654443383217,
          0.057423461228609085,
          -0.01661800779402256,
          0.10137233883142471,
          -0.3460078537464142,
          -0.007263810373842716,
          0.054887183010578156,
          -0.04785090312361717,
          -4.160732714808546e-05,
          0.019564930349588394,
          0.00025647785514593124,
          0.027077293023467064,
          -0.011632174253463745,
          0.04867494851350784,
          -0.04436291381716728,
          -0.013536735437810421,
          0.04051681607961655,
          -0.008282260969281197,
          0.0017883788095787168,
          -0.0016733030788600445,
          0.04395689442753792,
          -0.01784592494368553,
          -0.062231458723545074,
          -0.029740162193775177,
          0.02045789174735546,
          0.019798893481492996,
          -0.018033482134342194,
          -0.046064671128988266,
          -0.07379364967346191,
          0.015742849558591843,
          -0.04856804758310318,
          -0.008269940502941608,
          0.06905600428581238,
          0.0037866777274757624,
          0.007615196518599987,
          0.008825923316180706,
          0.007724345661699772,
          -0.022399861365556717,
          -0.07263185828924179,
          -0.006511190440505743,
          0.01639970764517784,
          -0.0426705963909626,
          -0.0111720385029912,
          -0.016785921528935432,
          0.05593792349100113,
          0.0567881278693676,
          0.009729363955557346,
          0.04513886570930481,
          -0.04941364750266075,
          0.050961222499608994,
          0.0496453158557415,
          -0.09875577688217163,
          -0.018210381269454956,
          0.06156231835484505,
          0.0026481179520487785,
          -0.011111128143966198,
          0.04555872082710266,
          -0.003774058073759079,
          0.020948830991983414,
          -0.001669651479460299,
          0.03284165635704994,
          -0.029010804370045662,
          0.027218114584684372,
          -0.03827345371246338,
          0.010824383236467838,
          -0.02053864859044552,
          -0.04976489767432213,
          0.0298474058508873,
          -0.058304570615291595,
          0.013206574134528637,
          -0.06668329983949661,
          -0.011736230924725533,
          -0.005263631232082844,
          -0.07447867840528488,
          -0.017474910244345665,
          -0.01661258190870285,
          0.10529577732086182,
          -0.0023553541395813227,
          -0.04460969567298889,
          0.008711225353181362,
          -0.03152763843536377,
          -0.0107384342700243,
          -0.10661021620035172,
          -0.006855085492134094,
          -0.0016853566048666835,
          0.018630050122737885,
          0.06223639100790024,
          -0.01944437064230442,
          0.03323651850223541,
          -0.08987804502248764,
          0.07101565599441528,
          0.0013229901669546962,
          -0.016728561371564865,
          -0.0015608874382451177,
          -0.05328447371721268,
          -0.021592475473880768,
          -0.03229771926999092,
          -0.05147302895784378,
          0.07691144198179245,
          0.06272518634796143,
          -0.045487478375434875,
          0.01966470293700695,
          0.04508384317159653,
          -0.0065791006200015545,
          -0.02082723379135132,
          0.035202328115701675,
          0.013022304512560368,
          0.03561723977327347,
          -0.025715835392475128,
          0.011336931958794594,
          -0.013323566876351833,
          0.022748369723558426,
          0.05591895803809166,
          0.08888749033212662,
          0.08731121569871902,
          0.035277001559734344,
          -0.017266174778342247,
          -0.060829855501651764,
          -0.03732201084494591,
          -0.018395844846963882,
          -0.01789197139441967,
          0.017874648794531822,
          -0.016109907999634743,
          0.012480485253036022,
          -0.0715055987238884,
          0.02979261241853237,
          0.033385906368494034,
          0.031351085752248764,
          0.02913714572787285,
          0.0235432256013155,
          -0.03371475636959076,
          -0.04777112975716591,
          -0.042190197855234146,
          0.04071531817317009,
          -0.0397530123591423,
          -0.05616248399019241,
          -0.05908886343240738,
          -0.03987637907266617,
          0.05960604548454285,
          -0.026457011699676514,
          -0.0015567054506391287,
          0.009545824490487576,
          0.03154681250452995,
          0.01380577590316534,
          -0.04529254883527756,
          0.007482264190912247,
          -0.05150849372148514,
          -0.03747711703181267,
          0.05763619393110275,
          0.0031152674928307533,
          -0.017113564535975456,
          -0.03375561162829399,
          -0.02734297700226307,
          0.04461786150932312,
          -0.04031038284301758,
          0.010011173784732819,
          0.023234060034155846,
          -0.044003088027238846,
          0.0004908917471766472,
          0.06664823740720749,
          0.0185816939920187,
          -0.08969800919294357,
          0.0028705333825200796,
          0.015682583674788475,
          -0.06106720492243767,
          0.03420118987560272,
          -0.00631285272538662,
          -0.01944342814385891,
          -0.015860913321375847,
          -0.02565573900938034,
          -0.05917264148592949,
          -0.0019305399619042873,
          -0.08671284466981888,
          -0.008565312251448631,
          -0.020220346748828888,
          0.020426511764526367,
          0.027116218581795692,
          0.024004939943552017,
          0.044675637036561966,
          -0.035638753324747086,
          -0.036515478044748306,
          0.03394642099738121,
          -0.0047765085473656654,
          0.0725712701678276,
          0.015504402108490467,
          0.008083293214440346,
          0.013741211965680122,
          0.046635691076517105,
          0.022833917289972305,
          0.04574567824602127,
          0.032059915363788605,
          -0.010173814371228218,
          -0.0385935977101326,
          -0.027685271576046944,
          0.020079035311937332,
          0.10134336352348328,
          0.013098220340907574,
          0.008143036626279354,
          -0.0337657555937767,
          -0.030025506392121315,
          -0.035035327076911926,
          -0.018193040043115616,
          -0.07426586747169495,
          -0.016659226268529892,
          -0.021471092477440834,
          -0.05284058675169945,
          0.03995495289564133,
          -0.04816168546676636,
          0.011197434738278389,
          -0.019165508449077606,
          -0.021536076441407204,
          -0.003973647020757198,
          -0.013723594136536121,
          -0.032291628420352936,
          -0.014741715975105762,
          0.08540431410074234,
          -0.04057827591896057,
          0.010618293657898903,
          -0.024217046797275543,
          0.0025924076326191425,
          0.0005733385914936662,
          0.003632101695984602,
          0.04331120848655701,
          0.021990448236465454,
          -0.03670739755034447,
          -0.00042182166362181306,
          0.012253453023731709,
          0.03990070894360542,
          0.0024901507422327995,
          -0.012855056673288345,
          0.008840722031891346,
          0.0391271635890007,
          -0.0029394186567515135,
          -0.012541578151285648,
          0.11581500619649887,
          0.03653007000684738,
          -0.034229639917612076,
          -0.04898059740662575,
          -0.061051011085510254,
          -0.047866340726614,
          0.05535275861620903,
          0.0021218047477304935,
          0.003218624275177717,
          -0.015251021832227707,
          -0.061466339975595474,
          -0.027496572583913803,
          -0.010716929100453854,
          -0.0180736742913723,
          0.06480150669813156,
          -0.025353332981467247,
          -0.009267850778996944,
          -0.013990544714033604,
          0.0236981064081192,
          -0.07163289934396744,
          0.07690617442131042,
          -0.0469457171857357,
          -0.05798359215259552,
          0.02409902773797512,
          0.020709076896309853,
          -0.03414308652281761,
          -0.007911814376711845,
          -0.08938626945018768,
          -0.0026701767928898335,
          -0.018728313967585564,
          -0.02544541470706463,
          0.020955553278326988,
          0.011785591952502728,
          -0.004800451919436455,
          0.0476885549724102,
          -0.030465850606560707,
          0.030921781435608864,
          0.03139482066035271,
          0.003943542949855328,
          -0.030665935948491096,
          0.013296547345817089,
          -0.0037020884919911623,
          0.05560411512851715,
          -0.03232759237289429,
          -0.0617397204041481,
          -0.11148680746555328,
          0.0017801295034587383,
          -0.011717995628714561,
          0.007487535942345858,
          -0.01847309060394764,
          -0.0063840774819254875,
          -0.0328817181289196,
          0.026118962094187737,
          -0.046843770891427994,
          0.01694394275546074,
          -0.03909511864185333,
          -0.009980500675737858,
          -0.038430456072092056,
          0.02546621672809124,
          0.032224204391241074,
          0.005393162835389376,
          -0.050821442157030106,
          0.007234846241772175,
          0.06652779877185822,
          0.010500271804630756,
          0.027863964438438416,
          -0.07973173260688782,
          -0.011019804514944553,
          0.01032407395541668,
          -0.021269412711262703,
          -0.011809836141765118,
          -0.00825507566332817,
          -0.02341492287814617,
          0.03270069882273674,
          0.009296480566263199,
          0.010335040278732777,
          0.03138342872262001,
          -0.04629075527191162,
          -0.05892137810587883,
          0.004119822755455971,
          0.037955548614263535,
          -0.07940956950187683,
          0.007933566346764565,
          0.007083024829626083,
          0.006803442724049091,
          0.0034164818935096264,
          -0.08080510050058365,
          0.0039877151139080524,
          -0.06063021719455719,
          -0.04800313711166382,
          -0.025860028341412544,
          -0.01775212772190571,
          -0.035892993211746216,
          -0.008384007029235363,
          0.028458748012781143,
          -0.043410155922174454,
          -0.06977907568216324,
          0.028323110193014145,
          -0.008070521987974644,
          -0.025710079818964005,
          0.019853556528687477,
          -0.018950555473566055,
          0.0675872415304184,
          -0.06996362656354904,
          0.045735448598861694,
          0.07886956632137299,
          0.063942089676857,
          -0.06759805977344513,
          0.013723949901759624,
          -0.02853401005268097,
          -0.0068545411340892315,
          -0.03187922015786171,
          -0.04385250434279442,
          0.02327265404164791,
          0.008295887149870396,
          0.04015824943780899,
          0.06957805156707764,
          0.011398566886782646,
          -0.027280190959572792,
          -0.05408240854740143,
          -0.00043159269262105227,
          0.051736969500780106,
          0.045602161437273026,
          -0.015227728523314,
          -0.08897598087787628,
          0.014253325760364532,
          -0.004250434227287769,
          0.009799539111554623,
          0.03563004359602928,
          -0.022493408992886543,
          0.011220197193324566,
          -0.04609444737434387,
          0.03449058160185814,
          -0.041286494582891464,
          -0.02226981148123741,
          -0.0748719647526741,
          -0.06816782057285309,
          0.01935243420302868,
          0.0228764358907938,
          -0.08155151456594467,
          0.06027746573090553,
          0.02319738082587719,
          0.09734684228897095,
          0.10110578685998917,
          -0.05299042910337448,
          0.007684952579438686,
          0.004950531758368015,
          -0.010119284503161907,
          0.08481493592262268,
          0.013511484488844872,
          -0.0441010445356369,
          0.0277870986610651,
          0.041546985507011414,
          0.01149437390267849,
          -0.07878021150827408,
          0.04133160039782524,
          -0.0052655963227152824,
          0.016773805022239685,
          -0.028545958921313286,
          -0.04859781265258789,
          0.03955667465925217,
          -0.01663198322057724,
          0.07596532255411148,
          0.017487049102783203,
          0.008171536959707737,
          -0.052418772131204605,
          -0.013378782197833061,
          -0.009479704312980175,
          -0.0007063785451464355,
          0.26154184341430664,
          -0.0385637991130352,
          0.011128860525786877,
          -0.004635007120668888,
          -0.011810441501438618,
          0.048778798431158066,
          -0.0243619866669178,
          0.020704371854662895,
          0.0022133952006697655,
          0.06978723406791687,
          0.017376473173499107,
          -0.030353674665093422,
          -0.04446488246321678,
          0.044331442564725876,
          -0.02413371577858925,
          -0.00823946762830019,
          -0.035923633724451065,
          0.04236219450831413,
          -0.09389194846153259,
          0.04962058365345001,
          0.05697852745652199,
          0.008276895619928837,
          0.05174928903579712,
          0.012788716703653336,
          -0.005836961790919304,
          0.038880959153175354,
          -0.05692218244075775,
          0.05116690695285797,
          -0.010239290073513985,
          0.03644115477800369,
          0.024449463933706284,
          -0.0019157988717779517,
          0.09373518824577332,
          0.03881110996007919,
          -0.025327002629637718,
          0.04773510992527008,
          -0.03622589632868767,
          -0.01944630779325962,
          0.052589718252420425,
          0.03533851355314255,
          -0.011007347144186497,
          0.03207625821232796,
          -0.006127730943262577,
          0.008253865875303745,
          -0.0018610833212733269,
          -0.020067190751433372,
          -0.04898030683398247,
          -0.010085230693221092,
          0.046487804502248764,
          -0.0596037432551384,
          0.01782756671309471,
          -0.04750187322497368,
          0.013232070952653885,
          -0.05095481872558594,
          -0.07281225174665451,
          0.015819529071450233,
          -0.03083634562790394,
          0.04064980149269104,
          -0.03437497839331627,
          -0.036670491099357605,
          -0.008957247249782085,
          -0.07534322142601013,
          0.04435199126601219,
          0.003450630232691765,
          -0.025743238627910614,
          0.00664302846416831,
          -0.023962385952472687,
          0.037971749901771545,
          -0.02588656358420849,
          -0.029760999605059624,
          0.041933324187994,
          0.018939005210995674,
          -0.011547504924237728,
          -0.015442168340086937,
          -0.01862364076077938,
          0.037669725716114044,
          0.03455176204442978,
          -0.02791053056716919,
          -0.04232444614171982,
          -0.0433754026889801,
          0.06988009065389633,
          0.08142068237066269,
          -0.03789113834500313,
          0.029864883050322533,
          0.023072775453329086,
          -0.008072594180703163,
          0.011003703810274601,
          -0.023039473220705986,
          0.054446253925561905,
          -0.036166198551654816,
          8.198742580134422e-05,
          0.029674958437681198,
          -0.0007153802434913814,
          0.028721900656819344,
          -0.03648379445075989,
          0.0068538193590939045,
          0.036827586591243744,
          0.013976208865642548,
          -0.0026873592287302017,
          -0.038554299622774124,
          0.0017963203135877848,
          0.0019397250143811107
        ]
      },
      "type": "document"
    },
    {
      "id": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "properties": {
        "page_content": "Fine-Tuning distilBERT for Enhanced Sentiment Classification\nSarah Ling\nMarkvilleSecondarySchool,Ontario,L3P7P5,Unionville,Canada\nAbstract:This research examines the fine-\ntuning of the DistilBERT model for sentiment\nclassification using the IMDB dataset of\n50,000 movie reviews. Sentiment analysis is\nvital in natural language processing (NLP),\nproviding insights into emotions and opinions\nwithin textual data. We compare the fine-\ntuned DistilBERT and LLaMA 3 models,\nfocusing on their ability to classify reviews as\npositive or negative. Through few-shot\ntraining on the dataset, our findings reveal\nthat while LLaMA 3 8B excels in capturing\ncomplex sentiments, DistilBERT-base-\nuncased offers a more efficient solution for\nsimpler tasks. The results underscore the\neffectiveness of fine-tuning. This paper\ncontributes to optimizing sentiment analysis\nmodels and suggests future research\ndirections, including hybrid models and\nadvanced training techniques for improved\nperformance across diverse contexts.\nKeywords: Sentiment Classification; Fine-\nTuning; Natural Language Processing; Large\nLanguage Models; Text Classification;\nMachine Learning; Transformer Models\n 1. Introduction\nSentiment analysis has been an established area\nofresearch innatural language processing (NLP)\nthat studies people’s sentiments, opinions,\nemotions, etc. through computational\nmethods[4][7].Thisfieldhasgainedsignifi-cant\ninterestinbothacademia andindustryfieldsdue\nto its useful applications in analyzing customer\nfeedback,decision-making,andproductcreation.\nSentiment classification can be defined as the\nprocedure of assigning predetermined sentiment\nclasses (positive, negative) depending on the\nemotional tone of a message through analyzing\ntext. In NLP this task is widely used to\ndetermine the polarity of opinions expressed in\ntext. In recent years, large language models\n(LLMs)havebeenpopularinvariousNLPtasks,\nand a deeper understanding of human emotions\nthrough sentiment classification is an important\nstepping stone towards developing artificial\nintelligence[1]\nRecent work shows that models such as BERT\n[2] and LLaMA [11] perform well in general\nsentiment analysis tasks but still struggle with\nnuancedorstructuredsentimenttasks,especially\nwhen more refined emotional or opinion-based\ndistinctions are required [9]. Despite\nadvancements in LLMs, there are challenges in\napplying them to complex sentiment tasks,\nincluding identifying subtle emotions and\nhandling domain-specific contexts [5, 7]. We\npropose comparing the fine-tuned DistilBERT\nand LLaMA 3 models, evaluating their\nperformance on datasets for sentiment analysis.\nDistilBERT offers computational efficiency,\nwhile LLaMA 3 leverages a larger architecture\nfor complex tasks. This allows for a practical\nassessmentoftrade-offsbetweenmodelsizeand\nperformance.This paper compares the\nperformance of DistilBERT and LLaMA 3 in\nsentiment analysis using few-shot training. We\nfine-tune both models on domain-specific\ndatasets, including the IMDB Kaggle movie\nreview dataset. In this research, we fine-tune\nboth models, asking them to classify reviews as\npositive or negative. We test both models in\nzero-shot and fine-tuning scenarios to evaluate\ntheirgeneralizationacrossdifferentdomains.\n",
        "themes": [
          "Fine-Tuning",
          "DistilBERT",
          "Sentiment Classification",
          "Natural Language Processing (NLP)",
          "Large Language Models (LLMs)",
          "IMDB Dataset",
          "LLaMA 3",
          "Few-Shot Training",
          "Model Efficiency",
          "Text Classification"
        ],
        "entities": [
          "Sarah Ling",
          "Markville Secondary School",
          "Ontario",
          "Unionville",
          "Canada",
          "DistilBERT",
          "IMDB",
          "LLaMA 3",
          "BERT",
          "Kaggle"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "1a65cb75-059f-484a-ba78-42b6880ff472",
      "properties": {
        "page_content": "1.1 Background Dataset\nAn IMDB Dataset of 50,000 Movie Reviews is\nused to train our models. This is a dataset for\nbinary sentiment classification containing\nsubstantiallymoredatathanpreviousbenchmark\ndatasets. This dataset provides 25,000 highly\npolar movie reviews for training and 25,000 for\ntesting[6].LargeLanguageModels(LLMs)\nA large language model (LLM) is a machine\nlearningmodeldesignedto processand generate\nhuman language text. Built on transformer\narchitectures [10], LLMs are trained on\nextensive datasets using deep learning\ntechniques to understand relationships among\ncharacters, words, and sentences. They analyze\npatterns in unstructured data to identify\n108\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
        "themes": [
          "IMDB Dataset",
          "Movie Reviews",
          "Binary Sentiment Classification",
          "Training Data",
          "Testing Data",
          "Large Language Models (LLMs)",
          "Machine Learning",
          "Transformer Architectures",
          "Deep Learning",
          "Natural Language Processing"
        ],
        "entities": [
          "IMDB Dataset",
          "Movie Reviews",
          "Large Language Models",
          "LLMs",
          "Journal of Big Data and Computing",
          "ISSN",
          "STEMM Institute Press"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "properties": {
        "page_content": "grammatical rules, semantics, and contextual\nnuances.\nThrough probabilistic methods, LLMs predict\nand generate coherent text without requiring\nhuman supervision during training. This enables\nthem to perform a variety of natural language\nprocessing tasks, such as summariza- tion,\ntranslation, and sentiment analysis. The\ntransformer-based approach significantly\nenhances their ability to process language\nefficiently and accurately, making LLMs a\npowerfultoolforadvancingAIapplications.\n 1.2 Models\nThe first model used is the Meta LLaMa 3 8B,\nwhich is the next generation of Meta’s state-of-\nthe-art open-source large language model[11].\nLlama3 comesin configurationsrangingfrom8\nbillion to 70 billion parameters, making it a\nhighly scalable and powerful model capable of\nprocessing large amounts of data for diverse\napplications[3]. It is designed to compete with\nand surpass existing models in terms of\nperformance across various tasks, including\nlanguage understanding, coding, reasoning, and\nmore.\nThe second model used is distilbert-base-\nuncasedwhichisadistilledversionoftheBERT\nbase model [8]. DistilBERT is a transformers\nmodel,smallerandfasterthanBERT,whichwas\npretrained on the same corpus in a self-\nsupervised fashion, using the BERT base model\nasateacher.Themodelwastrainedtoreturnthe\nsame probabilities as the BERT base model.\nDistilBERT has about 66 million parameters,\nwhich makes it smaller and more lightweight\nthantheoriginalBERTmodelandisdesignedto\nretain around 97% of BERT’s performance\nwhilebeing60%smallerandfaster.\nThe DistilBERT model was selected for this\ninvestigation since it is primarily used for tasks\nthat requiretransformermodelsbut requirefine-\ntuning. It has been widely used for fine-tuning\ntasks that use the whole sentence to make\ndecisions, such as sequence classification, token\nclassification,orquestionanswering.\nBoth models’ checkpoints hosted on\nhuggingfaceareusedfortheinference.\n 1.3 Fine-Tuning\nLanguage models are often further trained via a\nprocess named fine-tuning. Fine-tuning in\nmachine learning is the process of adapting a\npre-trainedmodelforspecifictasksorusecases.\nIt has become a fundamental deep learning\ntechnique, particularly in the training process of\nfoundation models used for generative AI. Fine-\ntuning for specific tasks such as interpreting\nquestions and generating responses, or\ntranslatingtextfromonelanguagetoanotherare\ncommon. In this investigation, we finetune the\ndistilbert-base-uncased and Llama 3 models and\ncompare their performance for the task of\nsentimentclassification.\n",
        "themes": [
          "Grammatical rules",
          "Semantics",
          "Contextual nuances",
          "Probabilistic methods",
          "Natural language processing tasks",
          "Transformer-based approach",
          "Large Language Models (LLMs)",
          "Model architectures (LLaMa 3, DistilBERT)",
          "Fine-tuning",
          "Sentiment classification"
        ],
        "entities": [
          "Meta",
          "LLaMa 3 8B",
          "Meta",
          "Llama3",
          "DistilBERT",
          "BERT",
          "DistilBERT",
          "BERT",
          "huggingface",
          "Llama 3"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "e66ec817-72cb-406b-acc2-482d3febd6b3",
      "properties": {
        "page_content": "2. System Design\n 2.1 Overview\nFigure 1. System Overview\nFigure1providesanoutlineofthestepstakento\nfinetuneamodel.\n 2.2 Data Pre-Processing\nThe firststage of our processinvolves preparing\nthe IMDB dataset for use with the transformer\nmodel. Since transformer-based models like\nDistilBERT require tokenized input, we\npreprocess the data by converting text into a\nformatthatthemodelcaninterpret.\nTokenization: Each movie review is tokenized\nusing a pre-trained tokenizer from the\nDistilBERT model. This tokenizer splits text\ninto subword units and converts them into\ninteger token IDs. The tokenizer also handles\npunctuation, case normalization (lowercasing),\nand truncation to ensure that the input sequence\nfitsthemodel’smaximuminputlength.\nPadding:Toensureuniforminputlengthsduring\nbatching, tokenized sequences are padded.\nPaddingaddsspecialtokenstoshortersequences\nso that they match the maximum sequence\nlengthineachbatch.Oncetokenizedandpadded,\nthe preprocessed dataset is ready for input into\nthemodel.\n3. Model Definition\nThe distilBERT model is initialized with\nweightspre-trainedonalargecorpusoftextdata,\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n109\nCopyright @ STEMM Institute Press\nhttp://www.stemmpress.com",
        "themes": [
          "System Design",
          "Data Pre-Processing",
          "Tokenization",
          "Padding",
          "Model Definition",
          "DistilBERT",
          "Transformer Model",
          "IMDB Dataset",
          "Fine-tuning",
          "Pre-trained Weights"
        ],
        "entities": [
          "IMDB",
          "DistilBERT",
          "Journal of Big Data and Computing",
          "STEMM Institute Press"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "properties": {
        "page_content": "allowing it to already understand general\nlanguage features. We then modify the pre-\ntrained model for the specific task of sentiment\nclassificationby:\nAdding a Classification Layer: The pre-trained\nDistilBERT model outputs a contextualized\nrepresentation for each token in the input\nsequence.A fully connectedlayer (classification\nhead) is added on top of the model, with two\noutputnodescorrespondingtothetwosentiment\nlabels: positive and negative.Load model: We\nload pre-trained DistilBERT model then add the\nclassificationlayerwith2outputlabels:positive,\nnegative.Label Mapping: A mapping between\nsentiment labels and numerical IDs is defined\n(e.g., 0 for negative and 1 for positive). This\nensures that predictions made by the model are\ninterpretable.\n 3.1 Finetuning\nThe fine-tuning process involves training the\npre-trained DistilBERT model on the IMDB\ndataset while updating its weights to specialize\nin sentiment analysis. Fine-tuning is a crucial\nstep because it allows the model to transfer its\ngeneral language understanding to the specific\ntask of classifying movie reviews. We set the\nseed to a random fixed number to ensure fair\ncomparison and reproductibility. A smaller\nsubset of 3000 reviews was created for training\nand testing for the distilBERT model. A subset\nof 100 reviews was used for the LLaMa 3 8B\nmodel due to constraints on available RAM for\nthe free version of Google Colab. This smaller\nsubset may affect the finetuning capabilities of\nthe LLaMa 3 8B model since it is not given as\nmuch training data as the distilBERT model.\nThisdifferenceintrainingdatawillbetakeninto\nconsideration for comparing the models’\nperformance.\nTraining: The model is trained using a\nsupervised learning approach. The tokenized\nIMDB dataset is split into training and\nevaluation sets. During training, the model\nlearns to minimize the classification error by\nadjusting its weights via backpropagation. A\nlearning rate scheduler and optimizer (AdamW)\nare used to ensure smooth convergence and\npreventoverfitting.\n 3.2 Training Parameters Were Configured\n• output_dir:Directorytosavethemodel.\n• num_train_epochs:Numberoftraining\nepochs,setto2.\n• per_device_train_batch_size:Batchsizeper\ndevice,setto16.•learning_rate:Setto2×\n10−5\n• weight_decay:Setto0.01\n• optimizer:PagedAdamWwith32-bit\nprecision.\nAfter fine-tuning, the model’s performance\nimproved significantly, achieving an overall\naccuracy of 0.88. This result demonstrates the\neffectiveness of the fine-tuning process. By the\nend of the fine-tuning stage, the model is well-\nadjusted to the task of sentiment classification\nand can be used to make predictions on unseen\nmoviereviews.\n",
        "themes": [
          "Pre-trained models",
          "Fine-tuning",
          "Sentiment classification",
          "DistilBERT",
          "LLaMa 3 8B",
          "Transfer learning",
          "Supervised learning",
          "Model architecture",
          "Training parameters",
          "Model evaluation"
        ],
        "entities": [
          "DistilBERT",
          "IMDB",
          "LLaMa 3 8B",
          "Google Colab",
          "AdamW",
          "Classification Layer",
          "Classification Head",
          "PagedAdamW",
          "Europe",
          "Asia"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "properties": {
        "page_content": "4. Evaluation\nThis section presents the experimental results\nobtained from evaluating the model after fine-\ntuning. The perfor- mance was assessed using\nprecision, recall, and F1-score metrics for each\nsentimentclass(positiveandnegative)aswellas\ntheoverallaccuracyofthemodel.\n 4.1 Performance\nThe confusion matrixes below provides a\nbreakdown of the model’s predictions across all\nsentiment classes, as shown in Figures 2 and 3.\nThis analysis helps in identifying common\nmisclassifications and understanding the\nmodel’sstrengthsandweaknesses.\nFigure 2. DistilBERT-Base-Uncased\nFigure 3. LLaMa-3-8B\n110\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
        "themes": [
          "Evaluation",
          "Experimental results",
          "Fine-tuning",
          "Performance metrics",
          "Precision",
          "Recall",
          "F1-score",
          "Sentiment classification",
          "Confusion matrix",
          "Model misclassifications"
        ],
        "entities": [
          "DistilBERT-Base-Uncased",
          "LLaMa-3-8B",
          "Journal of Big Data and Computing",
          "STEMM Institute Press"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "properties": {
        "page_content": "The evaluation of the model’s performance was\nbased on standard metrics including precision,\nrecall, and F1-score. These metrics were\ncomputed for each sentiment class (positive and\nnegative) as well as for the overall model\nperformance. The formulas for these metrics are\nasfollows.\nTP+TN\nTP+TN+FN+FP\nTP\nTP+FP\nTP\nTP+FN\nPrecision×Recall\nPrecision+Recall\n Table 1. Comparison of Model Performance\nModel Accuracy Precision Recall F1Score\ndistilBERT-base-\nuncased\nMeta-Llama-3-8B\n0.88\n0.66\n0.86075\n0.63793\n0.90666\n0.74000\n0.88311\n0.68518\n 4.2 Latency\nDistilBERT-base-uncased is much faster, with\nlow latency suitable for simple real-time\napplications such as sentiment classification due\nto its smaller size and efficient architecture. For\nthis task of sentiment classification, it took\napproximately2:23mintoruneachepoch.\nMeta-Llama-3-8B provides more powerful\ncapabilities and nuanced understanding but\ncomeswithhigher\nlatency, making it more suitable for applications\nwhere processingspeed is less critical compared\ntooutputquality.\n 5. Discussion\nThis study compared the performance of\nDistilBERT and LLaMA 3 on sentiment\nclassification using the IMDB dataset. Our\nfindings indicate that both models have their\nstrengths and weaknesses, and understanding\nthese can provide insights into optimizing\nsentiment analysis systems in practical\napplications.\nModel Performance:The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset.\nGeneralization Across Domains: The models\nperformed well on the IMDB dataset but may\nstruggle to generalize to different domains,\nwhere sentiment expressions vary significantly.\nImplementingdomainadaptationtechniquesand\nfine-tuning on diverse datasets could enhance\nmodel robustness. Highly polar movie reviews\nwere used, and both models’ ability to detect\nnuanced sentiments was not tested. Future work\ncould explore specialized training datasets to\nimprovetherecognitionofsubtlesentiments.\nPromising avenues for future research include\ndeveloping hybrid models that combine the\nstrengths of both LLaMA 3 and DistilBERT.\nEnhancing training techniques, such as few-shot\nand zero-shot learning, will also be essential for\nimproving performance across diverse contexts.\nAddressing the limitations identified in this\nstudy will be crucial for creating more accurate\nand efficient sentiment classification systems\nthat effectively meet user needs. These findings\nunderscore the importance of model selection\nbasedonapplicationrequirements.\n",
        "themes": [
          "Model Performance Evaluation",
          "Evaluation Metrics (Precision, Recall, F1-score, Accuracy)",
          "Sentiment Classification",
          "Model Comparison (DistilBERT vs. LLaMA 3)",
          "Latency and Computational Efficiency",
          "Generalization and Domain Adaptation",
          "Model Strengths and Weaknesses",
          "Future Research Directions",
          "Application Requirements and Model Selection",
          "IMDB Dataset"
        ],
        "entities": [
          "DistilBERT",
          "LLaMA 3",
          "IMDB",
          "Meta-Llama-3-8B",
          "distilBERT-base-uncased",
          "F1Score",
          "Precision",
          "Recall",
          "Accuracy",
          "Table 1"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "properties": {
        "page_content": "Future Work: Several avenues for future\nexplorationremain:\n• Leveraging hybrid models combining\nefficiencyandnuancedunderstanding[5].\n• Expanding experiments to include\ndomain-specific datasets to enhance\ngeneralization.\n• Investigating advanced fine-tuning\ntechniques, such as adapters and LoRA\nlayers,forfurtheroptimization[11].\n 6. Conclusion\nIn this study, we conduct an evaluation of\nsentiment classification by comparing the\nperformance of a large language model and a\nsmall language model. The DistilBERT model\nhad higher accuracy, precision, recall, and F1\nscore, suggesting that it outperformed LLaMa 3\non the task of sentiment analysis for our IMDB\ndataset. These findings suggest that model size\nalone does not guarantee better performance,\nemphasizing the importance of selecting the\nappropriate model for specific tasks. Future\nworkcouldexplore theinfluenceof datasetsize,\nfine-tuning strategies, and domain-specific\ntraining to further optimize sentiment\nclassificationmodels.\nReferences\n[1] Sébastien Bubeck, Varun Chandrasekaran,\nRonen Eldan, Johannes Gehrke, Eric\nHorvitz,EceKamar,PeterLee,YinTatLee,\nYuanzhi Li, Scott Lundberg, et al. 2023.\nSparks of artificial general intelligence:\nEarlyexperimentswithgpt-4.arXivpreprint\narXiv:2303.12712(2023).\nAccuracy=\nRecall=\nPrecision=\nF1=2 ×\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\n111\nCopyright @ STEMM Institute Press\nhttp://www.stemmpress.com",
        "themes": [
          "Future work",
          "Hybrid models",
          "Domain-specific datasets",
          "Generalization",
          "Fine-tuning techniques",
          "Adapters",
          "LoRA layers",
          "Sentiment classification",
          "Model comparison",
          "Model size vs. performance"
        ],
        "entities": [
          "DistilBERT",
          "LLaMa 3",
          "IMDB",
          "Sébastien Bubeck",
          "Varun Chandrasekaran",
          "Ronen Eldan",
          "Johannes Gehrke",
          "Eric Horvitz",
          "Ece Kamar",
          "Peter Lee"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "properties": {
        "page_content": "[2]JacobDevlin,Ming-WeiChang,KentonLee,\nand Kristina Toutanova. 2019. BERT: Pre-\ntraining of Deep Bidirectional Transformers\nforLanguageUnderstanding.InProceedings\nof the 2019 Conference of the North\nAmerican Chapter of the Association for\nComputational Linguistics: Human\nLanguage Technologies, Volume 1 (Long\nand Short Papers), Jill Burstein, Christy\nDoran, and Thamar Solorio\n(Eds.).Association for Computational\nLinguistics, Minneapolis, Minnesota,4171–\n4186.https://doi.org/10.18653/v1/N19-1423\n[3]Diefan Lin, Yi Wen, Weishi Wang, and Yan\nSu. 2024. Enhanced Sentiment Intensity\nRegression Through LoRA Fine-Tuning on\nLlama 3.IEEE Access 12 (2024), 108072–\n108087.\nhttps://doi.org/10.1109/ACCESS.2024.3438\n353\n[4] Bing Liu. 2020. Sentiment analysis: Mining\nopinions, sentiments, and emotions.\nCambridgeuniversitypress.\n[5]HaochenLiu,Sai KrishnaRallabandi,Yijing\nWu, Parag Pravin Dakle, and Preethi\nRaghavan. 2023. Self-training Strategies for\nSentiment Analysis: An Empirical Study.\narXivpreprintarXiv:2309.08777(2023).\n[6]AndrewL.Maas,RaymondE.Daly,PeterT.\nPham, Dan Huang, Andrew Y. Ng, and\nChristopher Potts. 2011. Learning Word\nVectors for Sentiment Analysis. In\nProceedings of the 49th Annual Meeting of\nthe Association for Computational\nLinguistics: HumanLanguage Technologies,\nDekang Lin, Yuji Matsumoto, and Rada\nMihalcea (Eds.). Association for\nComputational Linguistics, Portland,\nOregon, USA, 142–150.\nhttps://aclanthology.org/P11-1015Soujanya\nPoria, Devamanyu Hazarika, Navonil\nMajumder, and Rada Mihalcea. 2023.\nBeneath the Tip of the Iceberg: Current\nChallenges and New Directions in\nSentiment Analysis Research. IEEE\nTransactions on Affective Computing 14, 1\n(2023), 108–132.\nhttps://doi.org/10.1109/TAFFC.2020.30381\n67\n[7] V Sanh. 2019. DistilBERT, a distilled\nversion of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint\narXiv:1910.01108(2019).\n[8] Amira Samy Talaat. 2023. Sentiment\nanalysis classification system using hybrid\nBERT models. Journal of Big Data 10, 1\n(2023),110.\n[9] A Vaswani. 2017. Attention is all you need.\nAdvances in Neural Information Processing\nSystems(2017).\n[10] Peiyuan Zhang, Guangtao Zeng, Tianduo\nWang, and Wei Lu. 2024. Tinyllama: An\nopen-source small language model. arXiv\npreprintarXiv:2401.02385(2024).\n112\nJournal of Big Data and Computing (ISSN: 2959-0590) Vol. 2 No. 4, 2024\nhttp://www.stemmpress.com\nCopyright @ STEMM Institute Press",
        "document_metadata": {
          "producer": "Adobe Acrobat Pro 10.0.0",
          "creator": "Adobe Acrobat Pro 10.0.0",
          "creationdate": "2025-01-14T22:56:27+08:00",
          "author": "",
          "comments": "",
          "company": "",
          "keywords": "",
          "moddate": "2025-01-14T23:00:11+08:00",
          "subject": "",
          "title": "",
          "trapped": "/False",
          "source": "doc/Fine-Tuning_distilBERT_for_Enhanced_Sentiment_Clas.pdf",
          "total_pages": 5,
          "page": 4,
          "page_label": "5"
        },
        "headlines": [
          "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "Enhanced Sentiment Intensity Regression Through LoRA Fine-Tuning on Llama 3",
          "Sentiment analysis: Mining opinions, sentiments, and emotions",
          "Self-training Strategies for Sentiment Analysis: An Empirical Study",
          "Learning Word Vectors for Sentiment Analysis"
        ],
        "summary": "The text lists several key research papers in natural language processing and sentiment analysis. It includes the seminal BERT model from 2019 and its distilled version, DistilBERT. Other foundational works mentioned are the \"Attention is All You Need\" paper and early sentiment analysis research from 2011. Recent studies cover enhanced sentiment intensity regression using LoRA on Llama 3 and hybrid BERT models. It also references a sentiment analysis textbook and papers on self-training strategies and current research challenges. Newer, efficient models like TinyLlama are also noted. The compilation appears to be a reference list for a journal article in big data and computing.",
        "keyphrases": [
          "BERT",
          "Sentiment Analysis",
          "Deep Bidirectional Transformers",
          "LoRA Fine-Tuning",
          "Attention is all you need"
        ],
        "summary_embedding": [
          -0.03154522553086281,
          -0.004516022279858589,
          0.022067641839385033,
          -0.04294547066092491,
          -0.009079270996153355,
          -0.0733707994222641,
          -0.07030638307332993,
          0.017882654443383217,
          0.057423461228609085,
          -0.01661800779402256,
          0.10137233883142471,
          -0.3460078537464142,
          -0.007263810373842716,
          0.054887183010578156,
          -0.04785090312361717,
          -4.160732714808546e-05,
          0.019564930349588394,
          0.00025647785514593124,
          0.027077293023467064,
          -0.011632174253463745,
          0.04867494851350784,
          -0.04436291381716728,
          -0.013536735437810421,
          0.04051681607961655,
          -0.008282260969281197,
          0.0017883788095787168,
          -0.0016733030788600445,
          0.04395689442753792,
          -0.01784592494368553,
          -0.062231458723545074,
          -0.029740162193775177,
          0.02045789174735546,
          0.019798893481492996,
          -0.018033482134342194,
          -0.046064671128988266,
          -0.07379364967346191,
          0.015742849558591843,
          -0.04856804758310318,
          -0.008269940502941608,
          0.06905600428581238,
          0.0037866777274757624,
          0.007615196518599987,
          0.008825923316180706,
          0.007724345661699772,
          -0.022399861365556717,
          -0.07263185828924179,
          -0.006511190440505743,
          0.01639970764517784,
          -0.0426705963909626,
          -0.0111720385029912,
          -0.016785921528935432,
          0.05593792349100113,
          0.0567881278693676,
          0.009729363955557346,
          0.04513886570930481,
          -0.04941364750266075,
          0.050961222499608994,
          0.0496453158557415,
          -0.09875577688217163,
          -0.018210381269454956,
          0.06156231835484505,
          0.0026481179520487785,
          -0.011111128143966198,
          0.04555872082710266,
          -0.003774058073759079,
          0.020948830991983414,
          -0.001669651479460299,
          0.03284165635704994,
          -0.029010804370045662,
          0.027218114584684372,
          -0.03827345371246338,
          0.010824383236467838,
          -0.02053864859044552,
          -0.04976489767432213,
          0.0298474058508873,
          -0.058304570615291595,
          0.013206574134528637,
          -0.06668329983949661,
          -0.011736230924725533,
          -0.005263631232082844,
          -0.07447867840528488,
          -0.017474910244345665,
          -0.01661258190870285,
          0.10529577732086182,
          -0.0023553541395813227,
          -0.04460969567298889,
          0.008711225353181362,
          -0.03152763843536377,
          -0.0107384342700243,
          -0.10661021620035172,
          -0.006855085492134094,
          -0.0016853566048666835,
          0.018630050122737885,
          0.06223639100790024,
          -0.01944437064230442,
          0.03323651850223541,
          -0.08987804502248764,
          0.07101565599441528,
          0.0013229901669546962,
          -0.016728561371564865,
          -0.0015608874382451177,
          -0.05328447371721268,
          -0.021592475473880768,
          -0.03229771926999092,
          -0.05147302895784378,
          0.07691144198179245,
          0.06272518634796143,
          -0.045487478375434875,
          0.01966470293700695,
          0.04508384317159653,
          -0.0065791006200015545,
          -0.02082723379135132,
          0.035202328115701675,
          0.013022304512560368,
          0.03561723977327347,
          -0.025715835392475128,
          0.011336931958794594,
          -0.013323566876351833,
          0.022748369723558426,
          0.05591895803809166,
          0.08888749033212662,
          0.08731121569871902,
          0.035277001559734344,
          -0.017266174778342247,
          -0.060829855501651764,
          -0.03732201084494591,
          -0.018395844846963882,
          -0.01789197139441967,
          0.017874648794531822,
          -0.016109907999634743,
          0.012480485253036022,
          -0.0715055987238884,
          0.02979261241853237,
          0.033385906368494034,
          0.031351085752248764,
          0.02913714572787285,
          0.0235432256013155,
          -0.03371475636959076,
          -0.04777112975716591,
          -0.042190197855234146,
          0.04071531817317009,
          -0.0397530123591423,
          -0.05616248399019241,
          -0.05908886343240738,
          -0.03987637907266617,
          0.05960604548454285,
          -0.026457011699676514,
          -0.0015567054506391287,
          0.009545824490487576,
          0.03154681250452995,
          0.01380577590316534,
          -0.04529254883527756,
          0.007482264190912247,
          -0.05150849372148514,
          -0.03747711703181267,
          0.05763619393110275,
          0.0031152674928307533,
          -0.017113564535975456,
          -0.03375561162829399,
          -0.02734297700226307,
          0.04461786150932312,
          -0.04031038284301758,
          0.010011173784732819,
          0.023234060034155846,
          -0.044003088027238846,
          0.0004908917471766472,
          0.06664823740720749,
          0.0185816939920187,
          -0.08969800919294357,
          0.0028705333825200796,
          0.015682583674788475,
          -0.06106720492243767,
          0.03420118987560272,
          -0.00631285272538662,
          -0.01944342814385891,
          -0.015860913321375847,
          -0.02565573900938034,
          -0.05917264148592949,
          -0.0019305399619042873,
          -0.08671284466981888,
          -0.008565312251448631,
          -0.020220346748828888,
          0.020426511764526367,
          0.027116218581795692,
          0.024004939943552017,
          0.044675637036561966,
          -0.035638753324747086,
          -0.036515478044748306,
          0.03394642099738121,
          -0.0047765085473656654,
          0.0725712701678276,
          0.015504402108490467,
          0.008083293214440346,
          0.013741211965680122,
          0.046635691076517105,
          0.022833917289972305,
          0.04574567824602127,
          0.032059915363788605,
          -0.010173814371228218,
          -0.0385935977101326,
          -0.027685271576046944,
          0.020079035311937332,
          0.10134336352348328,
          0.013098220340907574,
          0.008143036626279354,
          -0.0337657555937767,
          -0.030025506392121315,
          -0.035035327076911926,
          -0.018193040043115616,
          -0.07426586747169495,
          -0.016659226268529892,
          -0.021471092477440834,
          -0.05284058675169945,
          0.03995495289564133,
          -0.04816168546676636,
          0.011197434738278389,
          -0.019165508449077606,
          -0.021536076441407204,
          -0.003973647020757198,
          -0.013723594136536121,
          -0.032291628420352936,
          -0.014741715975105762,
          0.08540431410074234,
          -0.04057827591896057,
          0.010618293657898903,
          -0.024217046797275543,
          0.0025924076326191425,
          0.0005733385914936662,
          0.003632101695984602,
          0.04331120848655701,
          0.021990448236465454,
          -0.03670739755034447,
          -0.00042182166362181306,
          0.012253453023731709,
          0.03990070894360542,
          0.0024901507422327995,
          -0.012855056673288345,
          0.008840722031891346,
          0.0391271635890007,
          -0.0029394186567515135,
          -0.012541578151285648,
          0.11581500619649887,
          0.03653007000684738,
          -0.034229639917612076,
          -0.04898059740662575,
          -0.061051011085510254,
          -0.047866340726614,
          0.05535275861620903,
          0.0021218047477304935,
          0.003218624275177717,
          -0.015251021832227707,
          -0.061466339975595474,
          -0.027496572583913803,
          -0.010716929100453854,
          -0.0180736742913723,
          0.06480150669813156,
          -0.025353332981467247,
          -0.009267850778996944,
          -0.013990544714033604,
          0.0236981064081192,
          -0.07163289934396744,
          0.07690617442131042,
          -0.0469457171857357,
          -0.05798359215259552,
          0.02409902773797512,
          0.020709076896309853,
          -0.03414308652281761,
          -0.007911814376711845,
          -0.08938626945018768,
          -0.0026701767928898335,
          -0.018728313967585564,
          -0.02544541470706463,
          0.020955553278326988,
          0.011785591952502728,
          -0.004800451919436455,
          0.0476885549724102,
          -0.030465850606560707,
          0.030921781435608864,
          0.03139482066035271,
          0.003943542949855328,
          -0.030665935948491096,
          0.013296547345817089,
          -0.0037020884919911623,
          0.05560411512851715,
          -0.03232759237289429,
          -0.0617397204041481,
          -0.11148680746555328,
          0.0017801295034587383,
          -0.011717995628714561,
          0.007487535942345858,
          -0.01847309060394764,
          -0.0063840774819254875,
          -0.0328817181289196,
          0.026118962094187737,
          -0.046843770891427994,
          0.01694394275546074,
          -0.03909511864185333,
          -0.009980500675737858,
          -0.038430456072092056,
          0.02546621672809124,
          0.032224204391241074,
          0.005393162835389376,
          -0.050821442157030106,
          0.007234846241772175,
          0.06652779877185822,
          0.010500271804630756,
          0.027863964438438416,
          -0.07973173260688782,
          -0.011019804514944553,
          0.01032407395541668,
          -0.021269412711262703,
          -0.011809836141765118,
          -0.00825507566332817,
          -0.02341492287814617,
          0.03270069882273674,
          0.009296480566263199,
          0.010335040278732777,
          0.03138342872262001,
          -0.04629075527191162,
          -0.05892137810587883,
          0.004119822755455971,
          0.037955548614263535,
          -0.07940956950187683,
          0.007933566346764565,
          0.007083024829626083,
          0.006803442724049091,
          0.0034164818935096264,
          -0.08080510050058365,
          0.0039877151139080524,
          -0.06063021719455719,
          -0.04800313711166382,
          -0.025860028341412544,
          -0.01775212772190571,
          -0.035892993211746216,
          -0.008384007029235363,
          0.028458748012781143,
          -0.043410155922174454,
          -0.06977907568216324,
          0.028323110193014145,
          -0.008070521987974644,
          -0.025710079818964005,
          0.019853556528687477,
          -0.018950555473566055,
          0.0675872415304184,
          -0.06996362656354904,
          0.045735448598861694,
          0.07886956632137299,
          0.063942089676857,
          -0.06759805977344513,
          0.013723949901759624,
          -0.02853401005268097,
          -0.0068545411340892315,
          -0.03187922015786171,
          -0.04385250434279442,
          0.02327265404164791,
          0.008295887149870396,
          0.04015824943780899,
          0.06957805156707764,
          0.011398566886782646,
          -0.027280190959572792,
          -0.05408240854740143,
          -0.00043159269262105227,
          0.051736969500780106,
          0.045602161437273026,
          -0.015227728523314,
          -0.08897598087787628,
          0.014253325760364532,
          -0.004250434227287769,
          0.009799539111554623,
          0.03563004359602928,
          -0.022493408992886543,
          0.011220197193324566,
          -0.04609444737434387,
          0.03449058160185814,
          -0.041286494582891464,
          -0.02226981148123741,
          -0.0748719647526741,
          -0.06816782057285309,
          0.01935243420302868,
          0.0228764358907938,
          -0.08155151456594467,
          0.06027746573090553,
          0.02319738082587719,
          0.09734684228897095,
          0.10110578685998917,
          -0.05299042910337448,
          0.007684952579438686,
          0.004950531758368015,
          -0.010119284503161907,
          0.08481493592262268,
          0.013511484488844872,
          -0.0441010445356369,
          0.0277870986610651,
          0.041546985507011414,
          0.01149437390267849,
          -0.07878021150827408,
          0.04133160039782524,
          -0.0052655963227152824,
          0.016773805022239685,
          -0.028545958921313286,
          -0.04859781265258789,
          0.03955667465925217,
          -0.01663198322057724,
          0.07596532255411148,
          0.017487049102783203,
          0.008171536959707737,
          -0.052418772131204605,
          -0.013378782197833061,
          -0.009479704312980175,
          -0.0007063785451464355,
          0.26154184341430664,
          -0.0385637991130352,
          0.011128860525786877,
          -0.004635007120668888,
          -0.011810441501438618,
          0.048778798431158066,
          -0.0243619866669178,
          0.020704371854662895,
          0.0022133952006697655,
          0.06978723406791687,
          0.017376473173499107,
          -0.030353674665093422,
          -0.04446488246321678,
          0.044331442564725876,
          -0.02413371577858925,
          -0.00823946762830019,
          -0.035923633724451065,
          0.04236219450831413,
          -0.09389194846153259,
          0.04962058365345001,
          0.05697852745652199,
          0.008276895619928837,
          0.05174928903579712,
          0.012788716703653336,
          -0.005836961790919304,
          0.038880959153175354,
          -0.05692218244075775,
          0.05116690695285797,
          -0.010239290073513985,
          0.03644115477800369,
          0.024449463933706284,
          -0.0019157988717779517,
          0.09373518824577332,
          0.03881110996007919,
          -0.025327002629637718,
          0.04773510992527008,
          -0.03622589632868767,
          -0.01944630779325962,
          0.052589718252420425,
          0.03533851355314255,
          -0.011007347144186497,
          0.03207625821232796,
          -0.006127730943262577,
          0.008253865875303745,
          -0.0018610833212733269,
          -0.020067190751433372,
          -0.04898030683398247,
          -0.010085230693221092,
          0.046487804502248764,
          -0.0596037432551384,
          0.01782756671309471,
          -0.04750187322497368,
          0.013232070952653885,
          -0.05095481872558594,
          -0.07281225174665451,
          0.015819529071450233,
          -0.03083634562790394,
          0.04064980149269104,
          -0.03437497839331627,
          -0.036670491099357605,
          -0.008957247249782085,
          -0.07534322142601013,
          0.04435199126601219,
          0.003450630232691765,
          -0.025743238627910614,
          0.00664302846416831,
          -0.023962385952472687,
          0.037971749901771545,
          -0.02588656358420849,
          -0.029760999605059624,
          0.041933324187994,
          0.018939005210995674,
          -0.011547504924237728,
          -0.015442168340086937,
          -0.01862364076077938,
          0.037669725716114044,
          0.03455176204442978,
          -0.02791053056716919,
          -0.04232444614171982,
          -0.0433754026889801,
          0.06988009065389633,
          0.08142068237066269,
          -0.03789113834500313,
          0.029864883050322533,
          0.023072775453329086,
          -0.008072594180703163,
          0.011003703810274601,
          -0.023039473220705986,
          0.054446253925561905,
          -0.036166198551654816,
          8.198742580134422e-05,
          0.029674958437681198,
          -0.0007153802434913814,
          0.028721900656819344,
          -0.03648379445075989,
          0.0068538193590939045,
          0.036827586591243744,
          0.013976208865642548,
          -0.0026873592287302017,
          -0.038554299622774124,
          0.0017963203135877848,
          0.0019397250143811107
        ]
      },
      "type": "document"
    }
  ],
  "relationships": [
    {
      "id": "171a1164-9462-4f00-9d92-4a0891c998b4",
      "type": "child",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "3815d8af-7b7e-4fea-a845-e27e95ff4a59",
      "type": "child",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "1a65cb75-059f-484a-ba78-42b6880ff472",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "7dd3c025-bdfe-48bb-bfad-51de1f050008",
      "type": "next",
      "source": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "target": "1a65cb75-059f-484a-ba78-42b6880ff472",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "0773ac89-301c-4b7e-a21a-017426800626",
      "type": "child",
      "source": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "target": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "76a7f42a-422d-4939-bec3-a8d6882adb6a",
      "type": "child",
      "source": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "target": "e66ec817-72cb-406b-acc2-482d3febd6b3",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "ee030a6e-b0c6-4623-a7ea-a0262671e5fb",
      "type": "next",
      "source": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "target": "e66ec817-72cb-406b-acc2-482d3febd6b3",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "53ef195c-fa81-4df6-9ae2-7d750761271b",
      "type": "child",
      "source": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "target": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "0fd24a30-5262-4101-b2b8-1facdf6e248d",
      "type": "child",
      "source": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "target": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "f2861a58-6bc4-4c1a-80ab-4e02c51740d7",
      "type": "next",
      "source": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "target": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "7f61f726-ce5d-4c36-b88b-5d28084613f9",
      "type": "child",
      "source": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "target": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "60dc9378-7594-42d3-af4b-a76e016890b6",
      "type": "child",
      "source": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "target": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "706a8504-dadf-424e-a24e-f9e729454711",
      "type": "next",
      "source": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "target": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "e25c4bbd-d09c-4a98-907a-4f3ec3a282e8",
      "type": "summary_similarity",
      "source": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "target": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8794387707931791
      }
    },
    {
      "id": "c928a891-2f11-472c-b095-f222ac0d7b3d",
      "type": "summary_similarity",
      "source": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8498970201776278
      }
    },
    {
      "id": "1374bf67-4e98-477b-a7ef-cf317fc80357",
      "type": "summary_similarity",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8211110665463415
      }
    },
    {
      "id": "7d009376-fbf6-4345-9add-f7a71c913d7e",
      "type": "summary_similarity",
      "source": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "target": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8274297588972569
      }
    },
    {
      "id": "5f338e98-4d15-492b-ae2e-0ac48b36b3b3",
      "type": "summary_similarity",
      "source": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8498970201776278
      }
    },
    {
      "id": "8e1acf1e-4b08-4880-97c0-fe45ac421eee",
      "type": "summary_similarity",
      "source": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.870614365125125
      }
    },
    {
      "id": "2f7c13b3-5069-49ee-9825-deac1b993716",
      "type": "summary_similarity",
      "source": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.870614365125125
      }
    },
    {
      "id": "6e9f0d58-651b-4aba-80c0-fd9ed2d15280",
      "type": "summary_similarity",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "ef6303a9-9a1d-4e0d-a9ae-8c3f63915b53",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8950975872740163
      }
    },
    {
      "id": "5309113d-1f40-404f-a883-66fb57b68c96",
      "type": "summary_similarity",
      "source": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 1.0
      }
    },
    {
      "id": "a6110a5a-c7f3-4f34-a4b7-8e83bfff7053",
      "type": "summary_similarity",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8884926917392532
      }
    },
    {
      "id": "bc0623d0-87a7-4894-a0d7-979f8a9cc52d",
      "type": "summary_similarity",
      "source": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.839570825837794
      }
    },
    {
      "id": "db6a4079-153e-4d8f-9932-e2715075fc72",
      "type": "summary_similarity",
      "source": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.839570825837794
      }
    },
    {
      "id": "c6e25de5-b94b-4a43-a9fc-0adf4914dd39",
      "type": "summary_similarity",
      "source": "5bac1ff6-3a82-4426-a391-e7f4dab806e3",
      "target": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8683425889864114
      }
    },
    {
      "id": "fd38af66-0874-43fd-bad3-7653ff1c26ba",
      "type": "summary_similarity",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "306e0eef-2977-4c2a-99c2-4876a855b6ed",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.9064119212732897
      }
    },
    {
      "id": "59921106-6cc8-40c9-8c10-6f5652f39302",
      "type": "summary_similarity",
      "source": "253a4754-48e6-48e4-a8e1-5b18c245d7ee",
      "target": "018d35d5-cc1e-4229-8b63-f524daee05ea",
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.8884926917392532
      }
    },
    {
      "id": "91c1a9a6-c5b8-4d49-8717-ae255d09099e",
      "type": "entities_overlap",
      "source": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "target": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.078125,
        "overlapped_items": [
          [
            "LLaMA 3",
            "LLaMa 3 8B"
          ],
          [
            "LLaMA 3",
            "Llama3"
          ],
          [
            "LLaMA 3",
            "Llama 3"
          ],
          [
            "BERT",
            "BERT"
          ],
          [
            "BERT",
            "BERT"
          ]
        ]
      }
    },
    {
      "id": "1e03c842-26ce-4d23-8d6e-5c4c3dc67466",
      "type": "entities_overlap",
      "source": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "target": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "LLaMA 3",
            "LLaMa 3 8B"
          ]
        ]
      }
    },
    {
      "id": "fefc5d7f-a8fd-4ecb-a437-9a15524dc50f",
      "type": "entities_overlap",
      "source": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "target": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "LLaMA 3",
            "LLaMA 3"
          ]
        ]
      }
    },
    {
      "id": "e4e91ada-af11-46bd-86a8-4300386c1aae",
      "type": "entities_overlap",
      "source": "8bd8bee7-47ee-407f-b229-e33e6d5a2a1e",
      "target": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "LLaMA 3",
            "LLaMa 3"
          ]
        ]
      }
    },
    {
      "id": "50154f11-427b-489e-afe5-dd4ba09a15db",
      "type": "entities_overlap",
      "source": "1a65cb75-059f-484a-ba78-42b6880ff472",
      "target": "e66ec817-72cb-406b-acc2-482d3febd6b3",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.14285714285714285,
        "overlapped_items": [
          [
            "Journal of Big Data and Computing",
            "Journal of Big Data and Computing"
          ],
          [
            "STEMM Institute Press",
            "STEMM Institute Press"
          ]
        ]
      }
    },
    {
      "id": "bf0735ca-3d47-4748-8784-c0d7f566d6ae",
      "type": "entities_overlap",
      "source": "1a65cb75-059f-484a-ba78-42b6880ff472",
      "target": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.07142857142857142,
        "overlapped_items": [
          [
            "Journal of Big Data and Computing",
            "Journal of Big Data and Computing"
          ],
          [
            "STEMM Institute Press",
            "STEMM Institute Press"
          ]
        ]
      }
    },
    {
      "id": "d4999d1b-c325-461c-b5dc-5aba36dbbb64",
      "type": "entities_overlap",
      "source": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "target": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.046875,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMa 3 8B"
          ],
          [
            "Llama3",
            "LLaMa 3 8B"
          ],
          [
            "Llama 3",
            "LLaMa 3 8B"
          ]
        ]
      }
    },
    {
      "id": "3135d0b4-5134-4983-9517-a2cc825e1750",
      "type": "entities_overlap",
      "source": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "target": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.0625,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMa-3-8B"
          ],
          [
            "Llama3",
            "LLaMa-3-8B"
          ]
        ]
      }
    },
    {
      "id": "baa69fc9-40e1-4ca6-bddc-77eba00ba572",
      "type": "entities_overlap",
      "source": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "target": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.046875,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMA 3"
          ],
          [
            "Llama3",
            "LLaMA 3"
          ],
          [
            "Llama 3",
            "LLaMA 3"
          ]
        ]
      }
    },
    {
      "id": "eee197c6-a623-4520-9fd0-c8e76a29629d",
      "type": "entities_overlap",
      "source": "a94f46e6-a000-49fb-a5a8-35fe81818ebb",
      "target": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.046875,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMa 3"
          ],
          [
            "Llama3",
            "LLaMa 3"
          ],
          [
            "Llama 3",
            "LLaMa 3"
          ]
        ]
      }
    },
    {
      "id": "38232314-5ec0-47a0-9592-b571bd77a271",
      "type": "entities_overlap",
      "source": "e66ec817-72cb-406b-acc2-482d3febd6b3",
      "target": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.25,
        "overlapped_items": [
          [
            "Journal of Big Data and Computing",
            "Journal of Big Data and Computing"
          ],
          [
            "STEMM Institute Press",
            "STEMM Institute Press"
          ]
        ]
      }
    },
    {
      "id": "f3c7d0d6-35aa-48d0-8785-5b1f5f42749a",
      "type": "entities_overlap",
      "source": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "target": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03125,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMa-3-8B"
          ]
        ]
      }
    },
    {
      "id": "0d70fbaf-0b1b-4c23-94fb-625dcde73c6c",
      "type": "entities_overlap",
      "source": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "target": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMA 3"
          ]
        ]
      }
    },
    {
      "id": "a31308fc-45ac-446b-aa62-7e61c10394c7",
      "type": "entities_overlap",
      "source": "1a29e1f7-d09f-4990-999a-a0adff8b93b4",
      "target": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "LLaMa 3 8B",
            "LLaMa 3"
          ]
        ]
      }
    },
    {
      "id": "53f8428f-ddb0-46d9-a4e2-cb1189a9e7af",
      "type": "entities_overlap",
      "source": "f08761f4-b7c4-4f7f-a0a3-be265383af10",
      "target": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03125,
        "overlapped_items": [
          [
            "DistilBERT-Base-Uncased",
            "distilBERT-base-uncased"
          ]
        ]
      }
    },
    {
      "id": "64d65ae5-c96f-46b6-85bb-9bf0b3d15625",
      "type": "entities_overlap",
      "source": "e288687c-0b9d-4058-bf2b-ac1ce4aed2d0",
      "target": "3ac8d8f3-0f2c-47c1-8119-decde6a7b68f",
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "LLaMA 3",
            "LLaMa 3"
          ]
        ]
      }
    }
  ]
}